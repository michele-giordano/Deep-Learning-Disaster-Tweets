{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860557f7",
   "metadata": {},
   "source": [
    "# Deep Learning - Module 4 Assignment<br>NLP Disaster Tweets Kaggle Mini-Project \n",
    "by M. Giordano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c0832",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This work covers all the required points of the rubric, following the steps in the same order of appearance:\n",
    "\n",
    "1. Brief description of the problem and data (5 pts) \n",
    "\n",
    "2. Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data (15 pts)\n",
    "\n",
    "3. Model Architecture (25 pts)\n",
    "\n",
    "4. Results and Analysis (35 pts) \n",
    "\n",
    "5. Conclusion (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4672b",
   "metadata": {},
   "source": [
    "## 1.  Brief description of the problem and data\n",
    "\n",
    "<b>This is an Deep Learning task focused on Text Classification with NLP.</b><br>\n",
    "The dataset in composed of 10.000 hand classified tweets: they can be about real disasters (class 1) or not (class 0).<br>\n",
    "<b>Our goal is to participate to the \"Natural Language Processing with Disaster Tweets\" Kaggle competition</b>, and try to achieve the highest accuracy score possible within the limited time limit for this assignment.\n",
    "\n",
    "This will be achieved through EDA, data cleaning, feature engineering, and model development from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba083ec6",
   "metadata": {},
   "source": [
    "### 1.2 Size, dimension, and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27d8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7613, 5)\n",
      "Test shape: (3263, 4)\n",
      "Sample submission shape: (3263, 2)\n",
      "\n",
      "Train columns: ['id', 'keyword', 'location', 'text', 'target']\n",
      "Test columns: ['id', 'keyword', 'location', 'text']\n",
      "Sample submission columns: ['id', 'target']\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train text length statistics:\n",
      "count    7613.000000\n",
      "mean      101.037436\n",
      "std        33.781325\n",
      "min         7.000000\n",
      "25%        78.000000\n",
      "50%       107.000000\n",
      "75%       133.000000\n",
      "max       157.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Sample training row:\n",
      "id                                                             1\n",
      "keyword                                                      NaN\n",
      "location                                                     NaN\n",
      "text           Our Deeds are the Reason of this #earthquake M...\n",
      "target                                                         1\n",
      "text_length                                                   69\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Sample test row:\n",
      "id                                           0\n",
      "keyword                                    NaN\n",
      "location                                   NaN\n",
      "text        Just happened a terrible car crash\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "globals().clear(); import gc; gc.collect()\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_DIR = \"./data\"\n",
    "\n",
    "# Define file paths\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATASET_DIR, \"test.csv\")\n",
    "SUBMISSION_PATH = os.path.join(DATASET_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "submission_df = pd.read_csv(SUBMISSION_PATH)\n",
    "\n",
    "# Basic shape and structure\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Sample submission shape:\", submission_df.shape)\n",
    "\n",
    "print(\"\\nTrain columns:\", train_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())\n",
    "print(\"Sample submission columns:\", submission_df.columns.tolist())\n",
    "\n",
    "# Label distribution (if present)\n",
    "if \"target\" in train_df.columns:\n",
    "    print(\"\\nTarget distribution:\")\n",
    "    print(train_df[\"target\"].value_counts())\n",
    "\n",
    "# Inspect text length statistics\n",
    "if \"text\" in train_df.columns:\n",
    "    train_df[\"text_length\"] = train_df[\"text\"].astype(str).apply(len)\n",
    "    print(\"\\nTrain text length statistics:\")\n",
    "    print(train_df[\"text_length\"].describe())\n",
    "\n",
    "# Inspect a sample row\n",
    "print(\"\\nSample training row:\")\n",
    "print(train_df.iloc[0])\n",
    "\n",
    "print(\"\\nSample test row:\")\n",
    "print(test_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074fc6e",
   "metadata": {},
   "source": [
    "The dataset provided for the competition is organized in tabular form and consists of three CSV files: `train.csv`, `test.csv`, and `sample_submission.csv`. The training set contains 7,613 samples, while the test set includes 3,263 samples, each corresponding to a single tweet.\n",
    "\n",
    "The structure of the training dataset includes the following columns:\n",
    "\n",
    "* **id**: unique identifier associated with each tweet.\n",
    "* **keyword**: optional keyword extracted from the tweet, which may provide additional contextual information.\n",
    "* **location**: optional free-text field indicating the reported location of the event.\n",
    "* **text**: raw tweet content, represented as a short textual sequence.\n",
    "* **target**: binary label indicating whether the tweet refers to a real disaster event (1) or not (0).\n",
    "\n",
    "The test dataset follows the same structure, excluding the target column, which is instead predicted and submitted according to the format specified in `sample_submission.csv`.\n",
    "\n",
    "From a data perspective, the textual inputs are relatively short, with an average length of approximately 100 characters and a maximum length below 160 characters, consistent with the constraints of the Twitter platform. The target distribution shows a moderate class imbalance, with non-disaster tweets being more frequent than disaster-related ones. This structure makes the dataset suitable for sequence-based modeling using recurrent neural networks for text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ff092",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360eb2a",
   "metadata": {},
   "source": [
    "### 2.1 Preliminary Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e7d557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      " - Number of records: 7613\n",
      " - Number of columns: 5\n",
      "\n",
      "Test dataset:\n",
      " - Number of records: 3263\n",
      " - Number of columns: 4\n",
      "\n",
      "Train columns:\n",
      "['id', 'keyword', 'location', 'text', 'target']\n",
      "\n",
      "Test columns:\n",
      "['id', 'keyword', 'location', 'text']\n",
      "\n",
      "First 10 training records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "5   8     NaN      NaN   \n",
       "6  10     NaN      NaN   \n",
       "7  13     NaN      NaN   \n",
       "8  14     NaN      NaN   \n",
       "9  15     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5                         #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires   \n",
       "6                                        #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "7                                                                            I'm on top of the hill and I can see a fire in the woods...   \n",
       "8                                                        There's an emergency evacuation happening now in the building across the street   \n",
       "9                                                                                   I'm afraid that the tornado is coming to our area...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset environment and free memory\n",
    "globals().clear(); import gc; gc.collect()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Dataset directory\n",
    "DATASET_DIR = \"./data\"\n",
    "\n",
    "# File paths\n",
    "TRAIN_PATH = os.path.join(DATASET_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATASET_DIR, \"test.csv\")\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Display number of records and columns\n",
    "print(\"Train dataset:\")\n",
    "print(\" - Number of records:\", train_df.shape[0])\n",
    "print(\" - Number of columns:\", train_df.shape[1])\n",
    "\n",
    "print(\"\\nTest dataset:\")\n",
    "print(\" - Number of records:\", test_df.shape[0])\n",
    "print(\" - Number of columns:\", test_df.shape[1])\n",
    "\n",
    "# Display feature / label names\n",
    "print(\"\\nTrain columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\nTest columns:\")\n",
    "print(test_df.columns.tolist())\n",
    "\n",
    "# Display first 10 records of the training set\n",
    "print(\"\\nFirst 10 training records:\")\n",
    "df_head_10 = train_df.head(10)\n",
    "display(df_head_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6cb3b0",
   "metadata": {},
   "source": [
    "#### 2.1.2 Preliminary Data Exploration Results\n",
    "\n",
    "An initial inspection of the dataset confirms a simple tabular structure where each record corresponds to a single tweet and, in the training set, an associated binary target label. \n",
    "\n",
    "The visualization of the first samples immediately shows that `text` is the primary informative feature, while `keyword` and `location` contain many missing values (NaN).\n",
    "\n",
    "Given the high presence of NaN in these auxiliary fields, it will be necessary to evaluate carefully whether `keyword` and `location` provide sufficient predictive signal to justify their inclusion, or whether the modeling pipeline should rely exclusively on the tweet text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb1f64",
   "metadata": {},
   "source": [
    "### 2.2 Plan of analysis\n",
    "\n",
    "Based on the initial inspection of the dataset, the analysis will follow a structured sequence of steps aimed at preparing the data for modeling while keeping the pipeline as simple and interpretable as possible.\n",
    "\n",
    "* **Data cleaning:** analyze the presence and distribution of missing values, verify data consistency, and inspect the language characteristics of the textual content.\n",
    "\n",
    "* **Extended data analysis:** evaluate the class balance and further analyze the nature of the textual content using appropriate metrics and visualizations.\n",
    "\n",
    "* **Feature dropping:** evaluate whether to completely remove features that are weakly informative or affected by a high proportion of missing values.\n",
    "* **Feature engineering:** assess whether creating auxiliary features, such as text length or related statistics, may be beneficial.\n",
    "\n",
    "\n",
    "These steps are intended to inform subsequent modeling choices and to ensure that the final input representation is coherent with the objectives of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c721b4",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "515f489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count per column (train):\n",
      "id: 0/7613\n",
      "keyword: 61/7613\n",
      "location: 2533/7613\n",
      "text: 0/7613\n",
      "target: 0/7613\n",
      "\n",
      "NaN count per column (test):\n",
      "id: 0/3263\n",
      "keyword: 26/3263\n",
      "location: 1105/3263\n",
      "text: 0/3263\n",
      "\n",
      "Number of duplicated rows (full row):\n",
      "Train: 0/7613\n",
      "Test: 0/3263\n",
      "\n",
      "Number of duplicated rows based on ['text', 'target']:\n",
      "Train: 92/7613\n",
      "\n",
      "Number of duplicated rows based on ['text']:\n",
      "Test: 20/3263\n",
      "\n",
      "Data types (train):\n",
      "id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "target       int64\n",
      "dtype: object\n",
      "\n",
      "Data types (test):\n",
      "id           int64\n",
      "keyword     object\n",
      "location    object\n",
      "text        object\n",
      "dtype: object\n",
      "\n",
      "Target dtype: int64\n",
      "Unique target values (2/7613): [1 0]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Data cleaning - basic checks\n",
    "# -------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Reload dataset (safe in case of previous modifications)\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Total number of records\n",
    "n_train = train_df.shape[0]\n",
    "n_test = test_df.shape[0]\n",
    "\n",
    "# 1) Count NaN values per feature\n",
    "print(\"NaN count per column (train):\")\n",
    "nan_counts_train = train_df.isna().sum()\n",
    "for col, n_nan in nan_counts_train.items():\n",
    "    print(f\"{col}: {n_nan}/{n_train}\")\n",
    "\n",
    "print(\"\\nNaN count per column (test):\")\n",
    "nan_counts_test = test_df.isna().sum()\n",
    "for col, n_nan in nan_counts_test.items():\n",
    "    print(f\"{col}: {n_nan}/{n_test}\")\n",
    "\n",
    "# 2) Check number of duplicated rows (full row)\n",
    "num_duplicates_train_full = train_df.duplicated().sum()\n",
    "num_duplicates_test_full = test_df.duplicated().sum()\n",
    "\n",
    "print(\"\\nNumber of duplicated rows (full row):\")\n",
    "print(f\"Train: {num_duplicates_train_full}/{n_train}\")\n",
    "print(f\"Test: {num_duplicates_test_full}/{n_test}\")\n",
    "\n",
    "# 2b) Check duplicated rows based on text and target only (train set)\n",
    "if {\"text\", \"target\"}.issubset(train_df.columns):\n",
    "    num_duplicates_train_text_target = train_df.duplicated(subset=[\"text\", \"target\"]).sum()\n",
    "    print(\"\\nNumber of duplicated rows based on ['text', 'target']:\")\n",
    "    print(f\"Train: {num_duplicates_train_text_target}/{n_train}\")\n",
    "\n",
    "\n",
    "# 2c) Check duplicated rows based on ['text'] only (test set)\n",
    "if \"text\" in test_df.columns:\n",
    "    num_duplicates_test_text = test_df.duplicated(subset=[\"text\"]).sum()\n",
    "    print(\"\\nNumber of duplicated rows based on ['text']:\")\n",
    "    print(f\"Test: {num_duplicates_test_text}/{n_test}\")\n",
    "\n",
    "# 3) Check data types consistency\n",
    "print(\"\\nData types (train):\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nData types (test):\")\n",
    "print(test_df.dtypes)\n",
    "\n",
    "# Optional: verify target type and values\n",
    "if \"target\" in train_df.columns:\n",
    "    unique_targets = train_df[\"target\"].unique()\n",
    "    print(\"\\nTarget dtype:\", train_df[\"target\"].dtype)\n",
    "    print(f\"Unique target values ({len(unique_targets)}/{n_train}):\", unique_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c90db",
   "metadata": {},
   "source": [
    "#### 2.3.2 Data Cleaning Conclusion\n",
    "\n",
    "The data cleaning phase highlights the following key observations:\n",
    "\n",
    "* No feature is entirely composed of missing values; therefore, dropping entire columns at this stage would not be appropriate.\n",
    "\n",
    "* Some features, particularly keyword and location, contain a substantial proportion of NaN values (approximately one third of the training set), making record-level removal impractical due to the significant loss of data it would entail.\n",
    "\n",
    "* No fully duplicated rows are present; however, duplicated samples based on textual content (and target label in the training set) are observed, indicating that identical texts may appear with different auxiliary information.\n",
    "\n",
    "As a consequence, **no data cleaning is made a priori** before knowing better the nature of the data, explored in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555bf78",
   "metadata": {},
   "source": [
    "### 2.4 Extended Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1b57dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# 2.4 Extended Data Analysis (EDA)\n",
    "# 3x3 grid of plots\n",
    "# --------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Reload datasets\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "n_train = train_df.shape[0]\n",
    "n_test = test_df.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Helper for stacked bars (total + portion)\n",
    "def stacked_bar(ax, labels, totals, portions, title, ylabel=\"Count\"):\n",
    "    ax.bar(labels, totals, color=\"steelblue\", label=\"Total\")\n",
    "    ax.bar(labels, portions, color=\"coral\", label=\"Portion\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "\n",
    "# 1) Records with NaN vs total\n",
    "train_with_nan = train_df.isna().any(axis=1).sum()\n",
    "test_with_nan = test_df.isna().any(axis=1).sum()\n",
    "\n",
    "stacked_bar(\n",
    "    axes[0],\n",
    "    [\"Train\", \"Test\"],\n",
    "    [n_train, n_test],\n",
    "    [train_with_nan, test_with_nan],\n",
    "    \"Records with at least one NaN\"\n",
    ")\n",
    "\n",
    "# 2) Duplicated texts with varied metadata\n",
    "dup_text_mask = train_df.duplicated(subset=[\"text\"], keep=False)\n",
    "dup_df = train_df.loc[dup_text_mask, [\"text\", \"keyword\", \"location\"]]\n",
    "groups = dup_df.groupby(\"text\", dropna=False)\n",
    "\n",
    "total_dup_groups = groups.ngroups\n",
    "varied_groups = sum(\n",
    "    (g[\"keyword\"].nunique(dropna=False) > 1) or (g[\"location\"].nunique(dropna=False) > 1)\n",
    "    for _, g in groups\n",
    ")\n",
    "\n",
    "stacked_bar(\n",
    "    axes[1],\n",
    "    [\"Train\"],\n",
    "    [total_dup_groups],\n",
    "    [varied_groups],\n",
    "    \"Duplicated texts with varied metadata\"\n",
    ")\n",
    "\n",
    "# 3) Non-ASCII characters\n",
    "def contains_non_ascii(s):\n",
    "    return any(ord(c) > 127 for c in s)\n",
    "\n",
    "train_non_ascii = train_df[\"text\"].astype(str).apply(contains_non_ascii).sum()\n",
    "test_non_ascii = test_df[\"text\"].astype(str).apply(contains_non_ascii).sum()\n",
    "\n",
    "stacked_bar(\n",
    "    axes[2],\n",
    "    [\"Train\", \"Test\"],\n",
    "    [n_train, n_test],\n",
    "    [train_non_ascii, test_non_ascii],\n",
    "    \"Texts with non-ASCII characters\"\n",
    ")\n",
    "\n",
    "# 4) Accented Latin letters\n",
    "accented_latin = re.compile(r\"[À-ÖØ-öø-ÿ]\")\n",
    "train_accented = train_df[\"text\"].astype(str).str.contains(accented_latin).sum()\n",
    "test_accented = test_df[\"text\"].astype(str).str.contains(accented_latin).sum()\n",
    "\n",
    "stacked_bar(\n",
    "    axes[3],\n",
    "    [\"Train\", \"Test\"],\n",
    "    [n_train, n_test],\n",
    "    [train_accented, test_accented],\n",
    "    \"Texts with accented Latin letters\"\n",
    ")\n",
    "\n",
    "# 5) Special patterns: any vs none (pie)\n",
    "url_pat = re.compile(r\"http[s]?://\", flags=re.IGNORECASE)\n",
    "hashtag_pat = re.compile(r\"#\\w+\")\n",
    "mention_pat = re.compile(r\"@\\w+\")\n",
    "\n",
    "text_series = train_df[\"text\"].astype(str)\n",
    "has_any = (\n",
    "    text_series.str.contains(url_pat)\n",
    "    | text_series.str.contains(hashtag_pat)\n",
    "    | text_series.str.contains(mention_pat)\n",
    ")\n",
    "\n",
    "axes[4].pie(\n",
    "    [has_any.sum(), (~has_any).sum()],\n",
    "    labels=[\"Has URL/#/@\", \"None\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"coral\", \"steelblue\"]\n",
    ")\n",
    "axes[4].set_title(\"Special patterns (any vs none)\")\n",
    "\n",
    "# 6) Target distribution (pie)\n",
    "target_counts = train_df[\"target\"].value_counts().sort_index()\n",
    "axes[5].pie(\n",
    "    target_counts.values,\n",
    "    labels=[\"target=0\", \"target=1\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=[\"steelblue\", \"coral\"]\n",
    ")\n",
    "axes[5].set_title(\"Target distribution\")\n",
    "\n",
    "# 7) Text length distribution\n",
    "text_len = train_df[\"text\"].astype(str).apply(len)\n",
    "axes[6].hist(text_len, bins=30, color=\"steelblue\")\n",
    "axes[6].set_title(\"Text length distribution\")\n",
    "axes[6].set_xlabel(\"Characters\")\n",
    "axes[6].set_ylabel(\"Frequency\")\n",
    "\n",
    "# 8) NaN in keyword\n",
    "nan_keyword = train_df[\"keyword\"].isna().sum()\n",
    "stacked_bar(\n",
    "    axes[7],\n",
    "    [\"keyword\"],\n",
    "    [n_train],\n",
    "    [nan_keyword],\n",
    "    \"NaN in keyword\"\n",
    ")\n",
    "\n",
    "# 9) NaN in location\n",
    "nan_location = train_df[\"location\"].isna().sum()\n",
    "stacked_bar(\n",
    "    axes[8],\n",
    "    [\"location\"],\n",
    "    [n_train],\n",
    "    [nan_location],\n",
    "    \"NaN in location\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acaa322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample (first 10) - records containing hashtag and/or URL:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash #BREAKING</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword    location  \\\n",
       "0    1     NaN         NaN   \n",
       "3    6     NaN         NaN   \n",
       "4    7     NaN         NaN   \n",
       "5    8     NaN         NaN   \n",
       "6   10     NaN         NaN   \n",
       "11  17     NaN         NaN   \n",
       "12  18     NaN         NaN   \n",
       "13  19     NaN         NaN   \n",
       "14  20     NaN         NaN   \n",
       "31  48  ablaze  Birmingham   \n",
       "\n",
       "                                                                                                                                 text  \\\n",
       "0                                                               Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "3                                                                   13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                            Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5                      #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires   \n",
       "6                                     #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "11  Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding   \n",
       "12                                                       #raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count    \n",
       "13                                                                                            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14                                                                           Damage to school bus on 80 in multi car crash #BREAKING    \n",
       "31                                                                            @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "31       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (first 10) - records with NaN in keyword and/or location:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "5   8     NaN      NaN   \n",
       "6  10     NaN      NaN   \n",
       "7  13     NaN      NaN   \n",
       "8  14     NaN      NaN   \n",
       "9  15     NaN      NaN   \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0                                                                  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                 Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                      13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                               Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5                         #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires   \n",
       "6                                        #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "7                                                                            I'm on top of the hill and I can see a fire in the woods...   \n",
       "8                                                        There's an emergency evacuation happening now in the building across the street   \n",
       "9                                                                                   I'm afraid that the tornado is coming to our area...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (first 10) - duplicated texts with different NaN pattern in keyword/location:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>5996</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>6023</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>Mysore, Karnataka</td>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>6031</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "      <td>#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>10169</td>\n",
       "      <td>violent%20storm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>10867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>5113</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589</th>\n",
       "      <td>5127</td>\n",
       "      <td>fatal</td>\n",
       "      <td>Varanasi</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>5130</td>\n",
       "      <td>fatal</td>\n",
       "      <td>Thane</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>5137</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>5140</td>\n",
       "      <td>fatal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          keyword           location  \\\n",
       "4221   5996        hazardous                NaN   \n",
       "4239   6023        hazardous  Mysore, Karnataka   \n",
       "4244   6031        hazardous   New Delhi, Delhi   \n",
       "7097  10169  violent%20storm                NaN   \n",
       "7607  10867              NaN                NaN   \n",
       "3578   5113            fatal                NaN   \n",
       "3589   5127            fatal           Varanasi   \n",
       "3591   5130            fatal              Thane   \n",
       "3597   5137            fatal                NaN   \n",
       "3600   5140            fatal                NaN   \n",
       "\n",
       "                                                                                                                                          text  \\\n",
       "4221                     #foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption   \n",
       "4239                     #foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption   \n",
       "4244                     #foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption   \n",
       "7097    #stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp   \n",
       "7607    #stormchase Violent Record Breaking EF-5 El Reno Oklahoma Tornado Nearly Runs Over ... - http://t.co/3SICroAaNz http://t.co/I27Oa0HISp   \n",
       "3578  11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...   \n",
       "3589  11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...   \n",
       "3591  11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...   \n",
       "3597  11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...   \n",
       "3600  11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...   \n",
       "\n",
       "      target  \n",
       "4221       1  \n",
       "4239       1  \n",
       "4244       0  \n",
       "7097       1  \n",
       "7607       1  \n",
       "3578       1  \n",
       "3589       1  \n",
       "3591       1  \n",
       "3597       1  \n",
       "3600       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (first 10) - texts with non-ASCII and/or accented characters:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>56</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>76</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintendent Lanford Salmon has r ... - http://t.co/vplR5Hka2u http://t.co/SxHW2TNNLf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>77</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Anaheim</td>\n",
       "      <td>Police: Arsonist Deliberately Set Black Church In North CarolinaåÊAblaze http://t.co/pcXarbH9An</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>81</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Sao Paulo, Brazil</td>\n",
       "      <td>Set our hearts ablaze and every city was a gift And every skyline was like a kiss upon the lips @Û_ https://t.co/cYoMPZ1A0Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>102</td>\n",
       "      <td>accident</td>\n",
       "      <td>St. Louis, MO</td>\n",
       "      <td>#stlouis #caraccidentlawyer Speeding Among Top Causes of Teen Accidents https://t.co/k4zoMOF319 https://t.co/S2kXVM0cBA Car Accident teeÛ_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>129</td>\n",
       "      <td>accident</td>\n",
       "      <td>Maldives</td>\n",
       "      <td>RT nAAYf: First accident in years. Turning onto Chandanee Magu from near MMA. Taxi rammed into me while I was halfway turned. Everyone confÛ_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>151</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yNXnvVKCDA | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/weQPesENku</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>156</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>US</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vAM5POdGyw | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/zEVakJaPcz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>164</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/M4JDZMGJoW | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/n0uhAsfkBv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>165</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>US</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vAM5POdGyw | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/zEVakJaPcz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword           location  \\\n",
       "38    56      ablaze                NaN   \n",
       "52    76      ablaze           Barbados   \n",
       "53    77      ablaze            Anaheim   \n",
       "57    81      ablaze  Sao Paulo, Brazil   \n",
       "71   102    accident      St. Louis, MO   \n",
       "87   129    accident           Maldives   \n",
       "104  151  aftershock        Switzerland   \n",
       "106  156  aftershock                 US   \n",
       "114  164  aftershock        Switzerland   \n",
       "115  165  aftershock                 US   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "38     Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J   \n",
       "52           SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintendent Lanford Salmon has r ... - http://t.co/vplR5Hka2u http://t.co/SxHW2TNNLf   \n",
       "53                                                  Police: Arsonist Deliberately Set Black Church In North CarolinaåÊAblaze http://t.co/pcXarbH9An   \n",
       "57                     Set our hearts ablaze and every city was a gift And every skyline was like a kiss upon the lips @Û_ https://t.co/cYoMPZ1A0Z   \n",
       "71      #stlouis #caraccidentlawyer Speeding Among Top Causes of Teen Accidents https://t.co/k4zoMOF319 https://t.co/S2kXVM0cBA Car Accident teeÛ_   \n",
       "87   RT nAAYf: First accident in years. Turning onto Chandanee Magu from near MMA. Taxi rammed into me while I was halfway turned. Everyone confÛ_   \n",
       "104      320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yNXnvVKCDA | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/weQPesENku   \n",
       "106      320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vAM5POdGyw | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/zEVakJaPcz   \n",
       "114      320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/M4JDZMGJoW | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/n0uhAsfkBv   \n",
       "115      320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vAM5POdGyw | @djicemoon | #Dubstep #TrapMusic #DnB #EDM #Dance #IcesÛ_ http://t.co/zEVakJaPcz   \n",
       "\n",
       "     target  \n",
       "38        1  \n",
       "52        0  \n",
       "53        1  \n",
       "57        0  \n",
       "71        0  \n",
       "87        1  \n",
       "104       0  \n",
       "106       0  \n",
       "114       0  \n",
       "115       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# 2.4 EDA - Samples by subcategory\n",
    "# --------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# Reload datasets\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# Patterns for special content\n",
    "has_url = train_df[\"text\"].astype(str).str.contains(r\"http[s]?://\", regex=True, na=False)\n",
    "has_hashtag = train_df[\"text\"].astype(str).str.contains(r\"#\\w+\", regex=True, na=False)\n",
    "has_special = has_url | has_hashtag\n",
    "\n",
    "# Patterns for non-standard / foreign characters\n",
    "non_ascii_regex = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "accented_latin_regex = re.compile(r\"[À-ÖØ-öø-ÿ]\")\n",
    "\n",
    "has_non_ascii = train_df[\"text\"].astype(str).str.contains(non_ascii_regex)\n",
    "has_accented = train_df[\"text\"].astype(str).str.contains(accented_latin_regex)\n",
    "\n",
    "# 1) Sample: records with hashtag or URL\n",
    "print(\"Sample (first 10) - records containing hashtag and/or URL:\")\n",
    "sample_special_10 = train_df.loc[\n",
    "    has_special, [\"id\", \"keyword\", \"location\", \"text\", \"target\"]\n",
    "].head(10)\n",
    "display(sample_special_10)\n",
    "\n",
    "# 2) Sample: records with any NaN in auxiliary fields (keyword/location)\n",
    "has_nan_aux = train_df[[\"keyword\", \"location\"]].isna().any(axis=1)\n",
    "print(\"\\nSample (first 10) - records with NaN in keyword and/or location:\")\n",
    "sample_nan_aux_10 = train_df.loc[\n",
    "    has_nan_aux, [\"id\", \"keyword\", \"location\", \"text\", \"target\"]\n",
    "].head(10)\n",
    "display(sample_nan_aux_10)\n",
    "\n",
    "# 3) Sample: duplicated text with different NaN pattern across keyword/location\n",
    "train_df[\"_kw_nan\"] = train_df[\"keyword\"].isna()\n",
    "train_df[\"_loc_nan\"] = train_df[\"location\"].isna()\n",
    "\n",
    "dup_text_mask = train_df.duplicated(subset=[\"text\"], keep=False)\n",
    "dup_df = train_df.loc[\n",
    "    dup_text_mask,\n",
    "    [\"id\", \"keyword\", \"location\", \"text\", \"target\", \"_kw_nan\", \"_loc_nan\"]\n",
    "].copy()\n",
    "\n",
    "groups = []\n",
    "for _, g in dup_df.groupby(\"text\", dropna=False):\n",
    "    kw_nan_varies = g[\"_kw_nan\"].nunique(dropna=False) > 1\n",
    "    loc_nan_varies = g[\"_loc_nan\"].nunique(dropna=False) > 1\n",
    "    if kw_nan_varies or loc_nan_varies:\n",
    "        groups.append(g)\n",
    "\n",
    "if groups:\n",
    "    varied_nan_dup_df = pd.concat(groups, axis=0)\n",
    "    print(\"\\nSample (first 10) - duplicated texts with different NaN pattern in keyword/location:\")\n",
    "    sample_varied_nan_dup_10 = varied_nan_dup_df[\n",
    "        [\"id\", \"keyword\", \"location\", \"text\", \"target\"]\n",
    "    ].head(10)\n",
    "    display(sample_varied_nan_dup_10)\n",
    "else:\n",
    "    print(\"\\nNo duplicated texts with different NaN pattern in keyword/location were found.\")\n",
    "\n",
    "# 4) Sample: texts with non-ASCII or accented foreign characters\n",
    "has_foreign_chars = has_non_ascii | has_accented\n",
    "print(\"\\nSample (first 10) - texts with non-ASCII and/or accented characters:\")\n",
    "sample_foreign_10 = train_df.loc[\n",
    "    has_foreign_chars, [\"id\", \"keyword\", \"location\", \"text\", \"target\"]\n",
    "].head(10)\n",
    "display(sample_foreign_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8f309",
   "metadata": {},
   "source": [
    "#### 2.4.2 Extended Data Analysis Conclusions\n",
    "\n",
    "Based on the extended data analysis, it emerges that duplicated rows with identical text and minor variations in auxiliary fields (such as location) correspond to reposted content.<br>\n",
    "Since the objective of this task is pure text classification rather than frequency or relevance estimation, these duplicates do not provide additional learning signal and will be reduced by retaining only the most informative instance per text.\n",
    "\n",
    "Furthermore, tweets containing hashtags and URLs will be normalized to preserve semantic content while removing tokens that are not directly informative for classification (e.g., URL prefixes). \n",
    "\n",
    "Finally, the observed class imbalance is mild and, given the limited dataset size, no explicit balancing strategy will be applied at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d3012",
   "metadata": {},
   "source": [
    "### 2.5 Feature Dropping\n",
    "\n",
    "At this stage, only the `id` feature can be safely removed, as it serves exclusively as an identifier and does not carry any semantic or predictive information for the classification task.\n",
    "\n",
    "The remaining auxiliary features (`keyword` and `location`) are retained. Although they contain missing values, when present they may still provide useful contextual signals for the model. For this reason, their NaN values are not used to justify feature removal.\n",
    "\n",
    "Instead, missing values in these features are replaced with a blank space (`\" \"`), ensuring consistent input formatting and avoiding the introduction of artificial zero-weight effects during the training phase of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926c4be",
   "metadata": {},
   "source": [
    "### 2.6 Feature engineering\n",
    "\n",
    "The feature engineering steps are applied in the following order, prioritizing operations that reduce dataset redundancy early in the pipeline in order to streamline subsequent processing:\n",
    "\n",
    "1. **Removal of less informative duplicated texts**: duplicated records sharing the same textual content are reduced by retaining only the most informative instance, favoring rows with fewer missing values in auxiliary features.\n",
    "2. **Normalization of special tokens**: removal of non-semantic prefixes such as `http`, `@`, and `#`, which do not carry discriminative information for the classification task.\n",
    "3. **Token boundary normalization**: replacement of link- and hashtag-related separators (e.g., hyphens and similar connectors) with whitespace to improve tokenization and downstream vector representations.\n",
    "4. **Character normalization**: normalization of non-standard, corrupted, or unreadable characters by mapping accented or malformed symbols to their neutral UTF-8 equivalents or replacing them with whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61cc8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auxiliary feature normalization completed\n",
      "Records processed: 7613\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Feature dropping & normalization on auxiliary fields\n",
    "# -------------------------------------------------\n",
    "def normalize_auxiliary_features(df):\n",
    "    before_count = df.shape[0]\n",
    "\n",
    "    # Columns to normalize (exclude id and target)\n",
    "    cols_to_clean = [\"keyword\", \"location\", \"text\"]\n",
    "\n",
    "    for col in cols_to_clean:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            # Replace URL-encoded artifacts\n",
    "            .str.replace(r\"%20\", \" \", regex=True)\n",
    "            .str.replace(r\"%amp;\", \" \", regex=True)\n",
    "            # Replace hyphens with spaces\n",
    "            .str.replace(r\"-\", \" \", regex=False)\n",
    "            # Normalize multiple spaces\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "    print(\"Auxiliary feature normalization completed\")\n",
    "    print(f\"Records processed: {before_count}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Apply normalization\n",
    "# -------------------------------\n",
    "train_df = normalize_auxiliary_features(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37748af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Supervised duplicate removal\n",
      "Records before : 7613\n",
      "Records after  : 7502\n",
      "Records removed: 111\n",
      "2) Removal of http / @ / # tokens: 5700 records modified\n",
      "3) Separator normalization (hyphens to spaces): 5733 records modified\n",
      "4) Unicode and accented character normalization: 661 records modified\n",
      "5) Global lowercase + feature normalization: 7394 records modified\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Utility: count how many rows changed in the text\n",
    "# -------------------------------------------------\n",
    "def count_modified_rows(before_df, after_df):\n",
    "    return (before_df[\"text\"] != after_df[\"text\"]).sum()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Remove less informative duplicates (supervised)\n",
    "# -------------------------------------------------\n",
    "def remove_less_informative_duplicates(train_df):\n",
    "    initial_count = len(train_df)\n",
    "\n",
    "    dup_mask = train_df.duplicated(subset=[\"text\"], keep=False)\n",
    "    dup_df = train_df.loc[dup_mask].copy()\n",
    "\n",
    "    groups = dup_df.groupby(\"text\")\n",
    "\n",
    "    rows_to_keep = []\n",
    "\n",
    "    for _, g in groups:\n",
    "        g = g.copy()\n",
    "        g[\"_nan_count\"] = g[[\"keyword\", \"location\"]].isna().sum(axis=1)\n",
    "\n",
    "        # keep row with fewer NaN, then lowest id\n",
    "        best_row = g.sort_values([\"_nan_count\", \"id\"]).iloc[0]\n",
    "        rows_to_keep.append(best_row.name)\n",
    "\n",
    "    all_dup_indices = set(dup_df.index)\n",
    "    keep_indices = set(rows_to_keep)\n",
    "    drop_indices = list(all_dup_indices - keep_indices)\n",
    "\n",
    "    train_df = train_df.drop(index=drop_indices).reset_index(drop=True)\n",
    "\n",
    "    final_count = len(train_df)\n",
    "\n",
    "    print(\"1) Supervised duplicate removal\")\n",
    "    print(f\"Records before : {initial_count}\")\n",
    "    print(f\"Records after  : {final_count}\")\n",
    "    print(f\"Records removed: {initial_count - final_count}\")\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Remove http / @ / # tokens\n",
    "# -------------------------------------------------\n",
    "def remove_special_prefixes(df):\n",
    "    before = df.copy()\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"@\\w+\", \"\", regex=True)\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"#\\w+\", \"\", regex=True)\n",
    "\n",
    "    modified = count_modified_rows(before, df)\n",
    "    print(f\"2) Removal of http / @ / # tokens: {modified} records modified\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Replace separators with spaces\n",
    "# -------------------------------------------------\n",
    "def replace_separators_with_spaces(df):\n",
    "    before = df.copy()\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"[-_/]\", \" \", regex=True)\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    modified = count_modified_rows(before, df)\n",
    "    print(f\"3) Separator normalization (hyphens to spaces): {modified} records modified\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Normalize unicode / accented characters\n",
    "# -------------------------------------------------\n",
    "def normalize_unicode_characters(df):\n",
    "    before = df.copy()\n",
    "\n",
    "    def normalize_text(s):\n",
    "        s = unicodedata.normalize(\"NFKD\", s)\n",
    "        s = s.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "        s = re.sub(r\"[^\\x00-\\x7F]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].astype(str).apply(normalize_text)\n",
    "\n",
    "    modified = count_modified_rows(before, df)\n",
    "    print(f\"4) Unicode and accented character normalization: {modified} records modified\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 5) Final normalization: lowercase + clean keyword/location\n",
    "def normalize_all_text_features(df):\n",
    "    before = df.copy()\n",
    "\n",
    "    text_cols = [\"keyword\", \"location\", \"text\"]\n",
    "\n",
    "    for col in text_cols:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            # lowercase\n",
    "            .str.lower()\n",
    "            # replace explicit junk tokens\n",
    "            .str.replace(\"&amp;\", \" \", regex=False)\n",
    "            .str.replace(\"%20\", \" \", regex=False)\n",
    "            # replace slashes and backslashes\n",
    "            .str.replace(r\"[\\\\/]\", \" \", regex=True)\n",
    "            # replace literal 'nan' strings\n",
    "            .str.replace(r\"\\bnan\\b\", \" \", regex=True)\n",
    "            # collapse multiple spaces\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "    modified = (\n",
    "        (before[text_cols] != df[text_cols])\n",
    "        .any(axis=1)\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "    print(f\"5) Global lowercase + feature normalization: {modified} records modified\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Apply feature engineering pipeline\n",
    "# -------------------------------------------------\n",
    "train_df = remove_less_informative_duplicates(train_df)\n",
    "train_df = remove_special_prefixes(train_df)\n",
    "train_df = replace_separators_with_spaces(train_df)\n",
    "train_df = normalize_unicode_characters(train_df)\n",
    "train_df = normalize_all_text_features(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2668cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Duplicated texts remaining: 662\n",
      "2) Records still containing http/@/#: 7\n",
      "3) Records still containing separators (- _ /): 0\n",
      "4) Records still containing non-ASCII characters: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "# --------------------------------\n",
    "# Final sanity checks (post-cleaning)\n",
    "# --------------------------------\n",
    "\n",
    "# 1) Number of duplicated texts\n",
    "num_duplicate_texts = train_df.duplicated(subset=[\"text\"]).sum()\n",
    "print(f\"1) Duplicated texts remaining: {num_duplicate_texts}\")\n",
    "\n",
    "# 2) Number of records still containing http / @ / #\n",
    "pattern_http_at_hash = re.compile(r\"http\\S+|@\\w+|#\\w+\", flags=re.IGNORECASE)\n",
    "num_special_tokens = train_df[\"text\"].astype(str).str.contains(pattern_http_at_hash).sum()\n",
    "print(f\"2) Records still containing http/@/#: {num_special_tokens}\")\n",
    "\n",
    "# 3) Number of strings still containing separators (- _ /)\n",
    "pattern_separators = re.compile(r\"[-_/]\")\n",
    "num_separators = train_df[\"text\"].astype(str).str.contains(pattern_separators).sum()\n",
    "print(f\"3) Records still containing separators (- _ /): {num_separators}\")\n",
    "\n",
    "# 4) Number of strings still containing non-ASCII / unnormalized characters\n",
    "def has_non_ascii(s):\n",
    "    return any(ord(c) > 127 for c in s)\n",
    "\n",
    "num_non_ascii = train_df[\"text\"].astype(str).apply(has_non_ascii).sum()\n",
    "print(f\"4) Records still containing non-ASCII characters: {num_non_ascii}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "04564cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 duplicated records (FULL TEXT, no truncation):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : debris\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #?? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : debris\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #?? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : suicide bomber\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #?? #??? #??? suicide bomber kills 15 in saudi security site mosque reuters\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : suicide bomber\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #?? #??? #??? suicide bomber kills 15 in saudi security site mosque reuters\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : debris\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #???? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : debris\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#?? #???? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : debris\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#??? #?? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : \n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "#??? #?? #??? #??? mh370: aircraft debris found on la reunion is from missing malaysia airlines ...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : oil spill\n",
      "Location : amarillo\n",
      "Target   : 1\n",
      "Text     :\n",
      "'california: spring oil spill estimate grows ' by the associated press via nyt\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : oil spill\n",
      "Location : corpus christi\n",
      "Target   : 1\n",
      "Text     :\n",
      "'california: spring oil spill estimate grows ' by the associated press via nyt\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : famine\n",
      "Location : new york, usa\n",
      "Target   : 1\n",
      "Text     :\n",
      "'food crematoria' provoke outrage amid crisis famine memories...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : famine\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      "'food crematoria' provoke outrage amid crisis famine memories...\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : flames\n",
      "Location : new york\n",
      "Target   : 0\n",
      "Text     :\n",
      "*new* snap on tools black baseball hat cap silver gray embroidered s logo flames full reu\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : flames\n",
      "Location : new york\n",
      "Target   : 0\n",
      "Text     :\n",
      "*new* snap on tools black baseball hat cap silver gray embroidered s logo flames full reu\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : flames\n",
      "Location : new york\n",
      "Target   : 0\n",
      "Text     :\n",
      "*new* snap on tools black baseball hat cap silver gray embroidered s logo flames full reu\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : screams\n",
      "Location : w.i.t.s academy\n",
      "Target   : 0\n",
      "Text     :\n",
      "*screams*\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : screams\n",
      "Location : blackfalds.\n",
      "Target   : 0\n",
      "Text     :\n",
      "*screams*\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : suicide bomb\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      ". .. .. pic of 16yr old pkk suicide bomber who detonated bomb in turkey army trench released\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : suicide bomb\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      ". .. .. pic of 16yr old pkk suicide bomber who detonated bomb in turkey army trench released\n",
      "--------------------------------------------------------------------------------\n",
      "Keyword  : refugees\n",
      "Location : \n",
      "Target   : 1\n",
      "Text     :\n",
      ". .....hmm 12000 nigerian refugees repatriated from cameroon (\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Full-text visualization of duplicated records\n",
    "# --------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure pandas does not truncate strings (extra safety)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Identify duplicated texts\n",
    "dup_mask = train_df.duplicated(subset=[\"text\"], keep=False)\n",
    "dup_df = train_df.loc[dup_mask].copy()\n",
    "\n",
    "# Sort by text, then by id for readability\n",
    "dup_df = dup_df.sort_values([\"text\"])\n",
    "\n",
    "# Take first 20 duplicated records\n",
    "sample_dup = dup_df.head(20)\n",
    "\n",
    "print(\"First 20 duplicated records (FULL TEXT, no truncation):\\n\")\n",
    "\n",
    "for _, row in sample_dup.iterrows():\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Keyword  : {row['keyword']}\")\n",
    "    print(f\"Location : {row['location']}\")\n",
    "    print(f\"Target   : {row['target']}\")\n",
    "    print(\"Text     :\")\n",
    "    print(row[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c598cf5",
   "metadata": {},
   "source": [
    "#### 2.6.2 Feature engineering results\n",
    "\n",
    "Observing records such as:\n",
    "\n",
    "| ID: 3106, keyword: debris, location: NaN Text: #?? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines …\n",
    "\n",
    "and:\n",
    "\n",
    "| ID: 3126, keyword: debris, location: NaN Text: #??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines …\n",
    "\n",
    "it can be observed that, after an initial sanitization step, the semantic content of the text is effectively identical.\n",
    "The remaining differences are limited to residual non-standard artifacts (e.g. malformed hashtag-like tokens), which differ byte-wise but carry no additional information.\n",
    "\n",
    "These artifacts do not contribute to the classification task and instead introduce unnecessary noise into the textual representation.\n",
    "For this reason, a second, more targeted cleaning step is applied to further normalize such residual patterns and ensure that semantically identical texts are treated consistently by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64dc29e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 30 duplicated records (same text, different rows):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "aftershock\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "aftershock\n",
      "--------------------------------------------------------------------------------\n",
      "experts in france begin examining airplane debris found on reunion island: french air accident experts on wedn...\n",
      "--------------------------------------------------------------------------------\n",
      "horrible accident man died in wings of airplane (29 07 2015)\n",
      "--------------------------------------------------------------------------------\n",
      "horrible accident man died in wings of airplane (29 07 2015)\n",
      "--------------------------------------------------------------------------------\n",
      "experts in france begin examining airplane debris found on reunion island: french air accident experts on wedn...\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "cop pulls drunk driver to safety seconds before his car is hit by train. via\n",
      "--------------------------------------------------------------------------------\n",
      "cop pulls drunk driver to safety seconds before his car is hit by train. via\n",
      "--------------------------------------------------------------------------------\n",
      "=====================================\n",
      "Junk hashtag patterns '#?' removed from text field\n",
      "Records modified: 24/7502\n",
      "\n",
      "First 30 duplicated records (same text, different rows):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "aftershock\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "320 [ir] icemoon [aftershock] | | | u\n",
      "--------------------------------------------------------------------------------\n",
      "aftershock\n",
      "--------------------------------------------------------------------------------\n",
      "experts in france begin examining airplane debris found on reunion island: french air accident experts on wedn...\n",
      "--------------------------------------------------------------------------------\n",
      "horrible accident man died in wings of airplane (29 07 2015)\n",
      "--------------------------------------------------------------------------------\n",
      "horrible accident man died in wings of airplane (29 07 2015)\n",
      "--------------------------------------------------------------------------------\n",
      "experts in france begin examining airplane debris found on reunion island: french air accident experts on wedn...\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "ambulance sprinter automatic frontline vehicle choice of 14 lez compliant | ebay\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "twelve feared killed in pakistani air ambulance helicopter crash\n",
      "--------------------------------------------------------------------------------\n",
      "cop pulls drunk driver to safety seconds before his car is hit by train. via\n",
      "--------------------------------------------------------------------------------\n",
      "cop pulls drunk driver to safety seconds before his car is hit by train. via\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Remove junk hashtag patterns like \"#?\" or \"#??...\"\n",
    "# -------------------------------------------------\n",
    "def remove_question_mark_hashtags(df):\n",
    "    # Keep a copy of text before modification\n",
    "    before_text = df[\"text\"].astype(str)\n",
    "\n",
    "    # Regex: '#' followed by one or more '?' characters\n",
    "    pattern = re.compile(r\"#\\?+\")\n",
    "\n",
    "    # Apply cleaning\n",
    "    after_text = (\n",
    "        before_text\n",
    "        .apply(lambda x: pattern.sub(\"\", x))\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Count how many records actually changed\n",
    "    modified_count = (before_text != after_text).sum()\n",
    "\n",
    "    # Assign cleaned text back\n",
    "    df[\"text\"] = after_text\n",
    "\n",
    "    print(\"Junk hashtag patterns '#?' removed from text field\")\n",
    "    print(f\"Records modified: {modified_count}/{df.shape[0]}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Show first N duplicated texts (full text, no truncation)\n",
    "# -------------------------------------------------\n",
    "def show_text_duplicates(df, n=30):\n",
    "    dup_mask = df.duplicated(subset=[\"text\"], keep=False)\n",
    "    dup_df = df.loc[dup_mask].sort_values(\"id\")\n",
    "\n",
    "    if len(dup_df) == 0:\n",
    "        print (\"No duplicates found.\")\n",
    "    else:\n",
    "        \n",
    "        print(f\"\\nFirst {n} duplicated records (same text, different rows):\\n\")\n",
    "\n",
    "        for _, row in dup_df.head(n).iterrows():\n",
    "            print(\"-\" * 80)\n",
    "            print(row[\"text\"])\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Apply second-pass cleaning\n",
    "# -------------------------------\n",
    "show_text_duplicates(train_df, n=30)\n",
    "print(\"=====================================\")\n",
    "train_df = remove_question_mark_hashtags(train_df)\n",
    "show_text_duplicates(train_df, n=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c55bb542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised text deduplication summary\n",
      "Records before : 7502\n",
      "Records after  : 6835\n",
      "Records removed: 667\n",
      "No duplicates found.\n",
      "\n",
      "Final dataset shape: (6835, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def supervised_text_deduplication(train_df):\n",
    "    \"\"\"\n",
    "    Supervised deduplication based on identical text.\n",
    "    Keeps the most informative row per text (fewer NaN in keyword/location,\n",
    "    then lowest id), removes all others.\n",
    "    \"\"\"\n",
    "\n",
    "    # Work on a copy to avoid side effects\n",
    "    df = train_df.copy()\n",
    "\n",
    "    cleaned_rows = []\n",
    "    dup_list = []\n",
    "\n",
    "    # Group by exact text\n",
    "    text_groups = df.groupby(\"text\", sort=False)\n",
    "\n",
    "    for text, group in text_groups:\n",
    "        if len(group) == 1:\n",
    "            # No duplicates, keep as-is\n",
    "            cleaned_rows.append(group.iloc[0])\n",
    "        else:\n",
    "            # Multiple rows with same text → supervised choice\n",
    "            group = group.copy()\n",
    "\n",
    "            # Count NaN in auxiliary fields\n",
    "            group[\"_nan_count\"] = group[[\"keyword\", \"location\"]].isna().sum(axis=1)\n",
    "\n",
    "            # Sort: fewer NaN first, then lower id\n",
    "            group_sorted = group.sort_values(\n",
    "                by=[\"_nan_count\", \"id\"],\n",
    "                ascending=[True, True]\n",
    "            )\n",
    "\n",
    "            # Winner\n",
    "            best_row = group_sorted.iloc[0]\n",
    "            cleaned_rows.append(best_row)\n",
    "\n",
    "            # All others go to duplicate list\n",
    "            dropped = group_sorted.iloc[1:].drop(columns=[\"_nan_count\"])\n",
    "            dup_list.append(dropped)\n",
    "\n",
    "    # Build final DataFrames\n",
    "    cleaned_df = pd.DataFrame(cleaned_rows).reset_index(drop=True)\n",
    "\n",
    "    if dup_list:\n",
    "        dup_df = pd.concat(dup_list, ignore_index=True)\n",
    "    else:\n",
    "        dup_df = pd.DataFrame(columns=train_df.columns)\n",
    "\n",
    "    # Report\n",
    "    print(\"Supervised text deduplication summary\")\n",
    "    print(f\"Records before : {train_df.shape[0]}\")\n",
    "    print(f\"Records after  : {cleaned_df.shape[0]}\")\n",
    "    print(f\"Records removed: {train_df.shape[0] - cleaned_df.shape[0]}\")\n",
    "\n",
    "    return cleaned_df, dup_df\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Apply supervised deduplication\n",
    "# -------------------------------\n",
    "train_df, dup_list = supervised_text_deduplication(train_df)\n",
    "show_text_duplicates(train_df, n=30)\n",
    "# -------------------------------------------------\n",
    "# Drop ID column (no longer needed)\n",
    "# -------------------------------------------------\n",
    "train_df = train_df.drop(columns=[\"id\"])\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", train_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71749b",
   "metadata": {},
   "source": [
    "#### 2.6.3 Feature Engineering Conclusions\n",
    "\n",
    "After all the cleaning and formatting procedures applied, we were able to pass from \n",
    "\n",
    "**7,613 original records to 6,847**, \n",
    "\n",
    "eliminating **766 unuseful and redundant records** and optimizing the dataset for the text classification task.\n",
    "\n",
    "These operations reduced noise, removed semantic redundancy, and normalized textual artifacts, resulting in a cleaner and more consistent input suitable for training an RNN-based NLP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb1de29",
   "metadata": {},
   "source": [
    "### 2.7 EDA Conclusions\n",
    "\n",
    "By inspecting a random sample of 30 cleaned text records, it is possible to verify that the semantic content of the tweets is fully preserved.\n",
    "\n",
    "All the relevant information is retained while redundant noise and formatting artifacts have been removed, resulting in a dataset that is coherent, readable, and well suited for downstream text classification tasks.\n",
    "\n",
    "This concludes the exploratory data analysis phase of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a217129e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of text records:\n",
      "\n",
      "K:sandstorm | L:united states | it wouldnt turn into a sandstorm if riot gave a good answer. instead they gave dumb excuses. at least new client is there | TARGET:0\n",
      "K:stretcher | L:oklahoma city | dan hughes was taken off on a stretcher after danielle robinson collided hope they are both ok he called some games over years | TARGET:0\n",
      "K:attacked | L:1 3 of the blam squad | i'm feeling so attacked | TARGET:0\n",
      "K:harm | L:hogwarts | talk to please don't harm your self in any way shape or form please we care about you and if i saw u right now u better | TARGET:0\n",
      "K:seismic | L:uk | england east coast. dogger bank westward. 1. seismic survey in progress by m v western regent towing a 8400 metre long cable within areau | TARGET:0\n",
      "K:explode | L:winnipeg | i'm ready to explode! | TARGET:0\n",
      "K:injured | L:mumbai | udhampur terror attack: militants attack police post 2 spos injured: suspected militants tonight attacked a p... | TARGET:1\n",
      "K:blew up | L: | i blew up snapchat for no reason ?? | TARGET:0\n",
      "K:dead | L:united states | typhoon soudelor taking dead aim at taiwan | TARGET:1\n",
      "K:survive | L:united states | we learn and grow and become stronger as we face and survive the trials through which we must pass. | TARGET:0\n",
      "K:sinkhole | L:atlanta(ish), ga | talk on goz is fantastic. most interesting fact so far is that they manually bought all the .ru domains to sinkhole rather than seek co op. | TARGET:0\n",
      "K:police | L:uk | dt : rt : uithe col police can catch a pickpocket in liverpool stree... | TARGET:1\n",
      "K:death | L:buffalo dc | thanks i narrowly averted death that was fun you're right | TARGET:1\n",
      "K:hailstorm | L:facebook.com tradcatknights | canada: hailstorm flash flooding slam calgary knocks out power to 20k customers | TARGET:1\n",
      "K:demolition | L: | demolition frog (2002) | TARGET:1\n",
      "K:collapse | L:brighton and hove | '60 all out? what!' world reacts to aussie collapse | TARGET:0\n",
      "K:war zone | L: | they turned jasmines house into a war zone. ?? | TARGET:0\n",
      "K:detonate | L:brasil | . apollo brown 'detonate' f. m.o.p. | TARGET:0\n",
      "K:cyclone | L:philippines | severe weather bulletin no. 5 for: typhoon uiu (soudelor) tropical cyclone: warning issued at 5:00 pm 06... | TARGET:1\n",
      "K:twister | L: | twister dance game dance console instructions cable 5 pre loaded songs | TARGET:0\n",
      "K:collide | L: | love you can't wait until collide!!!???? | TARGET:0\n",
      "K:stretcher | L:south africa eastern cape | mxaaaa south africans just can't appreciate effort was not that bad stop hating | TARGET:0\n",
      "K:forest fire | L:slatina,romania | i liked a video j. cole fire squad (2014 forest hills drive) | TARGET:0\n",
      "K:bomb | L:bolton & tewkesbury, uk | hiroshima prepares to remember the day the bomb dropped | TARGET:1\n",
      "K:whirlwind | L:somewhere between here & there | i stand alone don't piss and moan about my choices made if i must reap the whirlwind so be it i'll do so with demeanor calm and staid | TARGET:0\n",
      "K:drowned | L: | toddler drowned in bath after mum left room to fetch his pyjamas | TARGET:1\n",
      "K:sinkhole | L:san diego | 10news ? water main break disrupts trolley service | TARGET:1\n",
      "K:threat | L:kwajalein virginia dayton, oh | definite triple crown threat. him and harper both. | TARGET:1\n",
      "K:destroy | L: | (sj gist): 148 houses farm produce destroy... | | TARGET:1\n",
      "K:loud bang | L:kenya | kotolily : breaking news! unconfirmed! i just heard a loud bang nearby. in what appears to be a blast of wind from my neighbour's ass. | TARGET:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Randomly sample 50 rows\n",
    "sample_df = train_df.sample(n=30, random_state=42)\n",
    "\n",
    "print(\"Random sample of text records:\\n\")\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    keyword = row[\"keyword\"] if pd.notna(row[\"keyword\"]) else \"\"\n",
    "    location = row[\"location\"] if pd.notna(row[\"location\"]) else \"\"\n",
    "    text = row[\"text\"]\n",
    "    target = row[\"target\"]\n",
    "\n",
    "    print(f\"K:{keyword} | L:{location} | {text} | TARGET:{target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38af34f",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "In this section, we implement a sequential neural network for text classification using two different recurrent architectures: a canonical Vanilla RNN and a more advanced Gated Recurrent Unit (GRU).\n",
    "Both models rely on GloVe-based word embeddings to transform raw textual data into dense vector representations, enabling the networks to explicitly exploit the sequential nature of the input text.\n",
    "\n",
    "The goal of this step is to define and justify the model architectures themselves; performance comparison, hyperparameter tuning, and training refinements are intentionally deferred to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fedeb7",
   "metadata": {},
   "source": [
    "### 3.1 Word Embedding with GloVe\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding technique that represents words as dense vectors learned from global word co-occurrence statistics.\n",
    "Unlike simple bag-of-words or TF-IDF approaches, GloVe captures semantic relationships between words by placing them in a continuous vector space where distances encode meaning.\n",
    "\n",
    "GloVe embeddings are pre-trained on large text corpora and therefore inject external linguistic knowledge into the model, which is particularly beneficial when working with relatively small datasets.\n",
    "This makes GloVe well suited for sequential models such as RNNs, where preserving semantic and contextual information across time steps is essential.\n",
    "\n",
    "In this assignment, GloVe embeddings are used as the input representation for both the Vanilla RNN and the GRU architectures.\n",
    "The expected outcome is a more expressive and stable learning process compared to sparse representations, enabling the recurrent models to better capture contextual patterns relevant for disaster-related tweet classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a01a1d",
   "metadata": {},
   "source": [
    "### 3.2 Vanilla RNN classifier\n",
    "A baseline sequential classifier is implemented using a canonical Vanilla RNN. \n",
    "\n",
    "The model processes each tweet as a sequence of word vectors (GloVe), updates a hidden state through time, and produces a final binary prediction through a dense output layer. \n",
    "\n",
    "This architecture is intentionally simple and serves as a reference point to compare a gated alternative (GRU) in the next subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e86716c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META.JSON\n",
      "{'algorithm': {'command': None, 'id': 1, 'name': 'Global Vectors', 'tool': 'GloVe', 'url': 'https://nlp.stanford.edu/software/GloVe-1.2.zip', 'version': '1.2'}, 'contents': [{'filename': 'model.txt', 'format': 'text'}, {'filename': 'meta.json', 'format': 'json'}], 'corpus': [{'NER': False, 'case preserved': True, 'description': 'English Wikipedia Dump of February 2017', 'id': 1, 'language': 'eng', 'lemmatized': False, 'public': True, 'stop words removal': 'NLTK', 'tagger': None, 'tagset': None, 'tokens': 2252637050, 'tool': 'Wikipedia Extractor', 'url': 'https://dumps.wikimedia.org/'}], 'creators': [{'email': 'andreku@ifi.uio.no', 'name': 'Andrey Kutuzov'}], 'dimensions': 300, 'handle': 'http://vectors.nlpl.eu/repository/20/8.zip', 'id': 8, 'iterations': 100, 'vocabulary size': 302815, 'window': 5}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MODEL.TXT (first 5 lines)\n",
      "302815 300\n",
      "also 0.524366 0.595930 -0.618752 -0.611566 -0.650232 0.947725 -0.477549 0.219030 -1.094925 0.243742 -0.427229 0.710730 -0.417352 0.057608 1.213977 0.325752 0.471678 -0.762056 0.222388 -0.453681 0.274129 -0.619699 -0.168127 -0.702172 0.286212 0.203216 0.649900 -0.535597 -0.330065 -0.602106 -0.170549 -0.030364 0.547073 0.031663 0.508747 0.388720 0.625101 -0.120403 0.016793 0.366485 0.347702 0.810091 -0.219087 -0.737118 -1.139067 0.634593 0.137162 0.032622 -0.101096 0.107959 0.028579 1.043808 -0.600556 -0.262550 -0.857415 0.432940 -0.665081 -0.875301 0.910692 -0.060289 -0.293957 0.052424 -0.190738 0.348965 -0.113559 0.044649 -0.089624 -0.250805 0.413248 0.133167 0.745309 0.778830 0.279054 0.619242 -0.244836 0.785286 0.571971 0.021032 -0.413457 -0.478764 0.386825 -0.182046 -0.089810 -0.117901 0.007448 -0.472922 0.015512 0.051984 0.523467 -0.078192 -0.231814 0.253041 0.691081 0.439208 -0.615435 -0.443756 0.150771 0.276344 0.069553 0.095514 -0.213988 -0.195280 0.424841 -0.408609 0.022610 -0.019514 -1.090741 0.241214 -0.331090 -0.666676 0.048672 0.474623 0.343940 0.957588 -0.178539 0.522318 -0.105949 0.581609 -0.189184 0.548743 -0.365726 0.022804 0.237065 0.114810 -0.374202 0.139889 -0.620352 -0.512886 -0.554714 0.128645 1.037432 -0.514216 -0.726012 -0.128052 -0.318711 0.224300 0.982472 0.225370 0.519932 0.467262 -0.059084 -0.554354 -0.189393 -0.396583 0.496771 0.280841 1.066665 -0.727237 -0.095488 -0.061546 -0.075314 -0.311621 -0.082888 -0.189245 -0.498958 0.500476 0.411526 -0.126928 0.794237 -0.108263 0.389084 -0.742832 -0.163691 -0.372198 -0.647780 0.395181 0.406903 0.274600 1.316536 0.129032 -0.909707 -0.443468 -0.588848 0.267043 0.007955 0.184073 -0.095634 0.476788 -0.830577 -0.097638 -0.457462 0.267412 -0.021588 -0.050055 0.174542 0.086905 -0.531546 0.179000 -0.526524 -0.251271 -0.707784 -0.321000 0.359042 0.532083 -0.343741 0.695558 -0.387365 0.024677 -0.544282 0.238776 -0.043165 0.517559 0.470921 0.725697 -0.056493 0.603057 0.094410 0.193287 -0.317943 -0.606819 0.461551 -0.315039 0.015279 0.018710 -0.050636 -1.382160 -0.549342 0.087352 -0.225451 -0.247218 0.824786 -0.098537 -0.985966 0.618574 0.156842 0.259537 0.569061 1.055396 0.447553 -0.184381 -0.052691 -0.663209 0.747564 0.409876 -0.365184 -0.633136 -0.438896 -0.249113 0.141182 -0.378395 0.195578 0.130567 -0.527316 0.304935 -0.124102 -0.065974 0.456736 0.350716 0.503047 -0.111433 -0.587248 -0.186985 0.766966 0.392072 0.498672 -0.646746 -0.628918 -0.293684 -0.491767 1.240002 0.227312 -0.439197 -0.325282 0.430726 0.848485 0.361766 -0.349277 -0.843082 0.364327 -0.158823 0.055893 0.073513 -0.041413 0.769787 -0.750700 0.199750 -0.051843 0.219127 -0.214661 -0.160817 -0.834696 0.043312 -0.221776 0.006020 0.466823 0.188327 -0.888753 0.269279 -0.441852 0.999909 0.301304 0.303049 0.333690 0.299435 0.380226 -0.023445 0.028656 -0.217400 -0.115385 0.241728\n",
      "first 1.070535 0.018076 -0.263981 -0.309771 -0.251514 0.986175 -0.772503 0.301989 -1.205109 0.175363 -0.675247 0.571757 0.369378 0.173900 0.502927 0.411295 0.730715 -0.495145 0.163057 -0.247911 0.337838 -0.333946 0.403914 0.012824 0.772884 -0.243193 0.178788 0.001900 -0.863133 -0.918930 -0.071434 0.511078 0.705559 -0.545523 0.350121 -0.027660 0.587256 0.083249 0.848629 0.218880 0.148937 0.667431 -0.518372 -0.585003 -1.160492 0.870369 -0.109068 -0.195449 0.170865 -0.262694 0.330867 0.369000 -0.602245 0.975410 -0.548918 0.389667 -0.522060 0.079002 1.004679 -0.798482 0.026866 -0.297315 -0.060302 0.173612 0.191121 0.411445 -0.159689 -0.657662 0.684245 0.511136 0.055049 -0.596972 -0.282170 1.295121 -0.260353 0.017714 -0.067779 -0.328456 0.796636 -0.396547 0.424525 -0.040068 -0.041957 0.216765 -0.534668 -0.712134 -0.565150 0.195521 0.521496 -0.541764 -0.766695 -0.407944 -0.096221 -0.389466 -0.643568 -0.016025 0.214216 0.622610 -0.019604 -0.166077 0.189682 -0.490937 0.409888 -0.368830 0.337801 -0.586661 -0.723178 -0.018611 0.196380 -0.670880 -0.061893 -0.356432 0.061247 0.930261 -0.342767 1.135256 -0.378011 0.737640 -0.756207 -0.066352 -0.418373 -0.370567 -0.292864 -0.547092 -0.481868 -0.152374 -0.214462 -0.320688 -0.353911 0.180006 0.462642 -0.777428 0.192939 -0.346417 -0.815256 -0.179148 0.370520 0.325315 0.230667 1.123126 -0.211526 -0.436507 -0.461627 -0.612067 -0.528515 0.585656 1.519927 -0.136385 -0.486705 -0.106531 -0.001205 -0.089199 -0.111123 0.305585 -0.083996 0.254377 0.006023 -0.155793 1.003177 0.112226 -0.103158 -0.490944 -0.124314 -0.174143 -0.300297 0.633390 -0.446199 0.006333 1.331714 0.319417 -0.735000 0.302655 -0.858472 0.573595 0.355667 0.584332 0.222759 0.533166 -0.335150 -0.116221 -0.131828 -0.822577 -0.088525 0.225492 -0.159701 0.275454 0.034082 -0.237700 -0.435417 0.191788 -0.872920 0.260709 0.363699 0.004493 -0.645930 0.481337 -0.128754 0.181021 -0.104166 0.537704 0.319514 -0.154032 0.215166 0.871432 -0.099127 -0.262569 0.592683 0.384720 -0.108995 -0.337985 0.167318 -0.462740 -0.176166 -0.255501 -0.437742 -0.489086 -0.401722 0.145355 0.125132 0.018292 0.522609 -0.082464 -0.800768 -0.074349 0.250687 -0.807726 0.853965 0.577961 0.004652 -0.030827 0.038185 -1.072509 0.082670 0.866098 -0.353617 0.219353 -0.292270 -0.290519 -0.234678 -0.724613 0.025591 -0.407640 -0.241987 0.449147 -0.250424 -0.153806 0.354514 -0.077411 1.041767 0.283568 -0.680118 -0.059101 0.191426 0.050478 1.079014 -0.063534 -0.404136 -0.455830 0.210017 1.228927 -0.123881 -0.190327 0.084731 0.172926 0.864632 0.074869 -0.197825 -0.446440 -0.138677 0.028143 -0.049388 -0.305840 -0.441856 0.676894 -1.012522 -0.201761 -0.695244 0.226757 -0.106177 -0.564004 0.076789 -0.094647 0.344344 0.288439 0.171946 -0.207095 -0.242972 0.151816 0.317877 0.570799 0.386621 -0.026830 0.085815 -0.406957 0.492137 -0.036673 0.170537 -0.682464 -0.681183 -0.031539\n",
      "one 0.643663 0.735946 -0.348764 0.130345 0.049371 0.634555 -0.566881 0.167643 -0.525458 -0.041891 -0.400365 0.740935 -0.427533 0.562289 0.722806 0.620574 0.886922 0.093091 0.018115 -0.415273 0.302986 -0.409559 -0.276972 0.095410 0.381231 0.310571 0.534952 -0.415194 -0.947216 -0.195760 -0.151450 0.332015 0.931118 0.251421 0.472161 -0.463066 0.480801 0.214122 -0.587950 -0.086558 0.454003 0.313967 -0.838870 -0.863936 -0.901681 0.999431 -0.136033 0.033456 -0.683712 -0.337546 -0.590234 0.563165 -0.442090 1.011711 -0.717180 0.072819 -0.567623 -0.299550 0.812468 -0.976844 -0.159449 -0.134329 -0.286490 0.170466 -0.353062 0.445919 0.039759 -0.405901 0.657698 0.773935 0.046639 0.728927 0.098339 0.906778 -0.119671 0.721604 -0.134809 -0.279208 0.294697 0.041903 -0.013279 -0.074399 0.080001 -0.282005 -0.823822 -0.426679 -0.020594 0.044239 0.534082 -0.388521 -0.994509 0.020944 0.703759 0.008510 -0.636886 -0.000791 0.264701 0.832795 0.182064 -0.435323 -0.156641 -0.472377 -0.061721 0.118651 -0.359217 -0.238084 -0.755035 -0.541789 -0.009591 -0.827299 -0.186897 -0.239589 -0.155450 0.406672 -0.863120 0.739545 0.640085 0.501682 -0.680695 0.319592 -0.208598 -0.222242 0.066520 -0.505681 -0.087077 0.286735 -1.156598 -0.439829 -0.622217 -0.550167 0.422738 -0.683814 -0.238375 -0.324796 -0.758366 0.521420 0.529187 0.154021 -0.161270 0.479264 0.178278 -0.509510 0.091867 -0.896682 0.324824 0.816709 0.333964 -0.650943 -0.616890 -0.301834 0.371225 -0.259088 -0.275618 -0.176697 -0.252169 -0.483184 -0.584330 -0.788613 0.749654 -0.018505 0.061187 -0.241788 0.624982 0.004083 0.210626 0.330245 0.410551 -0.144262 1.502831 0.546785 -0.538215 0.246879 -0.991346 0.389814 0.402982 0.885414 -0.501270 0.666276 -0.176685 0.104880 -0.956000 -0.122641 -0.451527 0.136869 0.150519 0.140016 -0.101773 -0.290299 -0.594228 0.386442 -0.908854 0.012999 0.476659 0.069642 -0.190952 0.332343 -0.986069 0.014537 0.120858 -0.552990 -0.312042 0.504993 0.673219 0.051445 0.048852 0.471269 0.108052 0.660918 -0.187508 -1.052384 0.152642 -0.009757 0.275520 0.045458 -0.448910 -0.691880 -0.125778 0.324760 0.459488 0.150728 0.524402 -0.021063 -0.528313 0.149925 -0.357414 -0.409712 0.297917 0.961010 0.479341 -0.593689 0.165485 -0.591986 0.653932 0.494358 0.039629 -0.301842 -0.193943 -0.503415 0.068739 -0.534380 0.016833 -0.607677 -0.646529 -0.039814 0.209330 -0.678882 0.523895 0.571348 0.445341 -0.130810 -1.059592 -0.196109 -0.413939 0.306357 0.817313 0.330648 -0.634728 -0.465202 0.644608 0.542919 -0.064968 0.109089 0.198790 0.124047 0.968307 -0.212897 -0.172950 -0.297858 -0.401999 0.440827 0.226069 0.288316 -0.437549 0.356632 -0.950509 0.178145 0.078553 0.570451 -0.196760 -0.450654 0.043338 0.471152 0.542902 0.297477 0.346947 -0.154100 -0.247085 -0.194339 0.362456 0.628983 -0.018035 0.285309 -0.344576 0.083930 0.426853 -0.822308 0.130481 -0.624868 -0.259182 0.499301\n",
      "two 0.463906 0.597862 -0.314112 -0.322236 -0.224376 0.266306 -0.589130 0.235644 -0.452495 0.078001 -0.250286 0.516834 -0.224941 0.521864 0.420178 0.491212 0.600122 0.405252 -0.213882 -0.376174 -0.245173 -0.149893 -0.246608 0.241082 0.670710 0.598146 0.279116 -0.995276 -0.582730 -0.262887 -0.370024 -0.040683 0.332936 -0.640990 0.632992 0.051648 0.209198 0.035966 -0.075554 0.057707 0.192103 0.414297 -0.718897 -1.152614 -1.207481 0.699811 0.310668 -0.480372 -0.241439 0.128588 -0.760340 1.163423 -0.087802 0.817528 -0.711928 -0.068328 -0.538701 -0.548496 0.917397 -0.398100 -0.351589 -0.072704 0.393500 -0.055031 -0.651656 -0.075134 0.360823 -0.094672 0.385896 0.411003 0.300914 -0.164633 -0.575983 0.971020 -0.075970 0.699486 -0.403874 -0.338084 0.423740 -0.202741 -0.030561 -0.120555 -0.143201 -0.066270 -0.701390 -0.742150 0.338286 0.599856 0.181355 -0.744589 -0.947789 -0.499662 0.509234 -0.279892 -0.150375 0.266210 0.021067 -0.367525 0.411867 -0.300453 0.006104 0.430203 0.214733 0.175784 0.164095 -0.362350 -1.305787 -0.183735 -0.027598 -0.442977 -0.039946 -0.385630 -0.478226 0.895243 -0.027412 1.219657 0.117358 0.820019 -0.866839 -0.059297 -0.190564 -0.514437 0.378669 -0.427080 -0.172651 -0.015579 -1.408271 -0.223503 -0.456980 0.300103 0.951858 -0.157857 -0.660855 0.133185 -0.385405 0.355275 0.354321 0.085651 0.307199 0.407441 -0.092160 -0.757374 -0.107767 -0.596636 0.494907 0.430620 0.722583 -0.688561 -0.452432 -0.288065 -0.279032 -0.643661 0.002945 -0.093671 -0.463398 -1.274779 -0.720330 -1.591237 1.056253 0.195530 0.507753 0.144600 0.214361 -0.052278 0.247844 0.251528 -0.391188 0.283094 1.337737 0.557803 -0.543016 -0.017088 -0.939358 0.389197 0.514801 0.814862 0.260433 0.427263 -0.025425 0.061563 -0.963300 -0.102206 -0.051380 0.023278 0.447689 0.063581 -0.225346 -0.167762 -0.307801 0.074500 -0.771961 0.043632 0.401213 0.180106 -0.359242 0.252806 -0.781236 -0.500097 -0.016476 -0.353185 -0.249356 0.272056 0.879041 0.181343 0.311988 0.079118 0.285875 0.613889 -0.222600 -0.728660 0.162527 -0.319289 -0.577122 -0.663436 -1.188921 -1.270354 -0.512542 0.123952 0.301156 0.176240 0.648069 -0.313061 -0.551510 -0.063377 0.118941 -0.118079 0.184198 0.928841 0.321425 -0.493009 0.614164 -0.624453 0.189773 0.276388 -0.452969 -0.377773 -0.635019 -0.433001 0.148805 -0.416131 0.294550 -0.497978 -0.043932 -0.106650 -0.078414 -0.111831 0.179572 0.407706 0.427028 -0.420458 -0.946350 -0.240260 0.146625 -0.087906 0.696199 -0.055918 -0.205861 -0.860862 0.528706 0.119137 -0.355639 -0.047494 0.167002 0.590725 0.379171 0.044415 -0.505759 -0.905331 -0.729748 0.240794 -0.539124 0.517896 -0.124957 -0.165620 -0.567830 0.519132 0.312424 0.505341 -0.435017 -0.174662 0.179484 0.223363 0.219435 -0.032824 0.476426 0.084132 0.372156 -0.208623 0.522626 0.954866 0.268487 0.557766 -0.465929 0.355604 0.794504 -0.710937 0.268861 -0.731075 -0.355506 -0.422378\n",
      "\n",
      "================================================================================\n",
      "\n",
      "MODEL.BIN (first 64 bytes)\n",
      "b'302815 300\\nalso \\xda<\\x06?\\xde\\x8e\\x18?\\x88f\\x1e\\xbf\\x97\\x8f\\x1c\\xbf\\x9bu&\\xbf\\x1b\\x9er?M\\x81\\xf4\\xbefI`>\\x81&\\x8c\\xbf\\x81\\x97y>\\xc2\\xbd\\xda\\xbeg\\xf25?'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "BASE_PATH = r\"data/glove\"\n",
    "\n",
    "# meta.json\n",
    "with open(f\"{BASE_PATH}/meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "print(\"META.JSON\")\n",
    "print(meta)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# model.txt (prime 5 righe)\n",
    "print(\"MODEL.TXT (first 5 lines)\")\n",
    "with open(f\"{BASE_PATH}/model.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i, line in zip(range(5), f):\n",
    "        print(line.rstrip())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# model.bin (prime 64 bytes)\n",
    "print(\"MODEL.BIN (first 64 bytes)\")\n",
    "with open(f\"{BASE_PATH}/model.bin\", \"rb\") as f:\n",
    "    raw = f.read(64)\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7ae27f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting setup...\n",
      "...done. device = cuda | Elapsed: 0.00s\n",
      "starting dataset split...\n",
      "...done. train=6152 val=683 | Elapsed: 0.00s\n",
      "starting vocabulary build...\n",
      "...done. vocab_size=17391 | Elapsed: 0.02s\n",
      "starting GloVe load (filtered)...\n",
      "...done. glove_hits=8338 | Elapsed: 3.17s\n",
      "starting embedding matrix build...\n",
      "...done. coverage=0.4794 | Elapsed: 0.09s\n",
      "starting dataset and dataloaders...\n",
      "...done. | Elapsed: 0.00s\n",
      "starting model init...\n",
      "...done. | Elapsed: 0.03s\n",
      "starting training loop...\n",
      "epoch 1 training...\n",
      "...done. | Elapsed: 0.29s\n",
      "epoch 1 validation...\n",
      "...done. val_acc=0.6018 | Elapsed: 0.02s\n",
      "epoch 2 training...\n",
      "...done. | Elapsed: 0.17s\n",
      "epoch 2 validation...\n",
      "...done. val_acc=0.6018 | Elapsed: 0.02s\n",
      "training completed.\n"
     ]
    }
   ],
   "source": [
    "import os, io, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting setup...\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"...done. device = {device} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"target\"\n",
    "GLOVE_TXT = r\"data/glove/model.txt\"\n",
    "EMB_DIM = 300\n",
    "\n",
    "MAX_VOCAB = 60000\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def basic_tokenize(s):\n",
    "    return str(s).lower().strip().split()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting dataset split...\")\n",
    "df = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "idx = np.arange(len(df)); np.random.shuffle(idx)\n",
    "val_n = int(len(df) * VAL_FRAC)\n",
    "val_idx = idx[:val_n]; trn_idx = idx[val_n:]\n",
    "trn_df = df.iloc[trn_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "print(f\"...done. train={len(trn_df)} val={len(val_df)} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting vocabulary build...\")\n",
    "counter = Counter()\n",
    "for t in trn_df[TEXT_COL]:\n",
    "    counter.update(basic_tokenize(t))\n",
    "itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "for w, _ in counter.most_common():\n",
    "    if len(itos) >= MAX_VOCAB:\n",
    "        break\n",
    "    itos.append(w)\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "print(f\"...done. vocab_size={len(stoi)} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting GloVe load (filtered)...\")\n",
    "glove_vecs = {}\n",
    "with io.open(GLOVE_TXT, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\") as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) != EMB_DIM + 1:\n",
    "            continue\n",
    "        w = parts[0]\n",
    "        if w in stoi:\n",
    "            glove_vecs[w] = np.asarray(parts[1:], dtype=np.float32)\n",
    "print(f\"...done. glove_hits={len(glove_vecs)} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting embedding matrix build...\")\n",
    "vocab_size = len(stoi)\n",
    "emb_matrix = np.random.normal(0, 0.05, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "emb_matrix[stoi[PAD_TOKEN]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "hit = 0\n",
    "for w, i in stoi.items():\n",
    "    if w in glove_vecs:\n",
    "        emb_matrix[i] = glove_vecs[w]\n",
    "        hit += 1\n",
    "print(f\"...done. coverage={hit / vocab_size:.4f} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting dataset and dataloaders...\")\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        toks = basic_tokenize(self.df.loc[idx, TEXT_COL])[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        if len(ids) < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - len(ids))\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TextDataset(trn_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "    persistent_workers=(NUM_WORKERS > 0)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TextDataset(val_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "    persistent_workers=(NUM_WORKERS > 0)\n",
    ")\n",
    "print(f\"...done. | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting model init...\")\n",
    "class VanillaRNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.rnn = nn.RNN(EMB_DIM, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(self.embedding(x))\n",
    "        return self.fc(out[:, -1, :]).squeeze(1)\n",
    "\n",
    "model = VanillaRNNClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(f\"...done. | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return ((torch.sigmoid(logits) >= 0.5).float() == y).float().mean()\n",
    "\n",
    "EPOCHS = 2\n",
    "print(\"starting training loop...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    print(f\"epoch {epoch} training...\")\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"...done. | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"epoch {epoch} validation...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accs = []\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            accs.append(float(accuracy(model(x), y)))\n",
    "    print(f\"...done. val_acc={np.mean(accs):.4f} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "print(\"training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdaf9bc",
   "metadata": {},
   "source": [
    "#### 3.2.2 Vanilla RNN Results\n",
    "\n",
    "The Vanilla RNN classifier was evaluated on an internal train–validation split of the dataset, as the official test set does not provide ground-truth labels. Using this split, the model reached a validation accuracy of **0.6018 at the second epoch**, with no further improvement observed between the first and the second epoch.\n",
    "\n",
    "This behavior is expected for a basic Vanilla RNN architecture applied to short, noisy textual data such as tweets. The rapid convergence and early saturation indicate that the model is able to capture some sequential information, but its representational capacity is limited. These results therefore serve as a reliable baseline, against which more expressive recurrent models will be compared in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a4add",
   "metadata": {},
   "source": [
    "### 3.3 Gated Recurrent Unit (GRU) Classifier\n",
    "\n",
    "The Gated Recurrent Unit (GRU) is an evolution of the standard recurrent neural network architecture designed to address some of the intrinsic limitations of Vanilla RNNs, in particular the difficulty in modeling long-range dependencies due to vanishing and exploding gradients. To mitigate these issues, GRUs introduce a gating mechanism that explicitly regulates the flow of information across time steps.\n",
    "\n",
    "Specifically, the GRU employs two gates: an update gate, which controls how much of the previous hidden state should be retained, and a reset gate, which determines how strongly past information should be forgotten when incorporating new input. This structure allows the network to preserve relevant contextual information over longer sequences while remaining computationally more efficient than more complex gated architectures such as LSTMs.\n",
    "\n",
    "Compared to a Vanilla RNN, a GRU is expected to provide a more stable training process and a richer representation of sequential patterns in text. In the context of tweet classification, this translates into a better ability to capture dependencies across tokens and to filter out noise, which is particularly relevant given the informal and heterogeneous nature of the data.\n",
    "\n",
    "For these reasons, the GRU architecture is expected to outperform the Vanilla RNN baseline on the same train–validation split, achieving higher validation accuracy while maintaining similar training efficiency. Quantitative results and comparative analysis are presented in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c4891b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting setup...\n",
      "...done. device=cuda | Elapsed: 0.00s\n",
      "starting dataset split...\n",
      "...done. train=6152 val=683 | Elapsed: 0.00s\n",
      "starting vocabulary build...\n",
      "...done. vocab_size=17391 | Elapsed: 0.02s\n",
      "starting GloVe load (filtered)...\n",
      "...done. glove_hits=8338 | Elapsed: 3.04s\n",
      "starting embedding matrix build...\n",
      "...done. coverage=0.4794 | Elapsed: 0.10s\n",
      "starting GRU dataloaders...\n",
      "...done GRU dataloaders. | Elapsed: 0.00s\n",
      "starting GRU model init...\n",
      "...done GRU model init. | Elapsed: 0.02s\n",
      "starting GRU training loop...\n",
      "epoch 1 training...\n",
      "...done. | Elapsed: 0.34s\n",
      "epoch 1 validation...\n",
      "...done. val_loss=0.5355 val_acc=0.7716 | Elapsed: 0.02s\n",
      "epoch 2 training...\n",
      "...done. | Elapsed: 0.29s\n",
      "epoch 2 validation...\n",
      "...done. val_loss=0.4622 val_acc=0.7918 | Elapsed: 0.02s\n",
      "GRU training completed.\n"
     ]
    }
   ],
   "source": [
    "import io, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting setup...\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"target\"\n",
    "\n",
    "GLOVE_TXT = r\"data/glove/model.txt\"\n",
    "EMB_DIM = 300\n",
    "\n",
    "MAX_VOCAB = 60000\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "print(f\"...done. device={device} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "def basic_tokenize(s):\n",
    "    return str(s).lower().strip().split()\n",
    "\n",
    "def tprint(msg, t0):\n",
    "    print(f\"{msg} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting dataset split...\")\n",
    "df = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "idx = np.arange(len(df)); np.random.shuffle(idx)\n",
    "val_n = int(len(df) * VAL_FRAC)\n",
    "val_idx = idx[:val_n]\n",
    "trn_idx = idx[val_n:]\n",
    "trn_df = df.iloc[trn_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "print(f\"...done. train={len(trn_df)} val={len(val_df)} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting vocabulary build...\")\n",
    "counter = Counter()\n",
    "for t in trn_df[TEXT_COL]:\n",
    "    counter.update(basic_tokenize(t))\n",
    "itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "for w, _ in counter.most_common():\n",
    "    if len(itos) >= MAX_VOCAB:\n",
    "        break\n",
    "    itos.append(w)\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab_size = len(stoi)\n",
    "print(f\"...done. vocab_size={vocab_size} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting GloVe load (filtered)...\")\n",
    "glove_vecs = {}\n",
    "with io.open(GLOVE_TXT, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\") as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) != EMB_DIM + 1:\n",
    "            continue\n",
    "        w = parts[0]\n",
    "        if w in stoi:\n",
    "            glove_vecs[w] = np.asarray(parts[1:], dtype=np.float32)\n",
    "print(f\"...done. glove_hits={len(glove_vecs)} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting embedding matrix build...\")\n",
    "emb_matrix = np.random.normal(0, 0.05, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "emb_matrix[stoi[PAD_TOKEN]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "hit = 0\n",
    "for w, i in stoi.items():\n",
    "    v = glove_vecs.get(w)\n",
    "    if v is not None:\n",
    "        emb_matrix[i] = v\n",
    "        hit += 1\n",
    "print(f\"...done. coverage={hit / vocab_size:.4f} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting GRU dataloaders...\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        toks = basic_tokenize(self.df.loc[idx, TEXT_COL])[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        length = len(ids)\n",
    "        if length < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        return x, y, torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys, ls = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.stack(ys, dim=0)\n",
    "    l = torch.stack(ls, dim=0)\n",
    "    return x, y, l\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TextDataset(trn_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TextDataset(val_df),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "    drop_last=False\n",
    ")\n",
    "tprint(\"...done GRU dataloaders.\", t0)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting GRU model init...\")\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim=128, pad_idx=0, emb_weights=None, freeze_emb=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        if emb_weights is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(emb_weights))\n",
    "        self.embedding.weight.requires_grad = (not freeze_emb)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h_n = self.gru(packed)\n",
    "        last = h_n[-1]\n",
    "        return self.fc(last).squeeze(1)\n",
    "\n",
    "\n",
    "model_gru = GRUClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=128,\n",
    "    pad_idx=stoi[PAD_TOKEN],\n",
    "    emb_weights=emb_matrix,\n",
    "    freeze_emb=False\n",
    ").to(device)\n",
    "\n",
    "GLOBAL_TRESHOLD = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
    "tprint(\"...done GRU model init.\", t0)\n",
    "\n",
    "\n",
    "def accuracy_from_logits(logits, y, GLOBAL_TRESHOLD):\n",
    "    preds = (torch.sigmoid(logits) >= GLOBAL_TRESHOLD).float()\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    tot_loss = tot_acc = 0.0\n",
    "    n = 0\n",
    "    for x, y, lenghts in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x, lenghts)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot_loss += float(loss.detach().cpu())\n",
    "        tot_acc += float(accuracy_from_logits(logits.detach(), y, GLOBAL_TRESHOLD).detach().cpu())\n",
    "        n += 1\n",
    "    return tot_loss / max(n, 1), tot_acc / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for x, y, lengths in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x,lengths)\n",
    "        losses.append(float(criterion(logits, y).detach().cpu()))\n",
    "        accs.append(float(accuracy_from_logits(logits, y,GLOBAL_TRESHOLD).detach().cpu()))\n",
    "    return float(np.mean(losses)), float(np.mean(accs))\n",
    "\n",
    "EPOCHS = 2\n",
    "print(\"starting GRU training loop...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    print(f\"epoch {epoch} training...\")\n",
    "    tr_loss, tr_acc = train_one_epoch(model_gru, train_loader)\n",
    "    tprint(\"...done.\", t0)\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(f\"epoch {epoch} validation...\")\n",
    "    va_loss, va_acc = evaluate(model_gru, val_loader)\n",
    "    print(f\"...done. val_loss={va_loss:.4f} val_acc={va_acc:.4f} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "print(\"GRU training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61a4a8",
   "metadata": {},
   "source": [
    "#### 3.3.2 GRU Results\n",
    "\n",
    "The GRU classifier was evaluated on the same train–validation split and under the same non-tuned conditions adopted for the Vanilla RNN baseline. No hyperparameter optimization or architectural refinements were introduced at this stage, in order to ensure a fair and transparent comparison.\n",
    "\n",
    "Under these conditions, the GRU achieved a validation accuracy of **0.7716 after the first epoch**, improving further to **0.7918 at the second epoch**, with a corresponding and consistent decrease in validation loss. This represents a substantial improvement over the Vanilla RNN baseline, confirming the expected advantages of gated recurrent architectures in modeling sequential textual data.\n",
    "\n",
    "These results demonstrate that, even without tuning, the GRU is able to extract more informative representations from the input text. Further refinements and performance-oriented adjustments are intentionally deferred to Section 4, where the model will be systematically optimized to maximize classification performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a40747",
   "metadata": {},
   "source": [
    "## 4. Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4db52",
   "metadata": {},
   "source": [
    "### 4.1 Hyperparameter Tuning\n",
    "\n",
    "The following hyperparameters are explored due to their direct impact on training dynamics and model generalization:\n",
    "\n",
    "* **Batch size:** affects the stability and noise of gradient updates, with smaller batches introducing more stochasticity and larger batches providing more stable optimization.\n",
    "\n",
    "* **Learning rate:** controls the magnitude of weight updates and strongly influences convergence speed and stability.\n",
    "\n",
    "* **Number of epochs:** determines training duration and is adjusted to balance underfitting and overfitting.\n",
    "\n",
    "* **Early stopping:** halts training when validation performance plateaus, acting as a regularization mechanism.\n",
    "\n",
    "* **Different seeds:** multiple runs are performed with different random seeds to vary weight initialization and data splits, allowing evaluation of performance consistency rather than relying on a single run, which is especially important given the lack of ground truth labels in the Kaggle test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c77dda",
   "metadata": {},
   "source": [
    "### 4.2 RNN Best Configuration and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0fa54264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting setup...\n",
      "...done. device=cuda | Elapsed: 0.00s\n",
      "starting dataset split...\n",
      "...done. train=6152 val=683\n",
      "split completed. | Elapsed: 0.00s\n",
      "starting vocabulary build...\n",
      "...done. vocab_size=17391\n",
      "vocab completed. | Elapsed: 0.02s\n",
      "starting GloVe load (filtered)...\n",
      "...done. glove_hits=8338\n",
      "glove completed. | Elapsed: 3.20s\n",
      "starting embedding matrix build...\n",
      "...done. coverage=0.4794\n",
      "embedding matrix completed. | Elapsed: 0.10s\n",
      "starting RNN hyperparameter exploration...\n",
      "\n",
      "GRID RUN 1/36\n",
      "run start | seed=42 batch=32 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5345 acc=0.7497 | Elapsed: 0.90s\n",
      "epoch 1 val   | loss=0.4630 acc=0.7912 | Elapsed: 0.05s\n",
      "new best: val_acc=0.7912 at epoch 1 (val_loss=0.4630)\n",
      "epoch 2 train | loss=0.3648 acc=0.8449 | Elapsed: 0.75s\n",
      "epoch 2 val   | loss=0.4919 acc=0.8082 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8082 at epoch 2 (val_loss=0.4919)\n",
      "epoch 3 train | loss=0.1716 acc=0.9377 | Elapsed: 0.76s\n",
      "epoch 3 val   | loss=0.6134 acc=0.7572 | Elapsed: 0.05s\n",
      "epoch 4 train | loss=0.0704 acc=0.9744 | Elapsed: 0.71s\n",
      "epoch 4 val   | loss=0.6825 acc=0.7714 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0321 acc=0.9898 | Elapsed: 0.67s\n",
      "epoch 5 val   | loss=0.8998 acc=0.7331 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.0205 acc=0.9947 | Elapsed: 0.67s\n",
      "epoch 6 val   | loss=0.9583 acc=0.7629 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.0258 acc=0.9914 | Elapsed: 0.67s\n",
      "epoch 7 val   | loss=0.9782 acc=0.7670 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0256 acc=0.9917 | Elapsed: 0.66s\n",
      "epoch 8 val   | loss=1.0820 acc=0.7501 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0079 acc=0.9976 | Elapsed: 0.67s\n",
      "epoch 9 val   | loss=1.2253 acc=0.7500 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0193 acc=0.9932 | Elapsed: 0.67s\n",
      "epoch 10 val   | loss=1.0911 acc=0.7616 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0280 acc=0.9893 | Elapsed: 0.67s\n",
      "epoch 11 val   | loss=1.1370 acc=0.7331 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0095 acc=0.9969 | Elapsed: 0.69s\n",
      "epoch 12 val   | loss=1.2431 acc=0.7558 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0071 acc=0.9976 | Elapsed: 0.75s\n",
      "epoch 13 val   | loss=1.2672 acc=0.7473 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0027 acc=0.9990 | Elapsed: 0.74s\n",
      "epoch 14 val   | loss=1.2958 acc=0.7388 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0020 acc=0.9995 | Elapsed: 0.74s\n",
      "epoch 15 val   | loss=1.4864 acc=0.7642 | Elapsed: 0.05s\n",
      "epoch 16 train | loss=0.0020 acc=0.9992 | Elapsed: 0.75s\n",
      "epoch 16 val   | loss=1.4013 acc=0.7642 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0012 acc=0.9998 | Elapsed: 0.74s\n",
      "epoch 17 val   | loss=1.4037 acc=0.7359 | Elapsed: 0.05s\n",
      "epoch 18 train | loss=0.0008 acc=0.9997 | Elapsed: 0.72s\n",
      "epoch 18 val   | loss=1.3897 acc=0.7501 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0010 acc=0.9997 | Elapsed: 0.72s\n",
      "epoch 19 val   | loss=1.5175 acc=0.7557 | Elapsed: 0.05s\n",
      "epoch 20 train | loss=0.0223 acc=0.9938 | Elapsed: 0.71s\n",
      "epoch 20 val   | loss=1.5074 acc=0.6096 | Elapsed: 0.04s\n",
      "run end | best_val_acc=0.8082 best_val_loss=0.4919 best_epoch=2 elapsed=15.11s\n",
      "\n",
      "GRID RUN 2/36\n",
      "run start | seed=42 batch=32 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5468 acc=0.7341 | Elapsed: 0.69s\n",
      "epoch 1 val   | loss=0.4448 acc=0.7955 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7955 at epoch 1 (val_loss=0.4448)\n",
      "epoch 2 train | loss=0.4085 acc=0.8284 | Elapsed: 0.68s\n",
      "epoch 2 val   | loss=0.4571 acc=0.7955 | Elapsed: 0.03s\n",
      "epoch 3 train | loss=0.2789 acc=0.8915 | Elapsed: 0.68s\n",
      "epoch 3 val   | loss=0.4954 acc=0.7912 | Elapsed: 0.04s\n",
      "epoch 4 train | loss=0.1473 acc=0.9496 | Elapsed: 0.69s\n",
      "epoch 4 val   | loss=0.5628 acc=0.7784 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0660 acc=0.9793 | Elapsed: 0.67s\n",
      "epoch 5 val   | loss=0.7118 acc=0.7372 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0378 acc=0.9893 | Elapsed: 0.67s\n",
      "epoch 6 val   | loss=0.7670 acc=0.7770 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0199 acc=0.9937 | Elapsed: 0.68s\n",
      "epoch 7 val   | loss=0.8874 acc=0.7997 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7997 at epoch 7 (val_loss=0.8874)\n",
      "epoch 8 train | loss=0.0139 acc=0.9971 | Elapsed: 0.74s\n",
      "epoch 8 val   | loss=0.8868 acc=0.7884 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0122 acc=0.9964 | Elapsed: 0.75s\n",
      "epoch 9 val   | loss=0.9083 acc=0.7628 | Elapsed: 0.05s\n",
      "epoch 10 train | loss=0.0160 acc=0.9951 | Elapsed: 0.75s\n",
      "epoch 10 val   | loss=0.9528 acc=0.7585 | Elapsed: 0.05s\n",
      "epoch 11 train | loss=0.0106 acc=0.9966 | Elapsed: 0.75s\n",
      "epoch 11 val   | loss=1.0418 acc=0.7500 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0068 acc=0.9976 | Elapsed: 0.76s\n",
      "epoch 12 val   | loss=1.0935 acc=0.7514 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0138 acc=0.9961 | Elapsed: 0.76s\n",
      "epoch 13 val   | loss=1.1054 acc=0.7656 | Elapsed: 0.05s\n",
      "epoch 14 train | loss=0.0046 acc=0.9985 | Elapsed: 0.75s\n",
      "epoch 14 val   | loss=1.1043 acc=0.7443 | Elapsed: 0.05s\n",
      "epoch 15 train | loss=0.0061 acc=0.9985 | Elapsed: 0.75s\n",
      "epoch 15 val   | loss=1.1767 acc=0.7642 | Elapsed: 0.05s\n",
      "epoch 16 train | loss=0.0075 acc=0.9982 | Elapsed: 0.73s\n",
      "epoch 16 val   | loss=1.0055 acc=0.7459 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0028 acc=0.9995 | Elapsed: 0.69s\n",
      "epoch 17 val   | loss=1.1740 acc=0.7571 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0024 acc=0.9995 | Elapsed: 0.68s\n",
      "epoch 18 val   | loss=1.3264 acc=0.7628 | Elapsed: 0.04s\n",
      "epoch 19 train | loss=0.0021 acc=0.9995 | Elapsed: 0.70s\n",
      "epoch 19 val   | loss=1.1431 acc=0.7585 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0010 acc=0.9997 | Elapsed: 0.69s\n",
      "epoch 20 val   | loss=1.1727 acc=0.7543 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7997 best_val_loss=0.8874 best_epoch=7 elapsed=15.01s\n",
      "\n",
      "GRID RUN 3/36\n",
      "run start | seed=42 batch=32 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6306 acc=0.6490 | Elapsed: 0.69s\n",
      "epoch 1 val   | loss=0.4910 acc=0.7884 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7884 at epoch 1 (val_loss=0.4910)\n",
      "epoch 2 train | loss=0.4982 acc=0.7766 | Elapsed: 0.67s\n",
      "epoch 2 val   | loss=0.4556 acc=0.8026 | Elapsed: 0.04s\n",
      "new best: val_acc=0.8026 at epoch 2 (val_loss=0.4556)\n",
      "epoch 3 train | loss=0.4519 acc=0.8046 | Elapsed: 0.65s\n",
      "epoch 3 val   | loss=0.4404 acc=0.8111 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8111 at epoch 3 (val_loss=0.4404)\n",
      "epoch 4 train | loss=0.4086 acc=0.8279 | Elapsed: 0.66s\n",
      "epoch 4 val   | loss=0.4374 acc=0.8097 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.3577 acc=0.8546 | Elapsed: 0.69s\n",
      "epoch 5 val   | loss=0.4488 acc=0.7869 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.3090 acc=0.8802 | Elapsed: 0.69s\n",
      "epoch 6 val   | loss=0.4735 acc=0.7828 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.2540 acc=0.9028 | Elapsed: 0.71s\n",
      "epoch 7 val   | loss=0.5146 acc=0.7842 | Elapsed: 0.05s\n",
      "epoch 8 train | loss=0.2072 acc=0.9267 | Elapsed: 0.69s\n",
      "epoch 8 val   | loss=0.5464 acc=0.7771 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.1649 acc=0.9420 | Elapsed: 0.70s\n",
      "epoch 9 val   | loss=0.5573 acc=0.7743 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.1305 acc=0.9579 | Elapsed: 0.69s\n",
      "epoch 10 val   | loss=0.6909 acc=0.7770 | Elapsed: 0.05s\n",
      "epoch 11 train | loss=0.1044 acc=0.9671 | Elapsed: 0.70s\n",
      "epoch 11 val   | loss=0.7234 acc=0.7473 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0861 acc=0.9713 | Elapsed: 0.68s\n",
      "epoch 12 val   | loss=0.7452 acc=0.7459 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0715 acc=0.9785 | Elapsed: 0.67s\n",
      "epoch 13 val   | loss=0.7208 acc=0.7601 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0582 acc=0.9832 | Elapsed: 0.67s\n",
      "epoch 14 val   | loss=0.8007 acc=0.7543 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0466 acc=0.9867 | Elapsed: 0.67s\n",
      "epoch 15 val   | loss=0.9467 acc=0.7402 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0463 acc=0.9861 | Elapsed: 0.68s\n",
      "epoch 16 val   | loss=0.8803 acc=0.7386 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0366 acc=0.9890 | Elapsed: 0.67s\n",
      "epoch 17 val   | loss=0.8836 acc=0.7599 | Elapsed: 0.04s\n",
      "epoch 18 train | loss=0.0293 acc=0.9924 | Elapsed: 0.67s\n",
      "epoch 18 val   | loss=0.8729 acc=0.7557 | Elapsed: 0.04s\n",
      "epoch 19 train | loss=0.0260 acc=0.9935 | Elapsed: 0.67s\n",
      "epoch 19 val   | loss=0.9335 acc=0.7571 | Elapsed: 0.04s\n",
      "epoch 20 train | loss=0.0229 acc=0.9943 | Elapsed: 0.72s\n",
      "epoch 20 val   | loss=1.0313 acc=0.7699 | Elapsed: 0.05s\n",
      "run end | best_val_acc=0.8111 best_val_loss=0.4404 best_epoch=3 elapsed=14.41s\n",
      "\n",
      "GRID RUN 4/36\n",
      "run start | seed=42 batch=64 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5474 acc=0.7386 | Elapsed: 0.48s\n",
      "epoch 1 val   | loss=0.4579 acc=0.7913 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7913 at epoch 1 (val_loss=0.4579)\n",
      "epoch 2 train | loss=0.3935 acc=0.8363 | Elapsed: 0.46s\n",
      "epoch 2 val   | loss=0.4891 acc=0.7679 | Elapsed: 0.03s\n",
      "epoch 3 train | loss=0.2216 acc=0.9159 | Elapsed: 0.47s\n",
      "epoch 3 val   | loss=0.5590 acc=0.7849 | Elapsed: 0.03s\n",
      "epoch 4 train | loss=0.0895 acc=0.9702 | Elapsed: 0.48s\n",
      "epoch 4 val   | loss=0.6461 acc=0.7807 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.0363 acc=0.9892 | Elapsed: 0.48s\n",
      "epoch 5 val   | loss=0.8090 acc=0.7416 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.0227 acc=0.9948 | Elapsed: 0.47s\n",
      "epoch 6 val   | loss=0.9376 acc=0.7579 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0134 acc=0.9965 | Elapsed: 0.47s\n",
      "epoch 7 val   | loss=1.0284 acc=0.7813 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0134 acc=0.9966 | Elapsed: 0.46s\n",
      "epoch 8 val   | loss=1.0368 acc=0.7650 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0258 acc=0.9913 | Elapsed: 0.48s\n",
      "epoch 9 val   | loss=1.0201 acc=0.7380 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0088 acc=0.9974 | Elapsed: 0.46s\n",
      "epoch 10 val   | loss=1.1012 acc=0.7671 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0353 acc=0.9897 | Elapsed: 0.47s\n",
      "epoch 11 val   | loss=1.1752 acc=0.7409 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0196 acc=0.9934 | Elapsed: 0.45s\n",
      "epoch 12 val   | loss=1.1266 acc=0.7452 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0128 acc=0.9958 | Elapsed: 0.43s\n",
      "epoch 13 val   | loss=1.1371 acc=0.7587 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0047 acc=0.9982 | Elapsed: 0.42s\n",
      "epoch 14 val   | loss=1.2423 acc=0.7608 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0049 acc=0.9981 | Elapsed: 0.43s\n",
      "epoch 15 val   | loss=1.2682 acc=0.7651 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0045 acc=0.9982 | Elapsed: 0.46s\n",
      "epoch 16 val   | loss=1.2939 acc=0.7813 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0034 acc=0.9989 | Elapsed: 0.48s\n",
      "epoch 17 val   | loss=1.2783 acc=0.7580 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0033 acc=0.9989 | Elapsed: 0.47s\n",
      "epoch 18 val   | loss=1.3255 acc=0.7523 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0030 acc=0.9990 | Elapsed: 0.46s\n",
      "epoch 19 val   | loss=1.3784 acc=0.7622 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0031 acc=0.9990 | Elapsed: 0.46s\n",
      "epoch 20 val   | loss=1.3753 acc=0.7573 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7913 best_val_loss=0.4579 best_epoch=1 elapsed=9.76s\n",
      "\n",
      "GRID RUN 5/36\n",
      "run start | seed=42 batch=64 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5665 acc=0.7170 | Elapsed: 0.47s\n",
      "epoch 1 val   | loss=0.4569 acc=0.7927 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7927 at epoch 1 (val_loss=0.4569)\n",
      "epoch 2 train | loss=0.4346 acc=0.8098 | Elapsed: 0.45s\n",
      "epoch 2 val   | loss=0.4660 acc=0.7941 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7941 at epoch 2 (val_loss=0.4660)\n",
      "epoch 3 train | loss=0.3307 acc=0.8640 | Elapsed: 0.46s\n",
      "epoch 3 val   | loss=0.4762 acc=0.7750 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.2108 acc=0.9211 | Elapsed: 0.46s\n",
      "epoch 4 val   | loss=0.5427 acc=0.7920 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.1156 acc=0.9626 | Elapsed: 0.48s\n",
      "epoch 5 val   | loss=0.6139 acc=0.7615 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0614 acc=0.9816 | Elapsed: 0.47s\n",
      "epoch 6 val   | loss=0.7149 acc=0.7707 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0431 acc=0.9865 | Elapsed: 0.48s\n",
      "epoch 7 val   | loss=0.8042 acc=0.7792 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0200 acc=0.9947 | Elapsed: 0.47s\n",
      "epoch 8 val   | loss=0.8542 acc=0.7707 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.0117 acc=0.9963 | Elapsed: 0.46s\n",
      "epoch 9 val   | loss=0.8984 acc=0.7672 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0102 acc=0.9974 | Elapsed: 0.48s\n",
      "epoch 10 val   | loss=1.0254 acc=0.7572 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0107 acc=0.9976 | Elapsed: 0.48s\n",
      "epoch 11 val   | loss=1.1569 acc=0.7310 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0087 acc=0.9977 | Elapsed: 0.48s\n",
      "epoch 12 val   | loss=1.1125 acc=0.7388 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0077 acc=0.9982 | Elapsed: 0.48s\n",
      "epoch 13 val   | loss=1.1447 acc=0.7665 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0073 acc=0.9981 | Elapsed: 0.47s\n",
      "epoch 14 val   | loss=1.1210 acc=0.7771 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0036 acc=0.9994 | Elapsed: 0.47s\n",
      "epoch 15 val   | loss=1.2685 acc=0.7601 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0212 acc=0.9942 | Elapsed: 0.47s\n",
      "epoch 16 val   | loss=1.2804 acc=0.7650 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0133 acc=0.9955 | Elapsed: 0.48s\n",
      "epoch 17 val   | loss=1.0274 acc=0.7551 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0046 acc=0.9987 | Elapsed: 0.44s\n",
      "epoch 18 val   | loss=1.0600 acc=0.7331 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0041 acc=0.9992 | Elapsed: 0.44s\n",
      "epoch 19 val   | loss=1.1798 acc=0.7608 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0039 acc=0.9989 | Elapsed: 0.44s\n",
      "epoch 20 val   | loss=1.1549 acc=0.7381 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7941 best_val_loss=0.4660 best_epoch=2 elapsed=9.84s\n",
      "\n",
      "GRID RUN 6/36\n",
      "run start | seed=42 batch=64 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6676 acc=0.5999 | Elapsed: 0.45s\n",
      "epoch 1 val   | loss=0.6252 acc=0.6678 | Elapsed: 0.03s\n",
      "new best: val_acc=0.6678 at epoch 1 (val_loss=0.6252)\n",
      "epoch 2 train | loss=0.5371 acc=0.7500 | Elapsed: 0.44s\n",
      "epoch 2 val   | loss=0.4755 acc=0.7913 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7913 at epoch 2 (val_loss=0.4755)\n",
      "epoch 3 train | loss=0.4747 acc=0.7917 | Elapsed: 0.43s\n",
      "epoch 3 val   | loss=0.4544 acc=0.8026 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8026 at epoch 3 (val_loss=0.4544)\n",
      "epoch 4 train | loss=0.4408 acc=0.8104 | Elapsed: 0.45s\n",
      "epoch 4 val   | loss=0.4439 acc=0.8062 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8062 at epoch 4 (val_loss=0.4439)\n",
      "epoch 5 train | loss=0.4052 acc=0.8320 | Elapsed: 0.44s\n",
      "epoch 5 val   | loss=0.4405 acc=0.8019 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.3705 acc=0.8486 | Elapsed: 0.44s\n",
      "epoch 6 val   | loss=0.4485 acc=0.7991 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.3260 acc=0.8710 | Elapsed: 0.44s\n",
      "epoch 7 val   | loss=0.4767 acc=0.7913 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.2886 acc=0.8898 | Elapsed: 0.44s\n",
      "epoch 8 val   | loss=0.4902 acc=0.7906 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.2451 acc=0.9082 | Elapsed: 0.45s\n",
      "epoch 9 val   | loss=0.5087 acc=0.7835 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.2053 acc=0.9265 | Elapsed: 0.46s\n",
      "epoch 10 val   | loss=0.5707 acc=0.7771 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.1725 acc=0.9394 | Elapsed: 0.47s\n",
      "epoch 11 val   | loss=0.5982 acc=0.7629 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.1412 acc=0.9534 | Elapsed: 0.48s\n",
      "epoch 12 val   | loss=0.6610 acc=0.7459 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.1226 acc=0.9573 | Elapsed: 0.48s\n",
      "epoch 13 val   | loss=0.6394 acc=0.7721 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.1021 acc=0.9694 | Elapsed: 0.48s\n",
      "epoch 14 val   | loss=0.7112 acc=0.7516 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0910 acc=0.9713 | Elapsed: 0.48s\n",
      "epoch 15 val   | loss=0.8029 acc=0.7601 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0792 acc=0.9744 | Elapsed: 0.49s\n",
      "epoch 16 val   | loss=0.7991 acc=0.7296 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0697 acc=0.9786 | Elapsed: 0.48s\n",
      "epoch 17 val   | loss=0.7915 acc=0.7558 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0623 acc=0.9812 | Elapsed: 0.48s\n",
      "epoch 18 val   | loss=0.8915 acc=0.7714 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0507 acc=0.9857 | Elapsed: 0.47s\n",
      "epoch 19 val   | loss=0.8350 acc=0.7622 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0433 acc=0.9884 | Elapsed: 0.47s\n",
      "epoch 20 val   | loss=0.9819 acc=0.7331 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.8062 best_val_loss=0.4439 best_epoch=4 elapsed=9.79s\n",
      "\n",
      "GRID RUN 7/36\n",
      "run start | seed=42 batch=128 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5672 acc=0.7129 | Elapsed: 0.33s\n",
      "epoch 1 val   | loss=0.4599 acc=0.7932 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7932 at epoch 1 (val_loss=0.4599)\n",
      "epoch 2 train | loss=0.4228 acc=0.8151 | Elapsed: 0.31s\n",
      "epoch 2 val   | loss=0.4432 acc=0.8009 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8009 at epoch 2 (val_loss=0.4432)\n",
      "epoch 3 train | loss=0.2791 acc=0.8901 | Elapsed: 0.29s\n",
      "epoch 3 val   | loss=0.4834 acc=0.7880 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.1461 acc=0.9512 | Elapsed: 0.28s\n",
      "epoch 4 val   | loss=0.5795 acc=0.7750 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.0665 acc=0.9799 | Elapsed: 0.28s\n",
      "epoch 5 val   | loss=0.6890 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0306 acc=0.9919 | Elapsed: 0.28s\n",
      "epoch 6 val   | loss=0.8541 acc=0.7789 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0249 acc=0.9935 | Elapsed: 0.28s\n",
      "epoch 7 val   | loss=0.9372 acc=0.7619 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0432 acc=0.9877 | Elapsed: 0.28s\n",
      "epoch 8 val   | loss=0.7895 acc=0.7515 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0119 acc=0.9971 | Elapsed: 0.28s\n",
      "epoch 9 val   | loss=0.9389 acc=0.7555 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0062 acc=0.9986 | Elapsed: 0.28s\n",
      "epoch 10 val   | loss=0.9559 acc=0.7645 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0066 acc=0.9982 | Elapsed: 0.28s\n",
      "epoch 11 val   | loss=0.9654 acc=0.7555 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0089 acc=0.9973 | Elapsed: 0.28s\n",
      "epoch 12 val   | loss=1.0285 acc=0.7593 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0094 acc=0.9976 | Elapsed: 0.28s\n",
      "epoch 13 val   | loss=0.9682 acc=0.7580 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0075 acc=0.9974 | Elapsed: 0.28s\n",
      "epoch 14 val   | loss=1.0216 acc=0.7658 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0054 acc=0.9979 | Elapsed: 0.28s\n",
      "epoch 15 val   | loss=1.1733 acc=0.7723 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0063 acc=0.9979 | Elapsed: 0.28s\n",
      "epoch 16 val   | loss=1.1527 acc=0.7425 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0042 acc=0.9990 | Elapsed: 0.29s\n",
      "epoch 17 val   | loss=1.1378 acc=0.7685 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0031 acc=0.9992 | Elapsed: 0.28s\n",
      "epoch 18 val   | loss=1.5025 acc=0.7489 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0109 acc=0.9963 | Elapsed: 0.28s\n",
      "epoch 19 val   | loss=1.0366 acc=0.7360 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0024 acc=0.9992 | Elapsed: 0.28s\n",
      "epoch 20 val   | loss=1.1772 acc=0.7542 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8009 best_val_loss=0.4432 best_epoch=2 elapsed=6.10s\n",
      "\n",
      "GRID RUN 8/36\n",
      "run start | seed=42 batch=128 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5956 acc=0.6783 | Elapsed: 0.30s\n",
      "epoch 1 val   | loss=0.4692 acc=0.7893 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7893 at epoch 1 (val_loss=0.4692)\n",
      "epoch 2 train | loss=0.4612 acc=0.7962 | Elapsed: 0.29s\n",
      "epoch 2 val   | loss=0.4426 acc=0.8100 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8100 at epoch 2 (val_loss=0.4426)\n",
      "epoch 3 train | loss=0.3760 acc=0.8449 | Elapsed: 0.30s\n",
      "epoch 3 val   | loss=0.4471 acc=0.7958 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.2809 acc=0.8933 | Elapsed: 0.28s\n",
      "epoch 4 val   | loss=0.4764 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.2140 acc=0.9206 | Elapsed: 0.28s\n",
      "epoch 5 val   | loss=0.4970 acc=0.7867 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.1286 acc=0.9582 | Elapsed: 0.29s\n",
      "epoch 6 val   | loss=0.6104 acc=0.7906 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0719 acc=0.9782 | Elapsed: 0.29s\n",
      "epoch 7 val   | loss=0.6863 acc=0.7503 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0446 acc=0.9877 | Elapsed: 0.29s\n",
      "epoch 8 val   | loss=0.7810 acc=0.7646 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0281 acc=0.9919 | Elapsed: 0.29s\n",
      "epoch 9 val   | loss=0.8959 acc=0.7476 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0183 acc=0.9959 | Elapsed: 0.30s\n",
      "epoch 10 val   | loss=0.9284 acc=0.7763 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0176 acc=0.9954 | Elapsed: 0.29s\n",
      "epoch 11 val   | loss=0.9590 acc=0.7477 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0129 acc=0.9963 | Elapsed: 0.28s\n",
      "epoch 12 val   | loss=1.0445 acc=0.7399 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0137 acc=0.9957 | Elapsed: 0.29s\n",
      "epoch 13 val   | loss=1.0454 acc=0.7711 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0143 acc=0.9973 | Elapsed: 0.29s\n",
      "epoch 14 val   | loss=1.0969 acc=0.7607 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0085 acc=0.9978 | Elapsed: 0.28s\n",
      "epoch 15 val   | loss=1.1372 acc=0.7502 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0096 acc=0.9987 | Elapsed: 0.29s\n",
      "epoch 16 val   | loss=1.1112 acc=0.7528 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0122 acc=0.9965 | Elapsed: 0.29s\n",
      "epoch 17 val   | loss=1.0222 acc=0.7659 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0066 acc=0.9982 | Elapsed: 0.28s\n",
      "epoch 18 val   | loss=1.1348 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0039 acc=0.9997 | Elapsed: 0.29s\n",
      "epoch 19 val   | loss=1.2436 acc=0.7555 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0030 acc=0.9994 | Elapsed: 0.29s\n",
      "epoch 20 val   | loss=1.1951 acc=0.7581 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8100 best_val_loss=0.4426 best_epoch=2 elapsed=6.18s\n",
      "\n",
      "GRID RUN 9/36\n",
      "run start | seed=42 batch=128 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6773 acc=0.5848 | Elapsed: 0.30s\n",
      "epoch 1 val   | loss=0.6545 acc=0.6280 | Elapsed: 0.03s\n",
      "new best: val_acc=0.6280 at epoch 1 (val_loss=0.6545)\n",
      "epoch 2 train | loss=0.6296 acc=0.6583 | Elapsed: 0.29s\n",
      "epoch 2 val   | loss=0.5582 acc=0.7516 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7516 at epoch 2 (val_loss=0.5582)\n",
      "epoch 3 train | loss=0.5156 acc=0.7618 | Elapsed: 0.29s\n",
      "epoch 3 val   | loss=0.4720 acc=0.8061 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8061 at epoch 3 (val_loss=0.4720)\n",
      "epoch 4 train | loss=0.4805 acc=0.7814 | Elapsed: 0.28s\n",
      "epoch 4 val   | loss=0.4514 acc=0.8100 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8100 at epoch 4 (val_loss=0.4514)\n",
      "epoch 5 train | loss=0.4509 acc=0.8072 | Elapsed: 0.29s\n",
      "epoch 5 val   | loss=0.4450 acc=0.8061 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.4298 acc=0.8135 | Elapsed: 0.28s\n",
      "epoch 6 val   | loss=0.4350 acc=0.8140 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8140 at epoch 6 (val_loss=0.4350)\n",
      "epoch 7 train | loss=0.4014 acc=0.8313 | Elapsed: 0.29s\n",
      "epoch 7 val   | loss=0.4353 acc=0.8087 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.3688 acc=0.8508 | Elapsed: 0.28s\n",
      "epoch 8 val   | loss=0.4393 acc=0.8049 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.3374 acc=0.8656 | Elapsed: 0.29s\n",
      "epoch 9 val   | loss=0.4459 acc=0.8023 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.3075 acc=0.8809 | Elapsed: 0.28s\n",
      "epoch 10 val   | loss=0.4727 acc=0.8022 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.2757 acc=0.8916 | Elapsed: 0.28s\n",
      "epoch 11 val   | loss=0.4820 acc=0.7996 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.2423 acc=0.9096 | Elapsed: 0.28s\n",
      "epoch 12 val   | loss=0.5086 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.2135 acc=0.9232 | Elapsed: 0.27s\n",
      "epoch 13 val   | loss=0.5324 acc=0.7763 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.1825 acc=0.9353 | Elapsed: 0.28s\n",
      "epoch 14 val   | loss=0.6034 acc=0.7710 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.1621 acc=0.9420 | Elapsed: 0.30s\n",
      "epoch 15 val   | loss=0.6392 acc=0.7750 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.1559 acc=0.9515 | Elapsed: 0.31s\n",
      "epoch 16 val   | loss=0.6510 acc=0.7619 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.1310 acc=0.9562 | Elapsed: 0.31s\n",
      "epoch 17 val   | loss=0.6212 acc=0.7776 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.1110 acc=0.9670 | Elapsed: 0.30s\n",
      "epoch 18 val   | loss=0.6977 acc=0.7750 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0996 acc=0.9678 | Elapsed: 0.31s\n",
      "epoch 19 val   | loss=0.7545 acc=0.7489 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0861 acc=0.9745 | Elapsed: 0.31s\n",
      "epoch 20 val   | loss=0.7611 acc=0.7633 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8140 best_val_loss=0.4350 best_epoch=6 elapsed=6.27s\n",
      "\n",
      "GRID RUN 10/36\n",
      "run start | seed=42 batch=256 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5939 acc=0.6811 | Elapsed: 0.24s\n",
      "epoch 1 val   | loss=0.4915 acc=0.7749 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7749 at epoch 1 (val_loss=0.4915)\n",
      "epoch 2 train | loss=0.4489 acc=0.8020 | Elapsed: 0.24s\n",
      "epoch 2 val   | loss=0.4598 acc=0.7970 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7970 at epoch 2 (val_loss=0.4598)\n",
      "epoch 3 train | loss=0.3549 acc=0.8531 | Elapsed: 0.24s\n",
      "epoch 3 val   | loss=0.4825 acc=0.7873 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.2437 acc=0.9083 | Elapsed: 0.38s\n",
      "epoch 4 val   | loss=0.5124 acc=0.7950 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.1467 acc=0.9442 | Elapsed: 0.24s\n",
      "epoch 5 val   | loss=0.5759 acc=0.7768 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0837 acc=0.9750 | Elapsed: 0.24s\n",
      "epoch 6 val   | loss=0.7173 acc=0.7762 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0460 acc=0.9850 | Elapsed: 0.23s\n",
      "epoch 7 val   | loss=0.7493 acc=0.7658 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0214 acc=0.9953 | Elapsed: 0.23s\n",
      "epoch 8 val   | loss=0.8417 acc=0.7580 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0138 acc=0.9975 | Elapsed: 0.24s\n",
      "epoch 9 val   | loss=0.9106 acc=0.7378 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0117 acc=0.9978 | Elapsed: 0.24s\n",
      "epoch 10 val   | loss=0.9370 acc=0.7814 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0146 acc=0.9964 | Elapsed: 0.24s\n",
      "epoch 11 val   | loss=1.1676 acc=0.7677 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0176 acc=0.9959 | Elapsed: 0.23s\n",
      "epoch 12 val   | loss=0.9375 acc=0.7469 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0075 acc=0.9988 | Elapsed: 0.24s\n",
      "epoch 13 val   | loss=1.0073 acc=0.7580 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0049 acc=0.9991 | Elapsed: 0.22s\n",
      "epoch 14 val   | loss=1.1101 acc=0.7580 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0034 acc=0.9997 | Elapsed: 0.22s\n",
      "epoch 15 val   | loss=1.1456 acc=0.7671 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0027 acc=0.9995 | Elapsed: 0.22s\n",
      "epoch 16 val   | loss=1.1080 acc=0.7599 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0019 acc=0.9995 | Elapsed: 0.22s\n",
      "epoch 17 val   | loss=1.1464 acc=0.7638 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0022 acc=0.9995 | Elapsed: 0.22s\n",
      "epoch 18 val   | loss=1.2066 acc=0.7625 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0030 acc=0.9994 | Elapsed: 0.21s\n",
      "epoch 19 val   | loss=1.4476 acc=0.7521 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0045 acc=0.9988 | Elapsed: 0.22s\n",
      "epoch 20 val   | loss=1.1805 acc=0.7462 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7970 best_val_loss=0.4598 best_epoch=2 elapsed=5.15s\n",
      "\n",
      "GRID RUN 11/36\n",
      "run start | seed=42 batch=256 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.6423 acc=0.6284 | Elapsed: 0.24s\n",
      "epoch 1 val   | loss=0.5272 acc=0.7528 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7528 at epoch 1 (val_loss=0.5272)\n",
      "epoch 2 train | loss=0.4988 acc=0.7706 | Elapsed: 0.23s\n",
      "epoch 2 val   | loss=0.4694 acc=0.7996 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7996 at epoch 2 (val_loss=0.4694)\n",
      "epoch 3 train | loss=0.4372 acc=0.8098 | Elapsed: 0.22s\n",
      "epoch 3 val   | loss=0.4481 acc=0.8009 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8009 at epoch 3 (val_loss=0.4481)\n",
      "epoch 4 train | loss=0.3811 acc=0.8411 | Elapsed: 0.22s\n",
      "epoch 4 val   | loss=0.4543 acc=0.7996 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.3113 acc=0.8764 | Elapsed: 0.22s\n",
      "epoch 5 val   | loss=0.4607 acc=0.7911 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.2425 acc=0.9117 | Elapsed: 0.23s\n",
      "epoch 6 val   | loss=0.4998 acc=0.7742 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.1762 acc=0.9394 | Elapsed: 0.22s\n",
      "epoch 7 val   | loss=0.5626 acc=0.7859 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.1203 acc=0.9628 | Elapsed: 0.22s\n",
      "epoch 8 val   | loss=0.6181 acc=0.7697 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0786 acc=0.9767 | Elapsed: 0.22s\n",
      "epoch 9 val   | loss=0.7009 acc=0.7358 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0544 acc=0.9842 | Elapsed: 0.22s\n",
      "epoch 10 val   | loss=0.8389 acc=0.7807 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0392 acc=0.9892 | Elapsed: 0.22s\n",
      "epoch 11 val   | loss=0.7975 acc=0.7749 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0289 acc=0.9922 | Elapsed: 0.22s\n",
      "epoch 12 val   | loss=0.8372 acc=0.7755 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0211 acc=0.9953 | Elapsed: 0.21s\n",
      "epoch 13 val   | loss=0.8975 acc=0.7703 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0171 acc=0.9958 | Elapsed: 0.23s\n",
      "epoch 14 val   | loss=0.9739 acc=0.7801 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0175 acc=0.9953 | Elapsed: 0.23s\n",
      "epoch 15 val   | loss=0.9660 acc=0.7254 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0166 acc=0.9955 | Elapsed: 0.23s\n",
      "epoch 16 val   | loss=1.0168 acc=0.7664 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0149 acc=0.9959 | Elapsed: 0.23s\n",
      "epoch 17 val   | loss=0.9339 acc=0.7716 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0131 acc=0.9966 | Elapsed: 0.23s\n",
      "epoch 18 val   | loss=1.0940 acc=0.7723 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0112 acc=0.9973 | Elapsed: 0.22s\n",
      "epoch 19 val   | loss=1.0684 acc=0.7528 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0083 acc=0.9986 | Elapsed: 0.23s\n",
      "epoch 20 val   | loss=1.0902 acc=0.7554 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8009 best_val_loss=0.4481 best_epoch=3 elapsed=4.85s\n",
      "\n",
      "GRID RUN 12/36\n",
      "run start | seed=42 batch=256 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6827 acc=0.5759 | Elapsed: 0.23s\n",
      "epoch 1 val   | loss=0.6688 acc=0.6116 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6116 at epoch 1 (val_loss=0.6688)\n",
      "epoch 2 train | loss=0.6590 acc=0.6183 | Elapsed: 0.23s\n",
      "epoch 2 val   | loss=0.6515 acc=0.6272 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6272 at epoch 2 (val_loss=0.6515)\n",
      "epoch 3 train | loss=0.6331 acc=0.6483 | Elapsed: 0.22s\n",
      "epoch 3 val   | loss=0.6207 acc=0.6519 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6519 at epoch 3 (val_loss=0.6207)\n",
      "epoch 4 train | loss=0.5797 acc=0.7166 | Elapsed: 0.22s\n",
      "epoch 4 val   | loss=0.5126 acc=0.7677 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7677 at epoch 4 (val_loss=0.5126)\n",
      "epoch 5 train | loss=0.5050 acc=0.7697 | Elapsed: 0.22s\n",
      "epoch 5 val   | loss=0.4891 acc=0.7801 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7801 at epoch 5 (val_loss=0.4891)\n",
      "epoch 6 train | loss=0.4863 acc=0.7794 | Elapsed: 0.24s\n",
      "epoch 6 val   | loss=0.4774 acc=0.7833 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7833 at epoch 6 (val_loss=0.4774)\n",
      "epoch 7 train | loss=0.4694 acc=0.7819 | Elapsed: 0.22s\n",
      "epoch 7 val   | loss=0.4636 acc=0.8048 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8048 at epoch 7 (val_loss=0.4636)\n",
      "epoch 8 train | loss=0.4343 acc=0.8186 | Elapsed: 0.23s\n",
      "epoch 8 val   | loss=0.4587 acc=0.8042 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.4135 acc=0.8273 | Elapsed: 0.23s\n",
      "epoch 9 val   | loss=0.4522 acc=0.8068 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8068 at epoch 9 (val_loss=0.4522)\n",
      "epoch 10 train | loss=0.3985 acc=0.8383 | Elapsed: 0.24s\n",
      "epoch 10 val   | loss=0.4482 acc=0.8074 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8074 at epoch 10 (val_loss=0.4482)\n",
      "epoch 11 train | loss=0.3763 acc=0.8484 | Elapsed: 0.23s\n",
      "epoch 11 val   | loss=0.4489 acc=0.8055 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.3593 acc=0.8586 | Elapsed: 0.23s\n",
      "epoch 12 val   | loss=0.4508 acc=0.8022 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.3441 acc=0.8638 | Elapsed: 0.25s\n",
      "epoch 13 val   | loss=0.4496 acc=0.7989 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.3225 acc=0.8734 | Elapsed: 0.25s\n",
      "epoch 14 val   | loss=0.4569 acc=0.8009 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.3014 acc=0.8850 | Elapsed: 0.24s\n",
      "epoch 15 val   | loss=0.4706 acc=0.7846 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.2910 acc=0.8895 | Elapsed: 0.24s\n",
      "epoch 16 val   | loss=0.4741 acc=0.7892 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.2530 acc=0.9056 | Elapsed: 0.23s\n",
      "epoch 17 val   | loss=0.5224 acc=0.7892 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.2317 acc=0.9153 | Elapsed: 0.26s\n",
      "epoch 18 val   | loss=0.5150 acc=0.7807 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.2126 acc=0.9208 | Elapsed: 0.24s\n",
      "epoch 19 val   | loss=0.5365 acc=0.7684 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.1957 acc=0.9263 | Elapsed: 0.24s\n",
      "epoch 20 val   | loss=0.5437 acc=0.7736 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8074 best_val_loss=0.4482 best_epoch=10 elapsed=5.08s\n",
      "\n",
      "GRID RUN 13/36\n",
      "run start | seed=23 batch=32 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5373 acc=0.7404 | Elapsed: 0.74s\n",
      "epoch 1 val   | loss=0.4667 acc=0.7869 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7869 at epoch 1 (val_loss=0.4667)\n",
      "epoch 2 train | loss=0.3604 acc=0.8502 | Elapsed: 0.71s\n",
      "epoch 2 val   | loss=0.4816 acc=0.7827 | Elapsed: 0.04s\n",
      "epoch 3 train | loss=0.1724 acc=0.9377 | Elapsed: 0.69s\n",
      "epoch 3 val   | loss=0.6315 acc=0.7672 | Elapsed: 0.04s\n",
      "epoch 4 train | loss=0.0595 acc=0.9822 | Elapsed: 0.69s\n",
      "epoch 4 val   | loss=0.8113 acc=0.7275 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0317 acc=0.9922 | Elapsed: 0.70s\n",
      "epoch 5 val   | loss=0.9075 acc=0.7601 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0201 acc=0.9945 | Elapsed: 0.71s\n",
      "epoch 6 val   | loss=1.1191 acc=0.6779 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.0311 acc=0.9906 | Elapsed: 0.69s\n",
      "epoch 7 val   | loss=1.0177 acc=0.7501 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.0169 acc=0.9940 | Elapsed: 0.70s\n",
      "epoch 8 val   | loss=1.0156 acc=0.7614 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0093 acc=0.9977 | Elapsed: 0.70s\n",
      "epoch 9 val   | loss=1.2388 acc=0.7528 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.0051 acc=0.9985 | Elapsed: 0.70s\n",
      "epoch 10 val   | loss=1.2376 acc=0.7332 | Elapsed: 0.04s\n",
      "epoch 11 train | loss=0.0049 acc=0.9982 | Elapsed: 0.70s\n",
      "epoch 11 val   | loss=1.1858 acc=0.7262 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0073 acc=0.9979 | Elapsed: 0.70s\n",
      "epoch 12 val   | loss=1.3498 acc=0.7472 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0035 acc=0.9992 | Elapsed: 0.70s\n",
      "epoch 13 val   | loss=1.4542 acc=0.7571 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0044 acc=0.9992 | Elapsed: 0.70s\n",
      "epoch 14 val   | loss=1.2830 acc=0.7275 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0038 acc=0.9989 | Elapsed: 0.70s\n",
      "epoch 15 val   | loss=1.3519 acc=0.7587 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0046 acc=0.9990 | Elapsed: 0.70s\n",
      "epoch 16 val   | loss=1.4039 acc=0.7359 | Elapsed: 0.04s\n",
      "epoch 17 train | loss=0.0066 acc=0.9979 | Elapsed: 0.70s\n",
      "epoch 17 val   | loss=1.3941 acc=0.7658 | Elapsed: 0.04s\n",
      "epoch 18 train | loss=0.0237 acc=0.9927 | Elapsed: 0.69s\n",
      "epoch 18 val   | loss=1.4679 acc=0.6750 | Elapsed: 0.05s\n",
      "epoch 19 train | loss=0.0381 acc=0.9877 | Elapsed: 0.69s\n",
      "epoch 19 val   | loss=1.1285 acc=0.7362 | Elapsed: 0.04s\n",
      "epoch 20 train | loss=0.0086 acc=0.9977 | Elapsed: 0.69s\n",
      "epoch 20 val   | loss=1.3582 acc=0.7036 | Elapsed: 0.04s\n",
      "run end | best_val_acc=0.7869 best_val_loss=0.4667 best_epoch=1 elapsed=14.77s\n",
      "\n",
      "GRID RUN 14/36\n",
      "run start | seed=23 batch=32 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5420 acc=0.7369 | Elapsed: 0.73s\n",
      "epoch 1 val   | loss=0.4566 acc=0.7884 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7884 at epoch 1 (val_loss=0.4566)\n",
      "epoch 2 train | loss=0.4056 acc=0.8253 | Elapsed: 0.69s\n",
      "epoch 2 val   | loss=0.4595 acc=0.7841 | Elapsed: 0.05s\n",
      "epoch 3 train | loss=0.2724 acc=0.8948 | Elapsed: 0.70s\n",
      "epoch 3 val   | loss=0.5333 acc=0.7743 | Elapsed: 0.04s\n",
      "epoch 4 train | loss=0.1369 acc=0.9508 | Elapsed: 0.70s\n",
      "epoch 4 val   | loss=0.5995 acc=0.7713 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0652 acc=0.9809 | Elapsed: 0.69s\n",
      "epoch 5 val   | loss=0.7158 acc=0.7670 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0376 acc=0.9903 | Elapsed: 0.68s\n",
      "epoch 6 val   | loss=0.9747 acc=0.7443 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.0253 acc=0.9935 | Elapsed: 0.71s\n",
      "epoch 7 val   | loss=0.9161 acc=0.7812 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.0152 acc=0.9961 | Elapsed: 0.70s\n",
      "epoch 8 val   | loss=0.8992 acc=0.7500 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0159 acc=0.9960 | Elapsed: 0.69s\n",
      "epoch 9 val   | loss=1.0024 acc=0.7727 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.0096 acc=0.9979 | Elapsed: 0.68s\n",
      "epoch 10 val   | loss=1.1168 acc=0.7599 | Elapsed: 0.04s\n",
      "epoch 11 train | loss=0.0069 acc=0.9985 | Elapsed: 0.71s\n",
      "epoch 11 val   | loss=1.1231 acc=0.7699 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0054 acc=0.9987 | Elapsed: 0.70s\n",
      "epoch 12 val   | loss=1.0224 acc=0.7500 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0055 acc=0.9984 | Elapsed: 0.70s\n",
      "epoch 13 val   | loss=1.1364 acc=0.7642 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0049 acc=0.9990 | Elapsed: 0.68s\n",
      "epoch 14 val   | loss=1.2430 acc=0.7585 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0084 acc=0.9971 | Elapsed: 0.70s\n",
      "epoch 15 val   | loss=1.2203 acc=0.7528 | Elapsed: 0.04s\n",
      "epoch 16 train | loss=0.0264 acc=0.9901 | Elapsed: 0.70s\n",
      "epoch 16 val   | loss=1.0836 acc=0.7514 | Elapsed: 0.04s\n",
      "epoch 17 train | loss=0.0110 acc=0.9968 | Elapsed: 0.69s\n",
      "epoch 17 val   | loss=1.0557 acc=0.7544 | Elapsed: 0.05s\n",
      "epoch 18 train | loss=0.0041 acc=0.9989 | Elapsed: 0.69s\n",
      "epoch 18 val   | loss=1.1340 acc=0.7443 | Elapsed: 0.04s\n",
      "epoch 19 train | loss=0.0042 acc=0.9990 | Elapsed: 0.70s\n",
      "epoch 19 val   | loss=1.1585 acc=0.7402 | Elapsed: 0.04s\n",
      "epoch 20 train | loss=0.0026 acc=0.9992 | Elapsed: 0.71s\n",
      "epoch 20 val   | loss=1.1782 acc=0.7501 | Elapsed: 0.04s\n",
      "run end | best_val_acc=0.7884 best_val_loss=0.4566 best_epoch=1 elapsed=14.73s\n",
      "\n",
      "GRID RUN 15/36\n",
      "run start | seed=23 batch=32 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6262 acc=0.6499 | Elapsed: 0.78s\n",
      "epoch 1 val   | loss=0.5048 acc=0.7601 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7601 at epoch 1 (val_loss=0.5048)\n",
      "epoch 2 train | loss=0.4965 acc=0.7796 | Elapsed: 0.74s\n",
      "epoch 2 val   | loss=0.4632 acc=0.7983 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7983 at epoch 2 (val_loss=0.4632)\n",
      "epoch 3 train | loss=0.4504 acc=0.8065 | Elapsed: 0.75s\n",
      "epoch 3 val   | loss=0.4390 acc=0.7997 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7997 at epoch 3 (val_loss=0.4390)\n",
      "epoch 4 train | loss=0.4049 acc=0.8327 | Elapsed: 0.74s\n",
      "epoch 4 val   | loss=0.4411 acc=0.8026 | Elapsed: 0.04s\n",
      "new best: val_acc=0.8026 at epoch 4 (val_loss=0.4411)\n",
      "epoch 5 train | loss=0.3614 acc=0.8527 | Elapsed: 0.71s\n",
      "epoch 5 val   | loss=0.4545 acc=0.8054 | Elapsed: 0.04s\n",
      "new best: val_acc=0.8054 at epoch 5 (val_loss=0.4545)\n",
      "epoch 6 train | loss=0.3094 acc=0.8784 | Elapsed: 0.69s\n",
      "epoch 6 val   | loss=0.4761 acc=0.7984 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.2616 acc=0.8991 | Elapsed: 0.71s\n",
      "epoch 7 val   | loss=0.4982 acc=0.7913 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.2062 acc=0.9265 | Elapsed: 0.72s\n",
      "epoch 8 val   | loss=0.5338 acc=0.7856 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.1669 acc=0.9437 | Elapsed: 0.72s\n",
      "epoch 9 val   | loss=0.6227 acc=0.7800 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.1409 acc=0.9508 | Elapsed: 0.69s\n",
      "epoch 10 val   | loss=0.6033 acc=0.7601 | Elapsed: 0.04s\n",
      "epoch 11 train | loss=0.1117 acc=0.9641 | Elapsed: 0.71s\n",
      "epoch 11 val   | loss=0.7296 acc=0.7601 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0887 acc=0.9702 | Elapsed: 0.72s\n",
      "epoch 12 val   | loss=0.7569 acc=0.7558 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0758 acc=0.9757 | Elapsed: 0.71s\n",
      "epoch 13 val   | loss=0.8832 acc=0.7515 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0618 acc=0.9827 | Elapsed: 0.69s\n",
      "epoch 14 val   | loss=0.9261 acc=0.7544 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0624 acc=0.9814 | Elapsed: 0.71s\n",
      "epoch 15 val   | loss=0.8328 acc=0.7501 | Elapsed: 0.04s\n",
      "epoch 16 train | loss=0.0447 acc=0.9869 | Elapsed: 0.71s\n",
      "epoch 16 val   | loss=1.0131 acc=0.7587 | Elapsed: 0.04s\n",
      "epoch 17 train | loss=0.0369 acc=0.9896 | Elapsed: 0.71s\n",
      "epoch 17 val   | loss=1.0432 acc=0.7515 | Elapsed: 0.04s\n",
      "epoch 18 train | loss=0.0324 acc=0.9909 | Elapsed: 0.69s\n",
      "epoch 18 val   | loss=1.0084 acc=0.7501 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0284 acc=0.9934 | Elapsed: 0.70s\n",
      "epoch 19 val   | loss=1.0407 acc=0.7515 | Elapsed: 0.04s\n",
      "epoch 20 train | loss=0.0246 acc=0.9935 | Elapsed: 0.71s\n",
      "epoch 20 val   | loss=1.0636 acc=0.7601 | Elapsed: 0.04s\n",
      "run end | best_val_acc=0.8054 best_val_loss=0.4545 best_epoch=5 elapsed=15.06s\n",
      "\n",
      "GRID RUN 16/36\n",
      "run start | seed=23 batch=64 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5410 acc=0.7392 | Elapsed: 0.49s\n",
      "epoch 1 val   | loss=0.4683 acc=0.7856 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7856 at epoch 1 (val_loss=0.4683)\n",
      "epoch 2 train | loss=0.3798 acc=0.8409 | Elapsed: 0.47s\n",
      "epoch 2 val   | loss=0.4700 acc=0.7899 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7899 at epoch 2 (val_loss=0.4700)\n",
      "epoch 3 train | loss=0.2134 acc=0.9180 | Elapsed: 0.45s\n",
      "epoch 3 val   | loss=0.6019 acc=0.7537 | Elapsed: 0.03s\n",
      "epoch 4 train | loss=0.0860 acc=0.9720 | Elapsed: 0.47s\n",
      "epoch 4 val   | loss=0.7125 acc=0.7678 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.0462 acc=0.9868 | Elapsed: 0.47s\n",
      "epoch 5 val   | loss=0.7885 acc=0.7736 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0207 acc=0.9942 | Elapsed: 0.47s\n",
      "epoch 6 val   | loss=0.9576 acc=0.7345 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0138 acc=0.9963 | Elapsed: 0.45s\n",
      "epoch 7 val   | loss=1.0406 acc=0.7736 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0115 acc=0.9968 | Elapsed: 0.46s\n",
      "epoch 8 val   | loss=1.0059 acc=0.7288 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.0151 acc=0.9947 | Elapsed: 0.45s\n",
      "epoch 9 val   | loss=1.0963 acc=0.7551 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0082 acc=0.9973 | Elapsed: 0.46s\n",
      "epoch 10 val   | loss=1.0947 acc=0.7473 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0106 acc=0.9969 | Elapsed: 0.44s\n",
      "epoch 11 val   | loss=1.1131 acc=0.7608 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0070 acc=0.9979 | Elapsed: 0.46s\n",
      "epoch 12 val   | loss=1.3367 acc=0.7388 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0030 acc=0.9992 | Elapsed: 0.44s\n",
      "epoch 13 val   | loss=1.2449 acc=0.7253 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0040 acc=0.9994 | Elapsed: 0.46s\n",
      "epoch 14 val   | loss=1.3779 acc=0.7140 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0081 acc=0.9974 | Elapsed: 0.43s\n",
      "epoch 15 val   | loss=1.2340 acc=0.7324 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0051 acc=0.9989 | Elapsed: 0.45s\n",
      "epoch 16 val   | loss=1.3748 acc=0.7367 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0025 acc=0.9995 | Elapsed: 0.44s\n",
      "epoch 17 val   | loss=1.3102 acc=0.7289 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0024 acc=0.9994 | Elapsed: 0.47s\n",
      "epoch 18 val   | loss=1.3166 acc=0.7218 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0054 acc=0.9986 | Elapsed: 0.45s\n",
      "epoch 19 val   | loss=1.2768 acc=0.7211 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0033 acc=0.9992 | Elapsed: 0.44s\n",
      "epoch 20 val   | loss=1.4457 acc=0.7303 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7899 best_val_loss=0.4700 best_epoch=2 elapsed=9.68s\n",
      "\n",
      "GRID RUN 17/36\n",
      "run start | seed=23 batch=64 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5625 acc=0.7205 | Elapsed: 0.47s\n",
      "epoch 1 val   | loss=0.4695 acc=0.7749 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7749 at epoch 1 (val_loss=0.4695)\n",
      "epoch 2 train | loss=0.4322 acc=0.8125 | Elapsed: 0.45s\n",
      "epoch 2 val   | loss=0.4580 acc=0.7927 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7927 at epoch 2 (val_loss=0.4580)\n",
      "epoch 3 train | loss=0.3254 acc=0.8681 | Elapsed: 0.44s\n",
      "epoch 3 val   | loss=0.4709 acc=0.7905 | Elapsed: 0.03s\n",
      "epoch 4 train | loss=0.2048 acc=0.9262 | Elapsed: 0.45s\n",
      "epoch 4 val   | loss=0.5329 acc=0.7828 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.1126 acc=0.9628 | Elapsed: 0.45s\n",
      "epoch 5 val   | loss=0.6159 acc=0.7749 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.0663 acc=0.9797 | Elapsed: 0.44s\n",
      "epoch 6 val   | loss=0.7700 acc=0.7445 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0397 acc=0.9892 | Elapsed: 0.46s\n",
      "epoch 7 val   | loss=0.7876 acc=0.7657 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0230 acc=0.9939 | Elapsed: 0.44s\n",
      "epoch 8 val   | loss=0.8407 acc=0.7579 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.0152 acc=0.9966 | Elapsed: 0.46s\n",
      "epoch 9 val   | loss=0.9817 acc=0.7558 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0095 acc=0.9984 | Elapsed: 0.43s\n",
      "epoch 10 val   | loss=0.9624 acc=0.7537 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0091 acc=0.9976 | Elapsed: 0.45s\n",
      "epoch 11 val   | loss=1.1131 acc=0.7608 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0069 acc=0.9984 | Elapsed: 0.44s\n",
      "epoch 12 val   | loss=1.1227 acc=0.7700 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0054 acc=0.9987 | Elapsed: 0.46s\n",
      "epoch 13 val   | loss=1.0475 acc=0.7239 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0048 acc=0.9992 | Elapsed: 0.45s\n",
      "epoch 14 val   | loss=1.1958 acc=0.7438 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0048 acc=0.9986 | Elapsed: 0.47s\n",
      "epoch 15 val   | loss=1.1352 acc=0.7438 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0131 acc=0.9966 | Elapsed: 0.44s\n",
      "epoch 16 val   | loss=1.0839 acc=0.7558 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0087 acc=0.9973 | Elapsed: 0.46s\n",
      "epoch 17 val   | loss=1.1032 acc=0.6856 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0056 acc=0.9986 | Elapsed: 0.43s\n",
      "epoch 18 val   | loss=1.2235 acc=0.7459 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0018 acc=0.9997 | Elapsed: 0.45s\n",
      "epoch 19 val   | loss=1.1818 acc=0.7523 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0014 acc=0.9998 | Elapsed: 0.43s\n",
      "epoch 20 val   | loss=1.4394 acc=0.7565 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7927 best_val_loss=0.4580 best_epoch=2 elapsed=9.52s\n",
      "\n",
      "GRID RUN 18/36\n",
      "run start | seed=23 batch=64 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6631 acc=0.6037 | Elapsed: 0.49s\n",
      "epoch 1 val   | loss=0.6202 acc=0.6558 | Elapsed: 0.03s\n",
      "new best: val_acc=0.6558 at epoch 1 (val_loss=0.6202)\n",
      "epoch 2 train | loss=0.5332 acc=0.7524 | Elapsed: 0.43s\n",
      "epoch 2 val   | loss=0.4742 acc=0.7898 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7898 at epoch 2 (val_loss=0.4742)\n",
      "epoch 3 train | loss=0.4772 acc=0.7870 | Elapsed: 0.45s\n",
      "epoch 3 val   | loss=0.4527 acc=0.7920 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7920 at epoch 3 (val_loss=0.4527)\n",
      "epoch 4 train | loss=0.4393 acc=0.8146 | Elapsed: 0.43s\n",
      "epoch 4 val   | loss=0.4570 acc=0.7998 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7998 at epoch 4 (val_loss=0.4570)\n",
      "epoch 5 train | loss=0.4073 acc=0.8313 | Elapsed: 0.45s\n",
      "epoch 5 val   | loss=0.4466 acc=0.8083 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8083 at epoch 5 (val_loss=0.4466)\n",
      "epoch 6 train | loss=0.3700 acc=0.8491 | Elapsed: 0.44s\n",
      "epoch 6 val   | loss=0.4571 acc=0.8012 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.3278 acc=0.8665 | Elapsed: 0.44s\n",
      "epoch 7 val   | loss=0.4712 acc=0.7984 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.2825 acc=0.8908 | Elapsed: 0.45s\n",
      "epoch 8 val   | loss=0.5181 acc=0.7813 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.2459 acc=0.9091 | Elapsed: 0.44s\n",
      "epoch 9 val   | loss=0.5432 acc=0.7806 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.2110 acc=0.9246 | Elapsed: 0.45s\n",
      "epoch 10 val   | loss=0.5329 acc=0.7764 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.1730 acc=0.9415 | Elapsed: 0.44s\n",
      "epoch 11 val   | loss=0.5967 acc=0.7792 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.1459 acc=0.9525 | Elapsed: 0.45s\n",
      "epoch 12 val   | loss=0.6301 acc=0.7750 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.1217 acc=0.9607 | Elapsed: 0.43s\n",
      "epoch 13 val   | loss=0.6956 acc=0.7671 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.1082 acc=0.9634 | Elapsed: 0.45s\n",
      "epoch 14 val   | loss=0.7562 acc=0.7551 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0904 acc=0.9696 | Elapsed: 0.44s\n",
      "epoch 15 val   | loss=0.7809 acc=0.7572 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0771 acc=0.9765 | Elapsed: 0.46s\n",
      "epoch 16 val   | loss=0.8156 acc=0.7601 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0749 acc=0.9774 | Elapsed: 0.44s\n",
      "epoch 17 val   | loss=0.7989 acc=0.7757 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0664 acc=0.9807 | Elapsed: 0.45s\n",
      "epoch 18 val   | loss=0.8176 acc=0.7572 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0598 acc=0.9815 | Elapsed: 0.44s\n",
      "epoch 19 val   | loss=0.8351 acc=0.7558 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0521 acc=0.9845 | Elapsed: 0.46s\n",
      "epoch 20 val   | loss=0.9523 acc=0.7373 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8083 best_val_loss=0.4466 best_epoch=5 elapsed=9.47s\n",
      "\n",
      "GRID RUN 19/36\n",
      "run start | seed=23 batch=128 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5620 acc=0.7203 | Elapsed: 0.33s\n",
      "epoch 1 val   | loss=0.4643 acc=0.7815 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7815 at epoch 1 (val_loss=0.4643)\n",
      "epoch 2 train | loss=0.4156 acc=0.8246 | Elapsed: 0.29s\n",
      "epoch 2 val   | loss=0.4516 acc=0.7970 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7970 at epoch 2 (val_loss=0.4516)\n",
      "epoch 3 train | loss=0.2850 acc=0.8849 | Elapsed: 0.31s\n",
      "epoch 3 val   | loss=0.5137 acc=0.7971 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7971 at epoch 3 (val_loss=0.5137)\n",
      "epoch 4 train | loss=0.1593 acc=0.9466 | Elapsed: 0.29s\n",
      "epoch 4 val   | loss=0.5526 acc=0.7983 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7983 at epoch 4 (val_loss=0.5526)\n",
      "epoch 5 train | loss=0.0687 acc=0.9796 | Elapsed: 0.30s\n",
      "epoch 5 val   | loss=0.7405 acc=0.7723 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0376 acc=0.9876 | Elapsed: 0.31s\n",
      "epoch 6 val   | loss=0.7677 acc=0.7659 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0258 acc=0.9938 | Elapsed: 0.30s\n",
      "epoch 7 val   | loss=0.8122 acc=0.7502 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0134 acc=0.9963 | Elapsed: 0.31s\n",
      "epoch 8 val   | loss=0.9455 acc=0.7542 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0094 acc=0.9981 | Elapsed: 0.31s\n",
      "epoch 9 val   | loss=0.9890 acc=0.7633 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0065 acc=0.9986 | Elapsed: 0.29s\n",
      "epoch 10 val   | loss=1.0969 acc=0.7503 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0054 acc=0.9987 | Elapsed: 0.29s\n",
      "epoch 11 val   | loss=1.1064 acc=0.7529 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0182 acc=0.9959 | Elapsed: 0.31s\n",
      "epoch 12 val   | loss=0.9624 acc=0.7438 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0111 acc=0.9976 | Elapsed: 0.32s\n",
      "epoch 13 val   | loss=1.0264 acc=0.7425 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0047 acc=0.9990 | Elapsed: 0.29s\n",
      "epoch 14 val   | loss=1.1524 acc=0.7516 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0035 acc=0.9990 | Elapsed: 0.30s\n",
      "epoch 15 val   | loss=1.0858 acc=0.7659 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0032 acc=0.9990 | Elapsed: 0.30s\n",
      "epoch 16 val   | loss=1.1424 acc=0.7555 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0016 acc=0.9995 | Elapsed: 0.29s\n",
      "epoch 17 val   | loss=1.1759 acc=0.7555 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0024 acc=0.9994 | Elapsed: 0.29s\n",
      "epoch 18 val   | loss=1.3103 acc=0.7490 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0026 acc=0.9992 | Elapsed: 0.31s\n",
      "epoch 19 val   | loss=1.1239 acc=0.7347 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0044 acc=0.9994 | Elapsed: 0.29s\n",
      "epoch 20 val   | loss=1.1205 acc=0.7516 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7983 best_val_loss=0.5526 best_epoch=4 elapsed=6.49s\n",
      "\n",
      "GRID RUN 20/36\n",
      "run start | seed=23 batch=128 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5954 acc=0.6901 | Elapsed: 0.36s\n",
      "epoch 1 val   | loss=0.4989 acc=0.7697 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7697 at epoch 1 (val_loss=0.4989)\n",
      "epoch 2 train | loss=0.4627 acc=0.7961 | Elapsed: 0.30s\n",
      "epoch 2 val   | loss=0.4374 acc=0.8087 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8087 at epoch 2 (val_loss=0.4374)\n",
      "epoch 3 train | loss=0.3889 acc=0.8398 | Elapsed: 0.29s\n",
      "epoch 3 val   | loss=0.4341 acc=0.8178 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8178 at epoch 3 (val_loss=0.4341)\n",
      "epoch 4 train | loss=0.2889 acc=0.8903 | Elapsed: 0.29s\n",
      "epoch 4 val   | loss=0.4713 acc=0.8048 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.2026 acc=0.9273 | Elapsed: 0.33s\n",
      "epoch 5 val   | loss=0.5207 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.1359 acc=0.9525 | Elapsed: 0.31s\n",
      "epoch 6 val   | loss=0.6038 acc=0.7854 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0869 acc=0.9719 | Elapsed: 0.30s\n",
      "epoch 7 val   | loss=0.6856 acc=0.7828 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0596 acc=0.9831 | Elapsed: 0.32s\n",
      "epoch 8 val   | loss=0.6758 acc=0.7736 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0350 acc=0.9911 | Elapsed: 0.31s\n",
      "epoch 9 val   | loss=0.7905 acc=0.7646 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0236 acc=0.9939 | Elapsed: 0.31s\n",
      "epoch 10 val   | loss=0.8035 acc=0.7711 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0164 acc=0.9960 | Elapsed: 0.32s\n",
      "epoch 11 val   | loss=0.9033 acc=0.7515 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0135 acc=0.9971 | Elapsed: 0.31s\n",
      "epoch 12 val   | loss=1.0144 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0110 acc=0.9979 | Elapsed: 0.32s\n",
      "epoch 13 val   | loss=1.0796 acc=0.7581 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0102 acc=0.9987 | Elapsed: 0.30s\n",
      "epoch 14 val   | loss=1.0729 acc=0.7515 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0083 acc=0.9986 | Elapsed: 0.30s\n",
      "epoch 15 val   | loss=1.0657 acc=0.7724 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0059 acc=0.9990 | Elapsed: 0.31s\n",
      "epoch 16 val   | loss=1.1736 acc=0.7542 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0114 acc=0.9974 | Elapsed: 0.30s\n",
      "epoch 17 val   | loss=1.2241 acc=0.7347 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0102 acc=0.9955 | Elapsed: 0.29s\n",
      "epoch 18 val   | loss=1.0528 acc=0.7476 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0270 acc=0.9927 | Elapsed: 0.30s\n",
      "epoch 19 val   | loss=0.9947 acc=0.7659 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0071 acc=0.9984 | Elapsed: 0.32s\n",
      "epoch 20 val   | loss=1.1051 acc=0.7580 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8178 best_val_loss=0.4341 best_epoch=3 elapsed=6.70s\n",
      "\n",
      "GRID RUN 21/36\n",
      "run start | seed=23 batch=128 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6715 acc=0.5961 | Elapsed: 0.34s\n",
      "epoch 1 val   | loss=0.6532 acc=0.6280 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6280 at epoch 1 (val_loss=0.6532)\n",
      "epoch 2 train | loss=0.6219 acc=0.6590 | Elapsed: 0.32s\n",
      "epoch 2 val   | loss=0.5265 acc=0.7606 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7606 at epoch 2 (val_loss=0.5265)\n",
      "epoch 3 train | loss=0.5149 acc=0.7656 | Elapsed: 0.30s\n",
      "epoch 3 val   | loss=0.4668 acc=0.7879 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7879 at epoch 3 (val_loss=0.4668)\n",
      "epoch 4 train | loss=0.4800 acc=0.7875 | Elapsed: 0.31s\n",
      "epoch 4 val   | loss=0.4466 acc=0.8009 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8009 at epoch 4 (val_loss=0.4466)\n",
      "epoch 5 train | loss=0.4537 acc=0.8095 | Elapsed: 0.33s\n",
      "epoch 5 val   | loss=0.4370 acc=0.8022 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8022 at epoch 5 (val_loss=0.4370)\n",
      "epoch 6 train | loss=0.4266 acc=0.8221 | Elapsed: 0.31s\n",
      "epoch 6 val   | loss=0.4321 acc=0.8139 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8139 at epoch 6 (val_loss=0.4321)\n",
      "epoch 7 train | loss=0.3997 acc=0.8348 | Elapsed: 0.31s\n",
      "epoch 7 val   | loss=0.4336 acc=0.8165 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8165 at epoch 7 (val_loss=0.4336)\n",
      "epoch 8 train | loss=0.3612 acc=0.8563 | Elapsed: 0.32s\n",
      "epoch 8 val   | loss=0.4430 acc=0.8139 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.3300 acc=0.8715 | Elapsed: 0.31s\n",
      "epoch 9 val   | loss=0.4529 acc=0.8087 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.3080 acc=0.8758 | Elapsed: 0.31s\n",
      "epoch 10 val   | loss=0.4644 acc=0.8035 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.2789 acc=0.8925 | Elapsed: 0.32s\n",
      "epoch 11 val   | loss=0.4856 acc=0.8087 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.2445 acc=0.9123 | Elapsed: 0.31s\n",
      "epoch 12 val   | loss=0.5067 acc=0.8035 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.2131 acc=0.9262 | Elapsed: 0.30s\n",
      "epoch 13 val   | loss=0.5485 acc=0.7970 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.1883 acc=0.9343 | Elapsed: 0.32s\n",
      "epoch 14 val   | loss=0.5806 acc=0.7685 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.1629 acc=0.9460 | Elapsed: 0.31s\n",
      "epoch 15 val   | loss=0.5961 acc=0.7736 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.1387 acc=0.9563 | Elapsed: 0.31s\n",
      "epoch 16 val   | loss=0.6320 acc=0.7866 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.1257 acc=0.9598 | Elapsed: 0.33s\n",
      "epoch 17 val   | loss=0.7290 acc=0.7502 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.1207 acc=0.9619 | Elapsed: 0.32s\n",
      "epoch 18 val   | loss=0.6966 acc=0.7762 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.1058 acc=0.9680 | Elapsed: 0.31s\n",
      "epoch 19 val   | loss=0.7116 acc=0.7710 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.1003 acc=0.9708 | Elapsed: 0.33s\n",
      "epoch 20 val   | loss=0.8304 acc=0.7360 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8165 best_val_loss=0.4336 best_epoch=7 elapsed=6.76s\n",
      "\n",
      "GRID RUN 22/36\n",
      "run start | seed=23 batch=256 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5823 acc=0.6975 | Elapsed: 0.28s\n",
      "epoch 1 val   | loss=0.4972 acc=0.7645 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7645 at epoch 1 (val_loss=0.4972)\n",
      "epoch 2 train | loss=0.4389 acc=0.8144 | Elapsed: 0.24s\n",
      "epoch 2 val   | loss=0.4593 acc=0.7866 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7866 at epoch 2 (val_loss=0.4593)\n",
      "epoch 3 train | loss=0.3520 acc=0.8505 | Elapsed: 0.24s\n",
      "epoch 3 val   | loss=0.4572 acc=0.7970 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7970 at epoch 3 (val_loss=0.4572)\n",
      "epoch 4 train | loss=0.2410 acc=0.9153 | Elapsed: 0.26s\n",
      "epoch 4 val   | loss=0.5239 acc=0.7872 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.1421 acc=0.9509 | Elapsed: 0.25s\n",
      "epoch 5 val   | loss=0.5588 acc=0.7671 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.1013 acc=0.9681 | Elapsed: 0.39s\n",
      "epoch 6 val   | loss=0.6822 acc=0.7736 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0716 acc=0.9748 | Elapsed: 0.27s\n",
      "epoch 7 val   | loss=0.7828 acc=0.7768 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0856 acc=0.9723 | Elapsed: 0.25s\n",
      "epoch 8 val   | loss=0.7162 acc=0.7820 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0335 acc=0.9912 | Elapsed: 0.24s\n",
      "epoch 9 val   | loss=0.7930 acc=0.7723 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0158 acc=0.9969 | Elapsed: 0.24s\n",
      "epoch 10 val   | loss=0.8787 acc=0.7677 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0119 acc=0.9978 | Elapsed: 0.25s\n",
      "epoch 11 val   | loss=1.0276 acc=0.7547 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0214 acc=0.9947 | Elapsed: 0.24s\n",
      "epoch 12 val   | loss=0.9499 acc=0.7651 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0109 acc=0.9981 | Elapsed: 0.24s\n",
      "epoch 13 val   | loss=0.9338 acc=0.7612 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0085 acc=0.9989 | Elapsed: 0.26s\n",
      "epoch 14 val   | loss=1.0608 acc=0.7573 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0064 acc=0.9992 | Elapsed: 0.24s\n",
      "epoch 15 val   | loss=1.1218 acc=0.7606 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0043 acc=0.9997 | Elapsed: 0.24s\n",
      "epoch 16 val   | loss=1.1596 acc=0.7541 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0025 acc=0.9997 | Elapsed: 0.25s\n",
      "epoch 17 val   | loss=1.1965 acc=0.7554 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0021 acc=0.9997 | Elapsed: 0.26s\n",
      "epoch 18 val   | loss=1.1761 acc=0.7554 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0022 acc=0.9997 | Elapsed: 0.24s\n",
      "epoch 19 val   | loss=1.2181 acc=0.7534 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0017 acc=0.9997 | Elapsed: 0.26s\n",
      "epoch 20 val   | loss=1.2812 acc=0.7567 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7970 best_val_loss=0.4572 best_epoch=3 elapsed=5.55s\n",
      "\n",
      "GRID RUN 23/36\n",
      "run start | seed=23 batch=256 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.6262 acc=0.6502 | Elapsed: 0.29s\n",
      "epoch 1 val   | loss=0.5131 acc=0.7580 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7580 at epoch 1 (val_loss=0.5131)\n",
      "epoch 2 train | loss=0.4853 acc=0.7836 | Elapsed: 0.25s\n",
      "epoch 2 val   | loss=0.4583 acc=0.7944 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7944 at epoch 2 (val_loss=0.4583)\n",
      "epoch 3 train | loss=0.4305 acc=0.8123 | Elapsed: 0.25s\n",
      "epoch 3 val   | loss=0.4445 acc=0.7950 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7950 at epoch 3 (val_loss=0.4445)\n",
      "epoch 4 train | loss=0.3689 acc=0.8550 | Elapsed: 0.24s\n",
      "epoch 4 val   | loss=0.4651 acc=0.7957 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7957 at epoch 4 (val_loss=0.4651)\n",
      "epoch 5 train | loss=0.3138 acc=0.8730 | Elapsed: 0.27s\n",
      "epoch 5 val   | loss=0.4631 acc=0.7963 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7963 at epoch 5 (val_loss=0.4631)\n",
      "epoch 6 train | loss=0.2450 acc=0.9103 | Elapsed: 0.25s\n",
      "epoch 6 val   | loss=0.5234 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.1738 acc=0.9337 | Elapsed: 0.25s\n",
      "epoch 7 val   | loss=0.6075 acc=0.7814 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.1468 acc=0.9497 | Elapsed: 0.25s\n",
      "epoch 8 val   | loss=0.6023 acc=0.7840 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0893 acc=0.9730 | Elapsed: 0.27s\n",
      "epoch 9 val   | loss=0.7268 acc=0.7625 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0623 acc=0.9812 | Elapsed: 0.23s\n",
      "epoch 10 val   | loss=0.6726 acc=0.7319 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0530 acc=0.9867 | Elapsed: 0.24s\n",
      "epoch 11 val   | loss=0.8489 acc=0.7475 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0357 acc=0.9917 | Elapsed: 0.24s\n",
      "epoch 12 val   | loss=0.8239 acc=0.7612 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0271 acc=0.9930 | Elapsed: 0.27s\n",
      "epoch 13 val   | loss=1.0032 acc=0.7567 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0244 acc=0.9942 | Elapsed: 0.26s\n",
      "epoch 14 val   | loss=0.9502 acc=0.7508 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0188 acc=0.9959 | Elapsed: 0.25s\n",
      "epoch 15 val   | loss=0.9358 acc=0.7404 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0166 acc=0.9964 | Elapsed: 0.26s\n",
      "epoch 16 val   | loss=1.0449 acc=0.7241 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0178 acc=0.9950 | Elapsed: 0.25s\n",
      "epoch 17 val   | loss=1.0492 acc=0.7235 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0204 acc=0.9959 | Elapsed: 0.24s\n",
      "epoch 18 val   | loss=1.0001 acc=0.7365 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0140 acc=0.9964 | Elapsed: 0.25s\n",
      "epoch 19 val   | loss=1.1634 acc=0.7586 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0088 acc=0.9986 | Elapsed: 0.26s\n",
      "epoch 20 val   | loss=1.0613 acc=0.7677 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7963 best_val_loss=0.4631 best_epoch=5 elapsed=5.46s\n",
      "\n",
      "GRID RUN 24/36\n",
      "run start | seed=23 batch=256 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6758 acc=0.5962 | Elapsed: 0.29s\n",
      "epoch 1 val   | loss=0.6679 acc=0.6142 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6142 at epoch 1 (val_loss=0.6679)\n",
      "epoch 2 train | loss=0.6611 acc=0.5953 | Elapsed: 0.26s\n",
      "epoch 2 val   | loss=0.6503 acc=0.6220 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6220 at epoch 2 (val_loss=0.6503)\n",
      "epoch 3 train | loss=0.6379 acc=0.6502 | Elapsed: 0.27s\n",
      "epoch 3 val   | loss=0.6103 acc=0.6780 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6780 at epoch 3 (val_loss=0.6103)\n",
      "epoch 4 train | loss=0.5605 acc=0.7441 | Elapsed: 0.25s\n",
      "epoch 4 val   | loss=0.5187 acc=0.7729 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7729 at epoch 4 (val_loss=0.5187)\n",
      "epoch 5 train | loss=0.5145 acc=0.7714 | Elapsed: 0.26s\n",
      "epoch 5 val   | loss=0.4878 acc=0.7671 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.4857 acc=0.7869 | Elapsed: 0.25s\n",
      "epoch 6 val   | loss=0.4756 acc=0.7788 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7788 at epoch 6 (val_loss=0.4756)\n",
      "epoch 7 train | loss=0.4694 acc=0.7975 | Elapsed: 0.28s\n",
      "epoch 7 val   | loss=0.4659 acc=0.7872 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7872 at epoch 7 (val_loss=0.4659)\n",
      "epoch 8 train | loss=0.4321 acc=0.8228 | Elapsed: 0.25s\n",
      "epoch 8 val   | loss=0.4631 acc=0.7892 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7892 at epoch 8 (val_loss=0.4631)\n",
      "epoch 9 train | loss=0.4156 acc=0.8292 | Elapsed: 0.27s\n",
      "epoch 9 val   | loss=0.4498 acc=0.8048 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8048 at epoch 9 (val_loss=0.4498)\n",
      "epoch 10 train | loss=0.4170 acc=0.8261 | Elapsed: 0.27s\n",
      "epoch 10 val   | loss=0.4484 acc=0.8035 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.3934 acc=0.8402 | Elapsed: 0.25s\n",
      "epoch 11 val   | loss=0.4516 acc=0.8002 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.3743 acc=0.8494 | Elapsed: 0.26s\n",
      "epoch 12 val   | loss=0.4437 acc=0.8009 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.3380 acc=0.8689 | Elapsed: 0.25s\n",
      "epoch 13 val   | loss=0.4594 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.3162 acc=0.8808 | Elapsed: 0.27s\n",
      "epoch 14 val   | loss=0.4619 acc=0.8002 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.2963 acc=0.8877 | Elapsed: 0.26s\n",
      "epoch 15 val   | loss=0.4704 acc=0.7996 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.2761 acc=0.8939 | Elapsed: 0.25s\n",
      "epoch 16 val   | loss=0.4867 acc=0.7957 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.2517 acc=0.9097 | Elapsed: 0.26s\n",
      "epoch 17 val   | loss=0.5100 acc=0.7963 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.2358 acc=0.9195 | Elapsed: 0.26s\n",
      "epoch 18 val   | loss=0.5192 acc=0.7963 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.2045 acc=0.9306 | Elapsed: 0.25s\n",
      "epoch 19 val   | loss=0.5344 acc=0.7911 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.2034 acc=0.9366 | Elapsed: 0.25s\n",
      "epoch 20 val   | loss=0.6184 acc=0.7476 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8048 best_val_loss=0.4498 best_epoch=9 elapsed=5.63s\n",
      "\n",
      "GRID RUN 25/36\n",
      "run start | seed=999 batch=32 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5781 acc=0.7123 | Elapsed: 0.78s\n",
      "epoch 1 val   | loss=0.4541 acc=0.7912 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7912 at epoch 1 (val_loss=0.4541)\n",
      "epoch 2 train | loss=0.3865 acc=0.8378 | Elapsed: 0.76s\n",
      "epoch 2 val   | loss=0.4750 acc=0.7884 | Elapsed: 0.05s\n",
      "epoch 3 train | loss=0.1802 acc=0.9344 | Elapsed: 0.76s\n",
      "epoch 3 val   | loss=0.5915 acc=0.7686 | Elapsed: 0.04s\n",
      "epoch 4 train | loss=0.0733 acc=0.9754 | Elapsed: 0.79s\n",
      "epoch 4 val   | loss=0.9726 acc=0.7047 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0362 acc=0.9887 | Elapsed: 0.75s\n",
      "epoch 5 val   | loss=0.9255 acc=0.7785 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0176 acc=0.9950 | Elapsed: 0.82s\n",
      "epoch 6 val   | loss=0.9674 acc=0.7488 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0173 acc=0.9948 | Elapsed: 0.75s\n",
      "epoch 7 val   | loss=0.9416 acc=0.7517 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.0108 acc=0.9964 | Elapsed: 0.73s\n",
      "epoch 8 val   | loss=1.0188 acc=0.7574 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0100 acc=0.9969 | Elapsed: 0.77s\n",
      "epoch 9 val   | loss=1.1154 acc=0.7488 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.0079 acc=0.9977 | Elapsed: 0.73s\n",
      "epoch 10 val   | loss=1.0716 acc=0.7572 | Elapsed: 0.04s\n",
      "epoch 11 train | loss=0.0059 acc=0.9966 | Elapsed: 0.76s\n",
      "epoch 11 val   | loss=1.3485 acc=0.7670 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0239 acc=0.9926 | Elapsed: 0.73s\n",
      "epoch 12 val   | loss=1.0605 acc=0.7574 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0100 acc=0.9964 | Elapsed: 0.74s\n",
      "epoch 13 val   | loss=1.2170 acc=0.7105 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0083 acc=0.9979 | Elapsed: 0.72s\n",
      "epoch 14 val   | loss=1.1946 acc=0.7628 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0053 acc=0.9982 | Elapsed: 0.74s\n",
      "epoch 15 val   | loss=1.2951 acc=0.7333 | Elapsed: 0.04s\n",
      "epoch 16 train | loss=0.0044 acc=0.9984 | Elapsed: 0.73s\n",
      "epoch 16 val   | loss=1.3552 acc=0.7248 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0032 acc=0.9987 | Elapsed: 0.74s\n",
      "epoch 17 val   | loss=1.3285 acc=0.7064 | Elapsed: 0.04s\n",
      "epoch 18 train | loss=0.0022 acc=0.9994 | Elapsed: 0.72s\n",
      "epoch 18 val   | loss=1.4708 acc=0.7220 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0022 acc=0.9994 | Elapsed: 0.74s\n",
      "epoch 19 val   | loss=1.3466 acc=0.7501 | Elapsed: 0.05s\n",
      "epoch 20 train | loss=0.0015 acc=0.9994 | Elapsed: 0.72s\n",
      "epoch 20 val   | loss=1.5357 acc=0.6951 | Elapsed: 0.04s\n",
      "run end | best_val_acc=0.7912 best_val_loss=0.4541 best_epoch=1 elapsed=15.74s\n",
      "\n",
      "GRID RUN 26/36\n",
      "run start | seed=999 batch=32 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5542 acc=0.7257 | Elapsed: 0.76s\n",
      "epoch 1 val   | loss=0.4543 acc=0.7997 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7997 at epoch 1 (val_loss=0.4543)\n",
      "epoch 2 train | loss=0.4219 acc=0.8175 | Elapsed: 0.74s\n",
      "epoch 2 val   | loss=0.4494 acc=0.7940 | Elapsed: 0.04s\n",
      "epoch 3 train | loss=0.2852 acc=0.8884 | Elapsed: 0.77s\n",
      "epoch 3 val   | loss=0.4684 acc=0.7997 | Elapsed: 0.04s\n",
      "epoch 4 train | loss=0.1494 acc=0.9464 | Elapsed: 0.76s\n",
      "epoch 4 val   | loss=0.5581 acc=0.7912 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.0664 acc=0.9790 | Elapsed: 0.77s\n",
      "epoch 5 val   | loss=0.6761 acc=0.7798 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0332 acc=0.9916 | Elapsed: 0.76s\n",
      "epoch 6 val   | loss=0.9512 acc=0.7642 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.0252 acc=0.9932 | Elapsed: 0.75s\n",
      "epoch 7 val   | loss=0.8540 acc=0.7798 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.0128 acc=0.9960 | Elapsed: 0.74s\n",
      "epoch 8 val   | loss=0.8944 acc=0.7798 | Elapsed: 0.05s\n",
      "epoch 9 train | loss=0.0111 acc=0.9976 | Elapsed: 0.75s\n",
      "epoch 9 val   | loss=1.1806 acc=0.7514 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.0068 acc=0.9985 | Elapsed: 0.76s\n",
      "epoch 10 val   | loss=1.0521 acc=0.7798 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0044 acc=0.9984 | Elapsed: 0.78s\n",
      "epoch 11 val   | loss=1.0535 acc=0.7670 | Elapsed: 0.05s\n",
      "epoch 12 train | loss=0.0046 acc=0.9987 | Elapsed: 0.74s\n",
      "epoch 12 val   | loss=1.2435 acc=0.7601 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0234 acc=0.9926 | Elapsed: 0.75s\n",
      "epoch 13 val   | loss=1.0826 acc=0.6836 | Elapsed: 0.05s\n",
      "epoch 14 train | loss=0.0113 acc=0.9968 | Elapsed: 0.76s\n",
      "epoch 14 val   | loss=1.1669 acc=0.7614 | Elapsed: 0.05s\n",
      "epoch 15 train | loss=0.0086 acc=0.9974 | Elapsed: 0.75s\n",
      "epoch 15 val   | loss=1.1627 acc=0.7487 | Elapsed: 0.04s\n",
      "epoch 16 train | loss=0.0130 acc=0.9969 | Elapsed: 0.78s\n",
      "epoch 16 val   | loss=1.2018 acc=0.7432 | Elapsed: 0.05s\n",
      "epoch 17 train | loss=0.0031 acc=0.9989 | Elapsed: 0.75s\n",
      "epoch 17 val   | loss=1.2024 acc=0.7559 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0012 acc=0.9995 | Elapsed: 0.73s\n",
      "epoch 18 val   | loss=1.3584 acc=0.7585 | Elapsed: 0.04s\n",
      "epoch 19 train | loss=0.0025 acc=0.9992 | Elapsed: 0.73s\n",
      "epoch 19 val   | loss=1.3302 acc=0.7614 | Elapsed: 0.05s\n",
      "epoch 20 train | loss=0.0021 acc=0.9990 | Elapsed: 0.73s\n",
      "epoch 20 val   | loss=1.2696 acc=0.7417 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7997 best_val_loss=0.4543 best_epoch=1 elapsed=15.89s\n",
      "\n",
      "GRID RUN 27/36\n",
      "run start | seed=999 batch=32 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6352 acc=0.6376 | Elapsed: 0.77s\n",
      "epoch 1 val   | loss=0.4946 acc=0.7785 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7785 at epoch 1 (val_loss=0.4946)\n",
      "epoch 2 train | loss=0.4974 acc=0.7778 | Elapsed: 0.74s\n",
      "epoch 2 val   | loss=0.4576 acc=0.7969 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7969 at epoch 2 (val_loss=0.4576)\n",
      "epoch 3 train | loss=0.4498 acc=0.8093 | Elapsed: 0.72s\n",
      "epoch 3 val   | loss=0.4441 acc=0.7983 | Elapsed: 0.04s\n",
      "new best: val_acc=0.7983 at epoch 3 (val_loss=0.4441)\n",
      "epoch 4 train | loss=0.4085 acc=0.8303 | Elapsed: 0.74s\n",
      "epoch 4 val   | loss=0.4582 acc=0.7884 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.3618 acc=0.8540 | Elapsed: 0.74s\n",
      "epoch 5 val   | loss=0.4615 acc=0.7884 | Elapsed: 0.05s\n",
      "epoch 6 train | loss=0.3194 acc=0.8739 | Elapsed: 0.73s\n",
      "epoch 6 val   | loss=0.4814 acc=0.7869 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.2628 acc=0.8991 | Elapsed: 0.73s\n",
      "epoch 7 val   | loss=0.5105 acc=0.7729 | Elapsed: 0.05s\n",
      "epoch 8 train | loss=0.2178 acc=0.9226 | Elapsed: 0.73s\n",
      "epoch 8 val   | loss=0.6002 acc=0.7729 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.1763 acc=0.9362 | Elapsed: 0.71s\n",
      "epoch 9 val   | loss=0.6729 acc=0.7430 | Elapsed: 0.04s\n",
      "epoch 10 train | loss=0.1416 acc=0.9548 | Elapsed: 0.74s\n",
      "epoch 10 val   | loss=0.6749 acc=0.7288 | Elapsed: 0.04s\n",
      "epoch 11 train | loss=0.1126 acc=0.9641 | Elapsed: 0.73s\n",
      "epoch 11 val   | loss=0.7177 acc=0.7146 | Elapsed: 0.04s\n",
      "epoch 12 train | loss=0.0955 acc=0.9717 | Elapsed: 0.74s\n",
      "epoch 12 val   | loss=0.7497 acc=0.7572 | Elapsed: 0.04s\n",
      "epoch 13 train | loss=0.0776 acc=0.9754 | Elapsed: 0.75s\n",
      "epoch 13 val   | loss=0.7838 acc=0.7558 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.0710 acc=0.9798 | Elapsed: 0.78s\n",
      "epoch 14 val   | loss=0.7725 acc=0.7388 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0548 acc=0.9835 | Elapsed: 0.76s\n",
      "epoch 15 val   | loss=0.9104 acc=0.7217 | Elapsed: 0.04s\n",
      "epoch 16 train | loss=0.0466 acc=0.9875 | Elapsed: 0.76s\n",
      "epoch 16 val   | loss=1.0075 acc=0.7274 | Elapsed: 0.04s\n",
      "epoch 17 train | loss=0.0403 acc=0.9879 | Elapsed: 0.77s\n",
      "epoch 17 val   | loss=0.9983 acc=0.7288 | Elapsed: 0.05s\n",
      "epoch 18 train | loss=0.0343 acc=0.9896 | Elapsed: 0.79s\n",
      "epoch 18 val   | loss=1.0712 acc=0.7444 | Elapsed: 0.04s\n",
      "epoch 19 train | loss=0.0313 acc=0.9906 | Elapsed: 0.76s\n",
      "epoch 19 val   | loss=1.0549 acc=0.7231 | Elapsed: 0.04s\n",
      "epoch 20 train | loss=0.0264 acc=0.9926 | Elapsed: 0.76s\n",
      "epoch 20 val   | loss=1.1019 acc=0.7429 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7983 best_val_loss=0.4441 best_epoch=3 elapsed=15.75s\n",
      "\n",
      "GRID RUN 28/36\n",
      "run start | seed=999 batch=64 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5485 acc=0.7297 | Elapsed: 0.49s\n",
      "epoch 1 val   | loss=0.4476 acc=0.8019 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8019 at epoch 1 (val_loss=0.4476)\n",
      "epoch 2 train | loss=0.3933 acc=0.8352 | Elapsed: 0.48s\n",
      "epoch 2 val   | loss=0.4902 acc=0.7650 | Elapsed: 0.03s\n",
      "epoch 3 train | loss=0.2291 acc=0.9116 | Elapsed: 0.49s\n",
      "epoch 3 val   | loss=0.5480 acc=0.7700 | Elapsed: 0.03s\n",
      "epoch 4 train | loss=0.0982 acc=0.9688 | Elapsed: 0.48s\n",
      "epoch 4 val   | loss=0.6733 acc=0.7707 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.0379 acc=0.9882 | Elapsed: 0.48s\n",
      "epoch 5 val   | loss=0.8252 acc=0.7438 | Elapsed: 0.03s\n",
      "epoch 6 train | loss=0.0211 acc=0.9939 | Elapsed: 0.47s\n",
      "epoch 6 val   | loss=0.9265 acc=0.7396 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0350 acc=0.9899 | Elapsed: 0.49s\n",
      "epoch 7 val   | loss=0.9386 acc=0.6984 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0138 acc=0.9957 | Elapsed: 0.50s\n",
      "epoch 8 val   | loss=0.9828 acc=0.7481 | Elapsed: 0.04s\n",
      "epoch 9 train | loss=0.0107 acc=0.9971 | Elapsed: 0.48s\n",
      "epoch 9 val   | loss=1.1102 acc=0.7289 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0083 acc=0.9977 | Elapsed: 0.50s\n",
      "epoch 10 val   | loss=1.1067 acc=0.7239 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0058 acc=0.9974 | Elapsed: 0.49s\n",
      "epoch 11 val   | loss=1.2204 acc=0.7324 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0169 acc=0.9966 | Elapsed: 0.48s\n",
      "epoch 12 val   | loss=1.0673 acc=0.7317 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0046 acc=0.9986 | Elapsed: 0.46s\n",
      "epoch 13 val   | loss=1.2154 acc=0.7565 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0019 acc=0.9994 | Elapsed: 0.48s\n",
      "epoch 14 val   | loss=1.2061 acc=0.7431 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0032 acc=0.9989 | Elapsed: 0.47s\n",
      "epoch 15 val   | loss=1.2884 acc=0.7544 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0029 acc=0.9990 | Elapsed: 0.47s\n",
      "epoch 16 val   | loss=1.2928 acc=0.7104 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0016 acc=0.9997 | Elapsed: 0.46s\n",
      "epoch 17 val   | loss=1.3322 acc=0.7310 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0008 acc=0.9998 | Elapsed: 0.47s\n",
      "epoch 18 val   | loss=1.4975 acc=0.7601 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0008 acc=0.9998 | Elapsed: 0.49s\n",
      "epoch 19 val   | loss=1.4093 acc=0.7183 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0011 acc=0.9997 | Elapsed: 0.48s\n",
      "epoch 20 val   | loss=1.3981 acc=0.7239 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.8019 best_val_loss=0.4476 best_epoch=1 elapsed=10.22s\n",
      "\n",
      "GRID RUN 29/36\n",
      "run start | seed=999 batch=64 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.5711 acc=0.7102 | Elapsed: 0.51s\n",
      "epoch 1 val   | loss=0.4639 acc=0.7863 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7863 at epoch 1 (val_loss=0.4639)\n",
      "epoch 2 train | loss=0.4408 acc=0.8101 | Elapsed: 0.45s\n",
      "epoch 2 val   | loss=0.4499 acc=0.7891 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7891 at epoch 2 (val_loss=0.4499)\n",
      "epoch 3 train | loss=0.3384 acc=0.8623 | Elapsed: 0.50s\n",
      "epoch 3 val   | loss=0.4682 acc=0.7899 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7899 at epoch 3 (val_loss=0.4682)\n",
      "epoch 4 train | loss=0.2277 acc=0.9164 | Elapsed: 0.47s\n",
      "epoch 4 val   | loss=0.5347 acc=0.7665 | Elapsed: 0.03s\n",
      "epoch 5 train | loss=0.1238 acc=0.9586 | Elapsed: 0.49s\n",
      "epoch 5 val   | loss=0.6009 acc=0.7636 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.0677 acc=0.9791 | Elapsed: 0.47s\n",
      "epoch 6 val   | loss=0.7138 acc=0.7622 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.0464 acc=0.9858 | Elapsed: 0.47s\n",
      "epoch 7 val   | loss=0.8245 acc=0.7580 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0253 acc=0.9937 | Elapsed: 0.45s\n",
      "epoch 8 val   | loss=0.9372 acc=0.7580 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.0222 acc=0.9952 | Elapsed: 0.46s\n",
      "epoch 9 val   | loss=0.9867 acc=0.7594 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0142 acc=0.9961 | Elapsed: 0.48s\n",
      "epoch 10 val   | loss=1.1134 acc=0.7551 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0093 acc=0.9977 | Elapsed: 0.51s\n",
      "epoch 11 val   | loss=1.2001 acc=0.7551 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0208 acc=0.9936 | Elapsed: 0.46s\n",
      "epoch 12 val   | loss=1.0198 acc=0.7565 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0063 acc=0.9986 | Elapsed: 0.47s\n",
      "epoch 13 val   | loss=1.0702 acc=0.7558 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0052 acc=0.9989 | Elapsed: 0.47s\n",
      "epoch 14 val   | loss=1.1595 acc=0.7587 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0092 acc=0.9977 | Elapsed: 0.50s\n",
      "epoch 15 val   | loss=1.0903 acc=0.7750 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0072 acc=0.9986 | Elapsed: 0.49s\n",
      "epoch 16 val   | loss=1.2139 acc=0.7367 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0035 acc=0.9990 | Elapsed: 0.52s\n",
      "epoch 17 val   | loss=1.2264 acc=0.7608 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0028 acc=0.9990 | Elapsed: 0.52s\n",
      "epoch 18 val   | loss=1.3946 acc=0.7572 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0056 acc=0.9977 | Elapsed: 0.50s\n",
      "epoch 19 val   | loss=1.1862 acc=0.7601 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0023 acc=0.9994 | Elapsed: 0.50s\n",
      "epoch 20 val   | loss=1.2921 acc=0.7416 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7899 best_val_loss=0.4682 best_epoch=3 elapsed=10.26s\n",
      "\n",
      "GRID RUN 30/36\n",
      "run start | seed=999 batch=64 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6693 acc=0.5975 | Elapsed: 0.51s\n",
      "epoch 1 val   | loss=0.6266 acc=0.6628 | Elapsed: 0.03s\n",
      "new best: val_acc=0.6628 at epoch 1 (val_loss=0.6266)\n",
      "epoch 2 train | loss=0.5437 acc=0.7434 | Elapsed: 0.48s\n",
      "epoch 2 val   | loss=0.4778 acc=0.7934 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7934 at epoch 2 (val_loss=0.4778)\n",
      "epoch 3 train | loss=0.4781 acc=0.7869 | Elapsed: 0.53s\n",
      "epoch 3 val   | loss=0.4555 acc=0.7977 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7977 at epoch 3 (val_loss=0.4555)\n",
      "epoch 4 train | loss=0.4423 acc=0.8114 | Elapsed: 0.51s\n",
      "epoch 4 val   | loss=0.4671 acc=0.7849 | Elapsed: 0.04s\n",
      "epoch 5 train | loss=0.4075 acc=0.8283 | Elapsed: 0.58s\n",
      "epoch 5 val   | loss=0.4534 acc=0.7948 | Elapsed: 0.04s\n",
      "epoch 6 train | loss=0.3739 acc=0.8507 | Elapsed: 0.58s\n",
      "epoch 6 val   | loss=0.4677 acc=0.7842 | Elapsed: 0.04s\n",
      "epoch 7 train | loss=0.3391 acc=0.8642 | Elapsed: 0.57s\n",
      "epoch 7 val   | loss=0.4689 acc=0.7835 | Elapsed: 0.04s\n",
      "epoch 8 train | loss=0.2971 acc=0.8860 | Elapsed: 0.57s\n",
      "epoch 8 val   | loss=0.5071 acc=0.7764 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.2566 acc=0.9051 | Elapsed: 0.54s\n",
      "epoch 9 val   | loss=0.5805 acc=0.7509 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.2184 acc=0.9225 | Elapsed: 0.54s\n",
      "epoch 10 val   | loss=0.6464 acc=0.7246 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.2005 acc=0.9270 | Elapsed: 0.54s\n",
      "epoch 11 val   | loss=0.5567 acc=0.7580 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.1665 acc=0.9467 | Elapsed: 0.54s\n",
      "epoch 12 val   | loss=0.6541 acc=0.7388 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.1281 acc=0.9583 | Elapsed: 0.55s\n",
      "epoch 13 val   | loss=0.6676 acc=0.7572 | Elapsed: 0.04s\n",
      "epoch 14 train | loss=0.1110 acc=0.9657 | Elapsed: 0.54s\n",
      "epoch 14 val   | loss=0.7620 acc=0.7218 | Elapsed: 0.04s\n",
      "epoch 15 train | loss=0.0977 acc=0.9700 | Elapsed: 0.51s\n",
      "epoch 15 val   | loss=0.7892 acc=0.7374 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0806 acc=0.9771 | Elapsed: 0.54s\n",
      "epoch 16 val   | loss=0.8724 acc=0.7175 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0734 acc=0.9797 | Elapsed: 0.55s\n",
      "epoch 17 val   | loss=0.8669 acc=0.7381 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0633 acc=0.9828 | Elapsed: 0.54s\n",
      "epoch 18 val   | loss=0.9712 acc=0.7359 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0561 acc=0.9842 | Elapsed: 0.54s\n",
      "epoch 19 val   | loss=0.9368 acc=0.7381 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0560 acc=0.9844 | Elapsed: 0.56s\n",
      "epoch 20 val   | loss=0.8923 acc=0.7282 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.7977 best_val_loss=0.4555 best_epoch=3 elapsed=11.46s\n",
      "\n",
      "GRID RUN 31/36\n",
      "run start | seed=999 batch=128 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5676 acc=0.7138 | Elapsed: 0.37s\n",
      "epoch 1 val   | loss=0.4519 acc=0.8035 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8035 at epoch 1 (val_loss=0.4519)\n",
      "epoch 2 train | loss=0.4181 acc=0.8235 | Elapsed: 0.36s\n",
      "epoch 2 val   | loss=0.4373 acc=0.8048 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8048 at epoch 2 (val_loss=0.4373)\n",
      "epoch 3 train | loss=0.2819 acc=0.8884 | Elapsed: 0.35s\n",
      "epoch 3 val   | loss=0.5850 acc=0.7270 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.1658 acc=0.9346 | Elapsed: 0.35s\n",
      "epoch 4 val   | loss=0.5839 acc=0.7815 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.0817 acc=0.9724 | Elapsed: 0.35s\n",
      "epoch 5 val   | loss=0.6713 acc=0.7633 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.0532 acc=0.9842 | Elapsed: 0.35s\n",
      "epoch 6 val   | loss=0.7964 acc=0.7321 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0995 acc=0.9659 | Elapsed: 0.37s\n",
      "epoch 7 val   | loss=0.7543 acc=0.7620 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0193 acc=0.9947 | Elapsed: 0.36s\n",
      "epoch 8 val   | loss=0.9276 acc=0.7360 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0161 acc=0.9967 | Elapsed: 0.35s\n",
      "epoch 9 val   | loss=0.9664 acc=0.7477 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.0118 acc=0.9974 | Elapsed: 0.38s\n",
      "epoch 10 val   | loss=0.9600 acc=0.7386 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.0093 acc=0.9957 | Elapsed: 0.36s\n",
      "epoch 11 val   | loss=0.9867 acc=0.7529 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.0283 acc=0.9933 | Elapsed: 0.35s\n",
      "epoch 12 val   | loss=0.9791 acc=0.7204 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0152 acc=0.9976 | Elapsed: 0.37s\n",
      "epoch 13 val   | loss=0.9723 acc=0.7439 | Elapsed: 0.03s\n",
      "epoch 14 train | loss=0.0115 acc=0.9986 | Elapsed: 0.37s\n",
      "epoch 14 val   | loss=1.0299 acc=0.7516 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0078 acc=0.9986 | Elapsed: 0.38s\n",
      "epoch 15 val   | loss=1.0500 acc=0.7503 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.0038 acc=0.9992 | Elapsed: 0.38s\n",
      "epoch 16 val   | loss=1.0618 acc=0.7490 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0024 acc=0.9994 | Elapsed: 0.38s\n",
      "epoch 17 val   | loss=1.0889 acc=0.7503 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0014 acc=0.9997 | Elapsed: 0.40s\n",
      "epoch 18 val   | loss=1.2826 acc=0.7555 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0034 acc=0.9992 | Elapsed: 0.38s\n",
      "epoch 19 val   | loss=1.1479 acc=0.7503 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0030 acc=0.9990 | Elapsed: 0.39s\n",
      "epoch 20 val   | loss=1.1983 acc=0.7594 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.8048 best_val_loss=0.4373 best_epoch=2 elapsed=7.89s\n",
      "\n",
      "GRID RUN 32/36\n",
      "run start | seed=999 batch=128 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.6061 acc=0.6783 | Elapsed: 0.39s\n",
      "epoch 1 val   | loss=0.5215 acc=0.7503 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7503 at epoch 1 (val_loss=0.5215)\n",
      "epoch 2 train | loss=0.4773 acc=0.7892 | Elapsed: 0.38s\n",
      "epoch 2 val   | loss=0.4609 acc=0.7918 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7918 at epoch 2 (val_loss=0.4609)\n",
      "epoch 3 train | loss=0.3950 acc=0.8383 | Elapsed: 0.37s\n",
      "epoch 3 val   | loss=0.4378 acc=0.7971 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7971 at epoch 3 (val_loss=0.4378)\n",
      "epoch 4 train | loss=0.3057 acc=0.8787 | Elapsed: 0.36s\n",
      "epoch 4 val   | loss=0.4588 acc=0.8049 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8049 at epoch 4 (val_loss=0.4588)\n",
      "epoch 5 train | loss=0.2101 acc=0.9235 | Elapsed: 0.38s\n",
      "epoch 5 val   | loss=0.5449 acc=0.7879 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.1463 acc=0.9480 | Elapsed: 0.36s\n",
      "epoch 6 val   | loss=0.5833 acc=0.7516 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.0856 acc=0.9746 | Elapsed: 0.36s\n",
      "epoch 7 val   | loss=0.6588 acc=0.7711 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.0654 acc=0.9809 | Elapsed: 0.37s\n",
      "epoch 8 val   | loss=0.6752 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0516 acc=0.9858 | Elapsed: 0.37s\n",
      "epoch 9 val   | loss=0.8807 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0327 acc=0.9920 | Elapsed: 0.37s\n",
      "epoch 10 val   | loss=0.8775 acc=0.7684 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0210 acc=0.9933 | Elapsed: 0.34s\n",
      "epoch 11 val   | loss=0.9443 acc=0.7645 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0341 acc=0.9919 | Elapsed: 0.34s\n",
      "epoch 12 val   | loss=0.8708 acc=0.7502 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.0217 acc=0.9957 | Elapsed: 0.36s\n",
      "epoch 13 val   | loss=0.8729 acc=0.7567 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0118 acc=0.9976 | Elapsed: 0.36s\n",
      "epoch 14 val   | loss=1.1046 acc=0.7529 | Elapsed: 0.03s\n",
      "epoch 15 train | loss=0.0074 acc=0.9986 | Elapsed: 0.38s\n",
      "epoch 15 val   | loss=1.1645 acc=0.7515 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0057 acc=0.9994 | Elapsed: 0.35s\n",
      "epoch 16 val   | loss=1.2480 acc=0.7373 | Elapsed: 0.03s\n",
      "epoch 17 train | loss=0.0052 acc=0.9989 | Elapsed: 0.36s\n",
      "epoch 17 val   | loss=1.2017 acc=0.7451 | Elapsed: 0.03s\n",
      "epoch 18 train | loss=0.0038 acc=0.9995 | Elapsed: 0.38s\n",
      "epoch 18 val   | loss=1.1820 acc=0.7646 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.0036 acc=0.9992 | Elapsed: 0.37s\n",
      "epoch 19 val   | loss=1.2192 acc=0.7567 | Elapsed: 0.03s\n",
      "epoch 20 train | loss=0.0044 acc=0.9989 | Elapsed: 0.38s\n",
      "epoch 20 val   | loss=1.2266 acc=0.7645 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.8049 best_val_loss=0.4588 best_epoch=4 elapsed=7.83s\n",
      "\n",
      "GRID RUN 33/36\n",
      "run start | seed=999 batch=128 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6780 acc=0.5832 | Elapsed: 0.37s\n",
      "epoch 1 val   | loss=0.6585 acc=0.6176 | Elapsed: 0.03s\n",
      "new best: val_acc=0.6176 at epoch 1 (val_loss=0.6585)\n",
      "epoch 2 train | loss=0.6331 acc=0.6519 | Elapsed: 0.37s\n",
      "epoch 2 val   | loss=0.5633 acc=0.7399 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7399 at epoch 2 (val_loss=0.5633)\n",
      "epoch 3 train | loss=0.5256 acc=0.7602 | Elapsed: 0.38s\n",
      "epoch 3 val   | loss=0.4731 acc=0.7997 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7997 at epoch 3 (val_loss=0.4731)\n",
      "epoch 4 train | loss=0.4795 acc=0.7857 | Elapsed: 0.38s\n",
      "epoch 4 val   | loss=0.4577 acc=0.8022 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8022 at epoch 4 (val_loss=0.4577)\n",
      "epoch 5 train | loss=0.4488 acc=0.8039 | Elapsed: 0.39s\n",
      "epoch 5 val   | loss=0.4462 acc=0.8048 | Elapsed: 0.03s\n",
      "new best: val_acc=0.8048 at epoch 5 (val_loss=0.4462)\n",
      "epoch 6 train | loss=0.4273 acc=0.8206 | Elapsed: 0.39s\n",
      "epoch 6 val   | loss=0.4468 acc=0.7970 | Elapsed: 0.03s\n",
      "epoch 7 train | loss=0.4010 acc=0.8351 | Elapsed: 0.37s\n",
      "epoch 7 val   | loss=0.4475 acc=0.7879 | Elapsed: 0.03s\n",
      "epoch 8 train | loss=0.3725 acc=0.8477 | Elapsed: 0.39s\n",
      "epoch 8 val   | loss=0.4521 acc=0.7932 | Elapsed: 0.03s\n",
      "epoch 9 train | loss=0.3495 acc=0.8646 | Elapsed: 0.37s\n",
      "epoch 9 val   | loss=0.4745 acc=0.7750 | Elapsed: 0.03s\n",
      "epoch 10 train | loss=0.3178 acc=0.8772 | Elapsed: 0.37s\n",
      "epoch 10 val   | loss=0.5107 acc=0.7633 | Elapsed: 0.03s\n",
      "epoch 11 train | loss=0.2958 acc=0.8879 | Elapsed: 0.37s\n",
      "epoch 11 val   | loss=0.5031 acc=0.7802 | Elapsed: 0.03s\n",
      "epoch 12 train | loss=0.2595 acc=0.9070 | Elapsed: 0.37s\n",
      "epoch 12 val   | loss=0.5426 acc=0.7491 | Elapsed: 0.03s\n",
      "epoch 13 train | loss=0.2264 acc=0.9201 | Elapsed: 0.37s\n",
      "epoch 13 val   | loss=0.5803 acc=0.7386 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.1996 acc=0.9310 | Elapsed: 0.35s\n",
      "epoch 14 val   | loss=0.6032 acc=0.7672 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.1735 acc=0.9434 | Elapsed: 0.37s\n",
      "epoch 15 val   | loss=0.6179 acc=0.7620 | Elapsed: 0.03s\n",
      "epoch 16 train | loss=0.1554 acc=0.9491 | Elapsed: 0.35s\n",
      "epoch 16 val   | loss=0.6789 acc=0.7568 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.1324 acc=0.9579 | Elapsed: 0.36s\n",
      "epoch 17 val   | loss=0.6735 acc=0.7633 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.1197 acc=0.9593 | Elapsed: 0.37s\n",
      "epoch 18 val   | loss=0.7638 acc=0.7100 | Elapsed: 0.03s\n",
      "epoch 19 train | loss=0.1091 acc=0.9654 | Elapsed: 0.36s\n",
      "epoch 19 val   | loss=0.7241 acc=0.7594 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.1012 acc=0.9676 | Elapsed: 0.36s\n",
      "epoch 20 val   | loss=0.7572 acc=0.7503 | Elapsed: 0.03s\n",
      "run end | best_val_acc=0.8048 best_val_loss=0.4462 best_epoch=5 elapsed=7.95s\n",
      "\n",
      "GRID RUN 34/36\n",
      "run start | seed=999 batch=256 lr=0.001 epochs=20\n",
      "epoch 1 train | loss=0.5958 acc=0.6892 | Elapsed: 0.29s\n",
      "epoch 1 val   | loss=0.5065 acc=0.7684 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7684 at epoch 1 (val_loss=0.5065)\n",
      "epoch 2 train | loss=0.4604 acc=0.7984 | Elapsed: 0.26s\n",
      "epoch 2 val   | loss=0.4535 acc=0.7996 | Elapsed: 0.03s\n",
      "new best: val_acc=0.7996 at epoch 2 (val_loss=0.4535)\n",
      "epoch 3 train | loss=0.3665 acc=0.8473 | Elapsed: 0.26s\n",
      "epoch 3 val   | loss=0.4705 acc=0.7912 | Elapsed: 0.02s\n",
      "epoch 4 train | loss=0.2723 acc=0.8939 | Elapsed: 0.29s\n",
      "epoch 4 val   | loss=0.5336 acc=0.7697 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.1765 acc=0.9352 | Elapsed: 0.26s\n",
      "epoch 5 val   | loss=0.5170 acc=0.7807 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.1178 acc=0.9637 | Elapsed: 0.26s\n",
      "epoch 6 val   | loss=0.7101 acc=0.7020 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.2095 acc=0.9197 | Elapsed: 0.28s\n",
      "epoch 7 val   | loss=0.6108 acc=0.7670 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.0688 acc=0.9786 | Elapsed: 0.45s\n",
      "epoch 8 val   | loss=0.6862 acc=0.7547 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.0397 acc=0.9906 | Elapsed: 0.30s\n",
      "epoch 9 val   | loss=0.7007 acc=0.7696 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0222 acc=0.9955 | Elapsed: 0.24s\n",
      "epoch 10 val   | loss=0.7474 acc=0.7631 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0194 acc=0.9923 | Elapsed: 0.23s\n",
      "epoch 11 val   | loss=0.8220 acc=0.7736 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0292 acc=0.9933 | Elapsed: 0.22s\n",
      "epoch 12 val   | loss=0.8615 acc=0.7592 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0193 acc=0.9967 | Elapsed: 0.23s\n",
      "epoch 13 val   | loss=0.8229 acc=0.7658 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0131 acc=0.9978 | Elapsed: 0.24s\n",
      "epoch 14 val   | loss=0.9007 acc=0.7664 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0108 acc=0.9983 | Elapsed: 0.23s\n",
      "epoch 15 val   | loss=0.9693 acc=0.7671 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0079 acc=0.9983 | Elapsed: 0.23s\n",
      "epoch 16 val   | loss=0.9933 acc=0.7612 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0049 acc=0.9988 | Elapsed: 0.23s\n",
      "epoch 17 val   | loss=0.9541 acc=0.7729 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0030 acc=0.9995 | Elapsed: 0.23s\n",
      "epoch 18 val   | loss=1.1162 acc=0.7684 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0030 acc=0.9991 | Elapsed: 0.24s\n",
      "epoch 19 val   | loss=1.0661 acc=0.7664 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0020 acc=0.9997 | Elapsed: 0.24s\n",
      "epoch 20 val   | loss=1.0645 acc=0.7638 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7996 best_val_loss=0.4535 best_epoch=2 elapsed=5.61s\n",
      "\n",
      "GRID RUN 35/36\n",
      "run start | seed=999 batch=256 lr=0.0005 epochs=20\n",
      "epoch 1 train | loss=0.6367 acc=0.6359 | Elapsed: 0.25s\n",
      "epoch 1 val   | loss=0.5126 acc=0.7632 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7632 at epoch 1 (val_loss=0.5126)\n",
      "epoch 2 train | loss=0.5018 acc=0.7697 | Elapsed: 0.23s\n",
      "epoch 2 val   | loss=0.4618 acc=0.7885 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7885 at epoch 2 (val_loss=0.4618)\n",
      "epoch 3 train | loss=0.4416 acc=0.8080 | Elapsed: 0.23s\n",
      "epoch 3 val   | loss=0.4493 acc=0.8009 | Elapsed: 0.02s\n",
      "new best: val_acc=0.8009 at epoch 3 (val_loss=0.4493)\n",
      "epoch 4 train | loss=0.3786 acc=0.8422 | Elapsed: 0.23s\n",
      "epoch 4 val   | loss=0.4706 acc=0.7762 | Elapsed: 0.02s\n",
      "epoch 5 train | loss=0.3030 acc=0.8812 | Elapsed: 0.23s\n",
      "epoch 5 val   | loss=0.5040 acc=0.7859 | Elapsed: 0.02s\n",
      "epoch 6 train | loss=0.2854 acc=0.8919 | Elapsed: 0.24s\n",
      "epoch 6 val   | loss=0.5158 acc=0.7716 | Elapsed: 0.02s\n",
      "epoch 7 train | loss=0.2631 acc=0.8967 | Elapsed: 0.23s\n",
      "epoch 7 val   | loss=0.4936 acc=0.7963 | Elapsed: 0.02s\n",
      "epoch 8 train | loss=0.1642 acc=0.9447 | Elapsed: 0.23s\n",
      "epoch 8 val   | loss=0.5520 acc=0.7749 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.1248 acc=0.9577 | Elapsed: 0.23s\n",
      "epoch 9 val   | loss=0.6243 acc=0.7638 | Elapsed: 0.02s\n",
      "epoch 10 train | loss=0.0942 acc=0.9716 | Elapsed: 0.23s\n",
      "epoch 10 val   | loss=0.7030 acc=0.7508 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.0793 acc=0.9739 | Elapsed: 0.23s\n",
      "epoch 11 val   | loss=0.7001 acc=0.7749 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.0628 acc=0.9827 | Elapsed: 0.23s\n",
      "epoch 12 val   | loss=0.7343 acc=0.7267 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.0491 acc=0.9858 | Elapsed: 0.23s\n",
      "epoch 13 val   | loss=0.8292 acc=0.7612 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.0317 acc=0.9922 | Elapsed: 0.22s\n",
      "epoch 14 val   | loss=0.8238 acc=0.7384 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.0258 acc=0.9948 | Elapsed: 0.23s\n",
      "epoch 15 val   | loss=0.9738 acc=0.7547 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.0295 acc=0.9920 | Elapsed: 0.23s\n",
      "epoch 16 val   | loss=0.8956 acc=0.7365 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.0168 acc=0.9966 | Elapsed: 0.23s\n",
      "epoch 17 val   | loss=0.9385 acc=0.7430 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.0115 acc=0.9977 | Elapsed: 0.24s\n",
      "epoch 18 val   | loss=1.0691 acc=0.7475 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.0092 acc=0.9986 | Elapsed: 0.23s\n",
      "epoch 19 val   | loss=1.0772 acc=0.7436 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.0091 acc=0.9988 | Elapsed: 0.24s\n",
      "epoch 20 val   | loss=1.0540 acc=0.7560 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.8009 best_val_loss=0.4493 best_epoch=3 elapsed=5.01s\n",
      "\n",
      "GRID RUN 36/36\n",
      "run start | seed=999 batch=256 lr=0.0001 epochs=20\n",
      "epoch 1 train | loss=0.6827 acc=0.5758 | Elapsed: 0.25s\n",
      "epoch 1 val   | loss=0.6727 acc=0.5960 | Elapsed: 0.02s\n",
      "new best: val_acc=0.5960 at epoch 1 (val_loss=0.6727)\n",
      "epoch 2 train | loss=0.6658 acc=0.5953 | Elapsed: 0.23s\n",
      "epoch 2 val   | loss=0.6561 acc=0.6149 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6149 at epoch 2 (val_loss=0.6561)\n",
      "epoch 3 train | loss=0.6460 acc=0.6294 | Elapsed: 0.23s\n",
      "epoch 3 val   | loss=0.6233 acc=0.6676 | Elapsed: 0.02s\n",
      "new best: val_acc=0.6676 at epoch 3 (val_loss=0.6233)\n",
      "epoch 4 train | loss=0.5787 acc=0.7309 | Elapsed: 0.23s\n",
      "epoch 4 val   | loss=0.5141 acc=0.7606 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7606 at epoch 4 (val_loss=0.5141)\n",
      "epoch 5 train | loss=0.5155 acc=0.7616 | Elapsed: 0.23s\n",
      "epoch 5 val   | loss=0.4915 acc=0.7788 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7788 at epoch 5 (val_loss=0.4915)\n",
      "epoch 6 train | loss=0.4894 acc=0.7822 | Elapsed: 0.23s\n",
      "epoch 6 val   | loss=0.4769 acc=0.7853 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7853 at epoch 6 (val_loss=0.4769)\n",
      "epoch 7 train | loss=0.4660 acc=0.7964 | Elapsed: 0.25s\n",
      "epoch 7 val   | loss=0.4674 acc=0.7918 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7918 at epoch 7 (val_loss=0.4674)\n",
      "epoch 8 train | loss=0.4479 acc=0.8102 | Elapsed: 0.24s\n",
      "epoch 8 val   | loss=0.4636 acc=0.7846 | Elapsed: 0.02s\n",
      "epoch 9 train | loss=0.4349 acc=0.8170 | Elapsed: 0.25s\n",
      "epoch 9 val   | loss=0.4570 acc=0.7944 | Elapsed: 0.02s\n",
      "new best: val_acc=0.7944 at epoch 9 (val_loss=0.4570)\n",
      "epoch 10 train | loss=0.4112 acc=0.8322 | Elapsed: 0.24s\n",
      "epoch 10 val   | loss=0.4682 acc=0.7872 | Elapsed: 0.02s\n",
      "epoch 11 train | loss=0.3914 acc=0.8417 | Elapsed: 0.24s\n",
      "epoch 11 val   | loss=0.4626 acc=0.7931 | Elapsed: 0.02s\n",
      "epoch 12 train | loss=0.3696 acc=0.8572 | Elapsed: 0.24s\n",
      "epoch 12 val   | loss=0.4672 acc=0.7827 | Elapsed: 0.02s\n",
      "epoch 13 train | loss=0.3371 acc=0.8719 | Elapsed: 0.24s\n",
      "epoch 13 val   | loss=0.4778 acc=0.7872 | Elapsed: 0.02s\n",
      "epoch 14 train | loss=0.3140 acc=0.8833 | Elapsed: 0.24s\n",
      "epoch 14 val   | loss=0.4964 acc=0.7768 | Elapsed: 0.02s\n",
      "epoch 15 train | loss=0.2945 acc=0.8945 | Elapsed: 0.23s\n",
      "epoch 15 val   | loss=0.5048 acc=0.7814 | Elapsed: 0.02s\n",
      "epoch 16 train | loss=0.2833 acc=0.8900 | Elapsed: 0.25s\n",
      "epoch 16 val   | loss=0.5149 acc=0.7775 | Elapsed: 0.02s\n",
      "epoch 17 train | loss=0.2539 acc=0.9073 | Elapsed: 0.25s\n",
      "epoch 17 val   | loss=0.5379 acc=0.7762 | Elapsed: 0.02s\n",
      "epoch 18 train | loss=0.2425 acc=0.9091 | Elapsed: 0.24s\n",
      "epoch 18 val   | loss=0.5283 acc=0.7664 | Elapsed: 0.02s\n",
      "epoch 19 train | loss=0.2211 acc=0.9189 | Elapsed: 0.25s\n",
      "epoch 19 val   | loss=0.5531 acc=0.7814 | Elapsed: 0.02s\n",
      "epoch 20 train | loss=0.2244 acc=0.9220 | Elapsed: 0.24s\n",
      "epoch 20 val   | loss=0.5614 acc=0.7476 | Elapsed: 0.02s\n",
      "run end | best_val_acc=0.7944 best_val_loss=0.4570 best_epoch=9 elapsed=5.20s\n",
      "\n",
      "RNN hyperparameter exploration completed.\n",
      "grid completed. | Elapsed: 337.06s\n",
      "\n",
      "top runs by validation accuracy:\n",
      "{'seed': 23, 'batch_size': 128, 'lr': 0.0005, 'epochs': 20, 'best_val_acc': 0.8177991708119711, 'best_val_loss': 0.43410421907901764, 'best_epoch': 3}\n",
      "{'seed': 23, 'batch_size': 128, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8164970874786377, 'best_val_loss': 0.43356073399384815, 'best_epoch': 7}\n",
      "{'seed': 42, 'batch_size': 128, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8139534890651703, 'best_val_loss': 0.4349517474571864, 'best_epoch': 6}\n",
      "{'seed': 42, 'batch_size': 32, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8110795454545454, 'best_val_loss': 0.4403504566712813, 'best_epoch': 3}\n",
      "{'seed': 42, 'batch_size': 128, 'lr': 0.0005, 'epochs': 20, 'best_val_acc': 0.8100472390651703, 'best_val_loss': 0.44264664749304455, 'best_epoch': 2}\n",
      "{'seed': 23, 'batch_size': 64, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8083047054030679, 'best_val_loss': 0.44658343358473346, 'best_epoch': 5}\n",
      "{'seed': 42, 'batch_size': 32, 'lr': 0.001, 'epochs': 20, 'best_val_acc': 0.8082386363636364, 'best_val_loss': 0.49194106916812336, 'best_epoch': 2}\n",
      "{'seed': 42, 'batch_size': 256, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8074134985605875, 'best_val_loss': 0.4482179780801137, 'best_epoch': 10}\n",
      "{'seed': 42, 'batch_size': 64, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8061905394900929, 'best_val_loss': 0.4438875371759588, 'best_epoch': 4}\n",
      "{'seed': 23, 'batch_size': 32, 'lr': 0.0001, 'epochs': 20, 'best_val_acc': 0.8053977272727273, 'best_val_loss': 0.4545447304844856, 'best_epoch': 5}\n"
     ]
    }
   ],
   "source": [
    "# 4.2 RNN – hyperparameter exploration (AUTO-CONTAINED CELL)\n",
    "# Fixed epochs=20; for each (seed, batch_size, lr) it tracks and prints \"new best\" when val_acc improves.\n",
    "\n",
    "import io, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------\n",
    "# SETUP (expects train_df exists with columns: TEXT_COL, LABEL_COL)\n",
    "# -------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"starting setup...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"target\"\n",
    "\n",
    "MAX_VOCAB = 60000\n",
    "MAX_LEN = 64\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "GLOVE_TXT = r\"data/glove/model.txt\"\n",
    "EMB_DIM = 300\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "# GRID (requested)\n",
    "TUNE_BATCH_SIZES = [32, 64, 128, 256]\n",
    "TUNE_LRS = [1e-3, 5e-4, 1e-4]\n",
    "TUNE_EPOCHS = 20\n",
    "TUNE_SEEDS = [42, 23, 999]\n",
    "\n",
    "print(f\"...done. device={device} | Elapsed: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "def basic_tokenize(s):\n",
    "    return str(s).lower().strip().split()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def tprint(msg, t0):\n",
    "    print(f\"{msg} | Elapsed: {time.time() - t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# SPLIT (fixed for all runs)\n",
    "# -------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting dataset split...\")\n",
    "\n",
    "SEED_BASE_SPLIT = 42\n",
    "df = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "\n",
    "idx = np.arange(len(df))\n",
    "np.random.seed(SEED_BASE_SPLIT)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "val_n = int(len(df) * VAL_FRAC)\n",
    "val_idx = idx[:val_n]\n",
    "trn_idx = idx[val_n:]\n",
    "\n",
    "trn_df = df.iloc[trn_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"...done. train={len(trn_df)} val={len(val_df)}\")\n",
    "tprint(\"split completed.\", t0)\n",
    "\n",
    "# -------------------------\n",
    "# VOCAB\n",
    "# -------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting vocabulary build...\")\n",
    "\n",
    "counter = Counter()\n",
    "for t in trn_df[TEXT_COL]:\n",
    "    counter.update(basic_tokenize(t))\n",
    "\n",
    "itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "for w, _ in counter.most_common():\n",
    "    if len(itos) >= MAX_VOCAB:\n",
    "        break\n",
    "    itos.append(w)\n",
    "\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "print(f\"...done. vocab_size={vocab_size}\")\n",
    "tprint(\"vocab completed.\", t0)\n",
    "\n",
    "# -------------------------\n",
    "# GLOVE LOAD (FILTERED) + EMB MATRIX\n",
    "# -------------------------\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting GloVe load (filtered)...\")\n",
    "\n",
    "glove_vecs = {}\n",
    "with io.open(GLOVE_TXT, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\") as f:\n",
    "    _ = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) != EMB_DIM + 1:\n",
    "            continue\n",
    "        w = parts[0]\n",
    "        if w in stoi:\n",
    "            glove_vecs[w] = np.asarray(parts[1:], dtype=np.float32)\n",
    "\n",
    "print(f\"...done. glove_hits={len(glove_vecs)}\")\n",
    "tprint(\"glove completed.\", t0)\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"starting embedding matrix build...\")\n",
    "\n",
    "emb_matrix = np.random.normal(0, 0.05, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "emb_matrix[stoi[PAD_TOKEN]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "\n",
    "hit = 0\n",
    "for w, i in stoi.items():\n",
    "    v = glove_vecs.get(w)\n",
    "    if v is not None:\n",
    "        emb_matrix[i] = v\n",
    "        hit += 1\n",
    "\n",
    "print(f\"...done. coverage={hit / vocab_size:.4f}\")\n",
    "tprint(\"embedding matrix completed.\", t0)\n",
    "\n",
    "# -------------------------\n",
    "# DATASET (x, y, length) + COLLATE\n",
    "# -------------------------\n",
    "\n",
    "class TextDatasetXYL(Dataset):\n",
    "    def __init__(self, df_):\n",
    "        self.df = df_.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = basic_tokenize(self.df.loc[idx, TEXT_COL])[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        length = len(ids)\n",
    "        if length < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        l = torch.tensor(length, dtype=torch.long)\n",
    "        return x, y, l\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys, ls = zip(*batch)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.stack(ys, dim=0)\n",
    "    l = torch.stack(ls, dim=0)\n",
    "    return x, y, l\n",
    "\n",
    "# -------------------------\n",
    "# MODEL (Vanilla RNN uses lengths to pick last valid timestep)\n",
    "# -------------------------\n",
    "\n",
    "class VanillaRNNClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, freeze_emb=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.embedding.weight.requires_grad = (not freeze_emb)\n",
    "        self.rnn = nn.RNN(EMB_DIM, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.rnn(emb)\n",
    "        lengths = lengths.to(out.device)\n",
    "        idx = (lengths - 1).clamp(min=0)\n",
    "        b = torch.arange(out.size(0), device=out.device)\n",
    "        last = out[b, idx, :]\n",
    "        return self.fc(last).squeeze(1)\n",
    "\n",
    "# -------------------------\n",
    "# METRICS\n",
    "# -------------------------\n",
    "\n",
    "def accuracy_from_logits(logits, y, threshold=0.5):\n",
    "    preds = (torch.sigmoid(logits) >= threshold).float()\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN / EVAL\n",
    "# -------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, threshold=0.5):\n",
    "    model.train()\n",
    "    tot_loss = 0.0\n",
    "    tot_acc = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y, l in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        l = l.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x, l)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += float(loss.detach().cpu())\n",
    "        tot_acc += float(accuracy_from_logits(logits.detach(), y, threshold=threshold).detach().cpu())\n",
    "        n += 1\n",
    "\n",
    "    return tot_loss / max(n, 1), tot_acc / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for x, y, l in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        l = l.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x, l)\n",
    "        losses.append(float(criterion(logits, y).detach().cpu()))\n",
    "        accs.append(float(accuracy_from_logits(logits, y, threshold=threshold).detach().cpu()))\n",
    "\n",
    "    return float(np.mean(losses)), float(np.mean(accs))\n",
    "\n",
    "# -------------------------\n",
    "# SINGLE RUN (tracks best each epoch and prints \"new best\")\n",
    "# -------------------------\n",
    "\n",
    "def run_rnn_once(seed, batch_size, lr, epochs=20, threshold=0.5, hidden_dim=128, freeze_emb=False):\n",
    "    seed_all(seed)\n",
    "\n",
    "    train_loader_local = DataLoader(\n",
    "        TextDatasetXYL(trn_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_batch,\n",
    "        persistent_workers=(NUM_WORKERS > 0)\n",
    "    )\n",
    "\n",
    "    val_loader_local = DataLoader(\n",
    "        TextDatasetXYL(val_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_batch,\n",
    "        persistent_workers=(NUM_WORKERS > 0)\n",
    "    )\n",
    "\n",
    "    model = VanillaRNNClassifier(hidden_dim=hidden_dim, freeze_emb=freeze_emb).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_val_loss = 1e9\n",
    "    best_epoch = -1\n",
    "\n",
    "    print(f\"run start | seed={seed} batch={batch_size} lr={lr} epochs={epochs}\")\n",
    "    run_t0 = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_t0 = time.time()\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader_local, optimizer, criterion, threshold=threshold)\n",
    "        #print(f\"epoch {epoch} train | loss={tr_loss:.4f} acc={tr_acc:.4f} | Elapsed: {time.time() - tr_t0:.2f}s\")\n",
    "\n",
    "        va_t0 = time.time()\n",
    "        va_loss, va_acc = evaluate(model, val_loader_local, criterion, threshold=threshold)\n",
    "        print_text = f\"epoch {epoch} val   | loss={va_loss:.4f} acc={va_acc:.4f} | Elapsed: {time.time() - va_t0:.2f}s\"\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            best_val_loss = va_loss\n",
    "            best_epoch = epoch\n",
    "            print_text+=(f\" <- NEW BEST: val_acc={best_val_acc:.4f} at epoch {best_epoch} (val_loss={best_val_loss:.4f})\")\n",
    "        print (print_text)\n",
    "\n",
    "    print(f\"run end | best_val_acc={best_val_acc:.4f} best_val_loss={best_val_loss:.4f} best_epoch={best_epoch} elapsed={time.time() - run_t0:.2f}s\")\n",
    "    return best_val_acc, best_val_loss, best_epoch\n",
    "\n",
    "# -------------------------\n",
    "# GRID EXECUTION (epochs fixed to 20)\n",
    "# -------------------------\n",
    "\n",
    "print(\"starting RNN hyperparameter exploration...\")\n",
    "\n",
    "tune_results = []\n",
    "total_runs = len(TUNE_SEEDS) * len(TUNE_BATCH_SIZES) * len(TUNE_LRS)\n",
    "run_i = 0\n",
    "grid_t0 = time.time()\n",
    "\n",
    "for seed in TUNE_SEEDS:\n",
    "    for batch_size in TUNE_BATCH_SIZES:\n",
    "        for lr in TUNE_LRS:\n",
    "            run_i += 1\n",
    "            print(f\"\\nGRID RUN {run_i}/{total_runs}\")\n",
    "            best_acc, best_loss, best_epoch = run_rnn_once(\n",
    "                seed=seed,\n",
    "                batch_size=batch_size,\n",
    "                lr=lr,\n",
    "                epochs=TUNE_EPOCHS,\n",
    "                threshold=0.5\n",
    "            )\n",
    "            tune_results.append({\n",
    "                \"seed\": int(seed),\n",
    "                \"batch_size\": int(batch_size),\n",
    "                \"lr\": float(lr),\n",
    "                \"epochs\": int(TUNE_EPOCHS),\n",
    "                \"best_val_acc\": float(best_acc),\n",
    "                \"best_val_loss\": float(best_loss),\n",
    "                \"best_epoch\": int(best_epoch),\n",
    "            })\n",
    "\n",
    "print(\"\\nRNN hyperparameter exploration completed.\")\n",
    "tprint(\"grid completed.\", grid_t0)\n",
    "\n",
    "print(\"\\ntop runs by validation accuracy:\")\n",
    "for r in sorted(tune_results, key=lambda x: x[\"best_val_acc\"], reverse=True)[:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2ba77",
   "metadata": {},
   "source": [
    "#### 4.2.2 RNN Best Configurations and Results\n",
    "\n",
    "**Best Vanilla RNN results on validation split**\n",
    "\n",
    "| Seed | Batch Size | Learning Rate | Epochs | Best Epoch | Best Val Accuracy | Best Val Loss |\n",
    "|------|------------|---------------|--------|------------|------------------|---------------|\n",
    "| **23** | **128** | **5e-4** | **20** | **3** | **0.8178** | **0.4341** |\n",
    "| 23   | 128        | 1e-4          | 20     | 7          | 0.8165           | 0.4336        |\n",
    "| 42   | 128        | 1e-4          | 20     | 6          | 0.8140           | 0.4350        |\n",
    "| 42   | 32         | 1e-4          | 20     | 3          | 0.8111           | 0.4404        |\n",
    "| 42   | 128        | 5e-4          | 20     | 2          | 0.8100           | 0.4426        |\n",
    "\n",
    "A hyperparameter exploration was conducted for the Vanilla RNN by varying batch size, learning rate, number of epochs, and random seed, while keeping the architecture fixed. With the corrected implementation, the model consistently achieves validation accuracies around **0.80–0.82**, indicating stable and reproducible learning behavior across different configurations.\n",
    "\n",
    "Batch size **128** and low learning rates (**1e-4–5e-4**) systematically yield the strongest results, while performance differences between the top configurations remain marginal. In most cases, the best validation accuracy is reached within the first few epochs, confirming rapid convergence and limited benefits from extended training.\n",
    "\n",
    "Given the narrow performance spread among the best runs, **Occam’s razor** was applied in selecting the final configuration: the model achieving competitive accuracy with the **lowest computational cost** was preferred. This leads to choosing a configuration that converges in fewer epochs and avoids unnecessary training overhead, while preserving essentially the same validation performance.\n",
    "\n",
    "Overall, the Vanilla RNN proves to be a solid baseline when properly implemented and tuned, although its performance quickly saturates, motivating the exploration of more expressive recurrent architectures in the subsequent section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdceaa",
   "metadata": {},
   "source": [
    "### 4.3 GRU Best Configuration and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "206b617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW BEST at epoch 1, val_acc: 0.8181818181818182\n",
      "NEW BEST at epoch 1, val_acc: 0.8210227272727273\n",
      "NEW BEST at epoch 1, val_acc: 0.7401859516447241\n",
      "NEW BEST at epoch 2, val_acc: 0.8039772727272727\n",
      "NEW BEST at epoch 3, val_acc: 0.8053977272727273\n",
      "NEW BEST at epoch 4, val_acc: 0.8096590909090909\n",
      "NEW BEST at epoch 1, val_acc: 0.8097251599485223\n",
      "NEW BEST at epoch 1, val_acc: 0.8033496303991838\n",
      "NEW BEST at epoch 2, val_acc: 0.8111456144939769\n",
      "NEW BEST at epoch 1, val_acc: 0.6671181266958063\n",
      "NEW BEST at epoch 2, val_acc: 0.7586548599329862\n",
      "NEW BEST at epoch 3, val_acc: 0.8040763735771179\n",
      "NEW BEST at epoch 4, val_acc: 0.806917282668027\n",
      "NEW BEST at epoch 5, val_acc: 0.809031448581002\n",
      "NEW BEST at epoch 1, val_acc: 0.7944222390651703\n",
      "NEW BEST at epoch 2, val_acc: 0.8217357099056244\n",
      "NEW BEST at epoch 1, val_acc: 0.7788577973842621\n",
      "NEW BEST at epoch 2, val_acc: 0.8152555723985037\n",
      "NEW BEST at epoch 3, val_acc: 0.8177991708119711\n",
      "NEW BEST at epoch 1, val_acc: 0.6201853156089783\n",
      "NEW BEST at epoch 2, val_acc: 0.6839571197827657\n",
      "NEW BEST at epoch 3, val_acc: 0.761991282304128\n",
      "NEW BEST at epoch 4, val_acc: 0.7801598807175955\n",
      "NEW BEST at epoch 5, val_acc: 0.8022650182247162\n",
      "NEW BEST at epoch 6, val_acc: 0.8152555723985037\n",
      "NEW BEST at epoch 8, val_acc: 0.8269137541453043\n",
      "NEW BEST at epoch 1, val_acc: 0.7716404795646667\n",
      "NEW BEST at epoch 2, val_acc: 0.7917884985605875\n",
      "NEW BEST at epoch 3, val_acc: 0.8139163057009379\n",
      "NEW BEST at epoch 1, val_acc: 0.6987923383712769\n",
      "NEW BEST at epoch 2, val_acc: 0.7677266200383505\n",
      "NEW BEST at epoch 3, val_acc: 0.8048017223676046\n",
      "NEW BEST at epoch 1, val_acc: 0.5588983297348022\n",
      "NEW BEST at epoch 2, val_acc: 0.6070449550946554\n",
      "NEW BEST at epoch 3, val_acc: 0.6630040804545084\n",
      "NEW BEST at epoch 4, val_acc: 0.7033458153406779\n",
      "NEW BEST at epoch 5, val_acc: 0.7488959034283956\n",
      "NEW BEST at epoch 6, val_acc: 0.7638279795646667\n",
      "NEW BEST at epoch 7, val_acc: 0.7755314906438192\n",
      "NEW BEST at epoch 8, val_acc: 0.7917961080869039\n",
      "NEW BEST at epoch 10, val_acc: 0.8002558549245199\n",
      "NEW BEST at epoch 11, val_acc: 0.8093628088633219\n",
      "NEW BEST at epoch 1, val_acc: 0.8096590909090909\n",
      "NEW BEST at epoch 1, val_acc: 0.8068181818181818\n",
      "NEW BEST at epoch 1, val_acc: 0.7501291334629059\n",
      "NEW BEST at epoch 2, val_acc: 0.8053977272727273\n",
      "NEW BEST at epoch 3, val_acc: 0.8139204545454546\n",
      "NEW BEST at epoch 1, val_acc: 0.8040433417667042\n",
      "NEW BEST at epoch 2, val_acc: 0.8054637963121588\n",
      "NEW BEST at epoch 1, val_acc: 0.8054637963121588\n",
      "NEW BEST at epoch 1, val_acc: 0.6571419130672108\n",
      "NEW BEST at epoch 2, val_acc: 0.777781448581002\n",
      "NEW BEST at epoch 3, val_acc: 0.7941001599485223\n",
      "NEW BEST at epoch 4, val_acc: 0.8075779568065297\n",
      "NEW BEST at epoch 6, val_acc: 0.8089984113519842\n",
      "NEW BEST at epoch 7, val_acc: 0.8096921227195046\n",
      "NEW BEST at epoch 1, val_acc: 0.8035065432389578\n",
      "NEW BEST at epoch 2, val_acc: 0.8099866708119711\n",
      "NEW BEST at epoch 1, val_acc: 0.7931201557318369\n",
      "NEW BEST at epoch 2, val_acc: 0.8139534890651703\n",
      "NEW BEST at epoch 3, val_acc: 0.8165576557318369\n",
      "NEW BEST at epoch 1, val_acc: 0.6163396338621775\n",
      "NEW BEST at epoch 2, val_acc: 0.6826247572898865\n",
      "NEW BEST at epoch 3, val_acc: 0.7735889057318369\n",
      "NEW BEST at epoch 4, val_acc: 0.7996002932389578\n",
      "NEW BEST at epoch 5, val_acc: 0.8125908374786377\n",
      "NEW BEST at epoch 6, val_acc: 0.8164970874786377\n",
      "NEW BEST at epoch 7, val_acc: 0.8177991708119711\n",
      "NEW BEST at epoch 8, val_acc: 0.8191012541453043\n",
      "NEW BEST at epoch 10, val_acc: 0.8229772249857584\n",
      "NEW BEST at epoch 1, val_acc: 0.7800849874814352\n",
      "NEW BEST at epoch 2, val_acc: 0.7930981914202372\n",
      "NEW BEST at epoch 3, val_acc: 0.7989461421966553\n",
      "NEW BEST at epoch 1, val_acc: 0.6968430280685425\n",
      "NEW BEST at epoch 2, val_acc: 0.7944079041481018\n",
      "NEW BEST at epoch 3, val_acc: 0.803492029507955\n",
      "NEW BEST at epoch 4, val_acc: 0.8158656160036722\n",
      "NEW BEST at epoch 1, val_acc: 0.604448397954305\n",
      "NEW BEST at epoch 2, val_acc: 0.609649121761322\n",
      "NEW BEST at epoch 3, val_acc: 0.6467394630114237\n",
      "NEW BEST at epoch 4, val_acc: 0.6753776868184408\n",
      "NEW BEST at epoch 5, val_acc: 0.7365222970644633\n",
      "NEW BEST at epoch 6, val_acc: 0.7748766541481018\n",
      "NEW BEST at epoch 7, val_acc: 0.7846384644508362\n",
      "NEW BEST at epoch 8, val_acc: 0.7911412715911865\n",
      "NEW BEST at epoch 9, val_acc: 0.8054565588633219\n",
      "NEW BEST at epoch 10, val_acc: 0.8061114152272543\n",
      "NEW BEST at epoch 11, val_acc: 0.8165204723676046\n",
      "NEW BEST at epoch 1, val_acc: 0.8082386363636364\n",
      "NEW BEST at epoch 2, val_acc: 0.8181818181818182\n",
      "NEW BEST at epoch 1, val_acc: 0.8011363636363636\n",
      "NEW BEST at epoch 2, val_acc: 0.8167613636363636\n",
      "NEW BEST at epoch 1, val_acc: 0.7474173578349027\n",
      "NEW BEST at epoch 2, val_acc: 0.8011363636363636\n",
      "NEW BEST at epoch 3, val_acc: 0.8082386363636364\n",
      "NEW BEST at epoch 4, val_acc: 0.8153409090909091\n",
      "NEW BEST at epoch 1, val_acc: 0.8012024326757952\n",
      "NEW BEST at epoch 2, val_acc: 0.8111456144939769\n",
      "NEW BEST at epoch 1, val_acc: 0.7990882667628202\n",
      "NEW BEST at epoch 2, val_acc: 0.8061905394900929\n",
      "NEW BEST at epoch 1, val_acc: 0.6579016934741627\n",
      "NEW BEST at epoch 2, val_acc: 0.7629162235693498\n",
      "NEW BEST at epoch 3, val_acc: 0.7941001599485223\n",
      "NEW BEST at epoch 4, val_acc: 0.8132928122173656\n",
      "NEW BEST at epoch 1, val_acc: 0.808714876572291\n",
      "NEW BEST at epoch 2, val_acc: 0.8139534890651703\n",
      "NEW BEST at epoch 1, val_acc: 0.7814619640509287\n",
      "NEW BEST at epoch 2, val_acc: 0.8100472390651703\n",
      "NEW BEST at epoch 1, val_acc: 0.6306625505288442\n",
      "NEW BEST at epoch 2, val_acc: 0.6761748989423116\n",
      "NEW BEST at epoch 3, val_acc: 0.7632933656374613\n",
      "NEW BEST at epoch 4, val_acc: 0.7853682140509287\n",
      "NEW BEST at epoch 5, val_acc: 0.8035368223985037\n",
      "NEW BEST at epoch 6, val_acc: 0.8126514057318369\n",
      "NEW BEST at epoch 7, val_acc: 0.8165576557318369\n",
      "NEW BEST at epoch 1, val_acc: 0.7638203700383505\n",
      "NEW BEST at epoch 2, val_acc: 0.8100176652272543\n",
      "NEW BEST at epoch 1, val_acc: 0.6949013074239095\n",
      "NEW BEST at epoch 2, val_acc: 0.7813946803410848\n",
      "NEW BEST at epoch 3, val_acc: 0.806751032670339\n",
      "NEW BEST at epoch 4, val_acc: 0.8100100557009379\n",
      "NEW BEST at epoch 1, val_acc: 0.579069197177887\n",
      "NEW BEST at epoch 2, val_acc: 0.633711020151774\n",
      "NEW BEST at epoch 3, val_acc: 0.6545443534851074\n",
      "NEW BEST at epoch 4, val_acc: 0.6955485542615255\n",
      "NEW BEST at epoch 5, val_acc: 0.7384868462880453\n",
      "NEW BEST at epoch 6, val_acc: 0.7514619827270508\n",
      "NEW BEST at epoch 7, val_acc: 0.7820342977841696\n",
      "NEW BEST at epoch 8, val_acc: 0.7872502406438192\n",
      "NEW BEST at epoch 9, val_acc: 0.7918037374814352\n",
      "NEW BEST at epoch 10, val_acc: 0.7996162374814352\n",
      "NEW BEST at epoch 11, val_acc: 0.8041697144508362\n",
      "GRU grid completed.\n",
      "{'seed': 42, 'batch_size': 128, 'lr': 0.0001, 'best_val_acc': 0.8269137541453043, 'best_epoch': 8}\n",
      "{'seed': 23, 'batch_size': 128, 'lr': 0.0001, 'best_val_acc': 0.8229772249857584, 'best_epoch': 10}\n",
      "{'seed': 42, 'batch_size': 128, 'lr': 0.001, 'best_val_acc': 0.8217357099056244, 'best_epoch': 2}\n",
      "{'seed': 42, 'batch_size': 32, 'lr': 0.0005, 'best_val_acc': 0.8210227272727273, 'best_epoch': 1}\n",
      "{'seed': 42, 'batch_size': 32, 'lr': 0.001, 'best_val_acc': 0.8181818181818182, 'best_epoch': 1}\n",
      "{'seed': 999, 'batch_size': 32, 'lr': 0.001, 'best_val_acc': 0.8181818181818182, 'best_epoch': 2}\n",
      "{'seed': 42, 'batch_size': 128, 'lr': 0.0005, 'best_val_acc': 0.8177991708119711, 'best_epoch': 3}\n",
      "{'seed': 999, 'batch_size': 32, 'lr': 0.0005, 'best_val_acc': 0.8167613636363636, 'best_epoch': 2}\n",
      "{'seed': 23, 'batch_size': 128, 'lr': 0.0005, 'best_val_acc': 0.8165576557318369, 'best_epoch': 3}\n",
      "{'seed': 999, 'batch_size': 128, 'lr': 0.0001, 'best_val_acc': 0.8165576557318369, 'best_epoch': 7}\n"
     ]
    }
   ],
   "source": [
    "# 4.3 GRU – hyperparameter exploration (AUTO-CONTAINED CELL)\n",
    "# - Early stopping: active after epoch 10, patience=2 on val_acc\n",
    "# - Per-epoch train/val prints commented out\n",
    "\n",
    "import io, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# -------------------------\n",
    "# SETUP\n",
    "# -------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"target\"\n",
    "\n",
    "GLOVE_TXT = r\"data/glove/model.txt\"\n",
    "EMB_DIM = 300\n",
    "\n",
    "MAX_VOCAB = 60000\n",
    "MAX_LEN = 64\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "TUNE_BATCH_SIZES = [32, 64, 128, 256]\n",
    "TUNE_LRS = [1e-3, 5e-4, 1e-4]\n",
    "TUNE_EPOCHS = 20\n",
    "TUNE_SEEDS = [42, 23, 999]\n",
    "\n",
    "GLOBAL_THRESHOLD = 0.5\n",
    "HIDDEN_DIM = 128\n",
    "FREEZE_EMB = False\n",
    "\n",
    "EARLY_STOP_START = 10\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "\n",
    "def basic_tokenize(s):\n",
    "    return str(s).lower().strip().split()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# -------------------------\n",
    "# DATA PREPARATION (split, vocab, embeddings)\n",
    "# -------------------------\n",
    "\n",
    "df = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "\n",
    "idx = np.arange(len(df))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "val_n = int(len(df) * VAL_FRAC)\n",
    "val_idx = idx[:val_n]\n",
    "trn_idx = idx[val_n:]\n",
    "\n",
    "trn_df = df.iloc[trn_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "counter = Counter()\n",
    "for t in trn_df[TEXT_COL]:\n",
    "    counter.update(basic_tokenize(t))\n",
    "\n",
    "itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "for w, _ in counter.most_common():\n",
    "    if len(itos) >= MAX_VOCAB:\n",
    "        break\n",
    "    itos.append(w)\n",
    "\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "glove_vecs = {}\n",
    "with io.open(GLOVE_TXT, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\") as f:\n",
    "    _ = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) != EMB_DIM + 1:\n",
    "            continue\n",
    "        w = parts[0]\n",
    "        if w in stoi:\n",
    "            glove_vecs[w] = np.asarray(parts[1:], dtype=np.float32)\n",
    "\n",
    "emb_matrix = np.random.normal(0, 0.05, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "emb_matrix[stoi[PAD_TOKEN]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "\n",
    "for w, i in stoi.items():\n",
    "    if w in glove_vecs:\n",
    "        emb_matrix[i] = glove_vecs[w]\n",
    "\n",
    "# -------------------------\n",
    "# DATASET\n",
    "# -------------------------\n",
    "\n",
    "class TextDatasetXYL(Dataset):\n",
    "    def __init__(self, df_):\n",
    "        self.df = df_.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = basic_tokenize(self.df.loc[idx, TEXT_COL])[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        length = len(ids)\n",
    "        if length < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        l = torch.tensor(length, dtype=torch.long)\n",
    "        return x, y, l\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys, ls = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys), torch.stack(ls)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL\n",
    "# -------------------------\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.gru = nn.GRU(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h_n = self.gru(packed)\n",
    "        return self.fc(h_n[-1]).squeeze(1)\n",
    "\n",
    "# -------------------------\n",
    "# METRICS\n",
    "# -------------------------\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    preds = (torch.sigmoid(logits) >= GLOBAL_THRESHOLD).float()\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN / EVAL\n",
    "# -------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for x, y, l in loader:\n",
    "        x, y, l = x.to(device), y.to(device), l.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss = criterion(model(x, l), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for x, y, l in loader:\n",
    "        x, y, l = x.to(device), y.to(device), l.to(device)\n",
    "        logits = model(x, l)\n",
    "        accs.append(float(accuracy_from_logits(logits, y)))\n",
    "    return float(np.mean(accs))\n",
    "\n",
    "# -------------------------\n",
    "# SINGLE RUN WITH EARLY STOPPING\n",
    "# -------------------------\n",
    "\n",
    "def run_gru_once(seed, batch_size, lr):\n",
    "    seed_all(seed)\n",
    "\n",
    "    train_loader = DataLoader(TextDatasetXYL(trn_df), batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=collate_batch, num_workers=0)\n",
    "    val_loader = DataLoader(TextDatasetXYL(val_df), batch_size=batch_size, shuffle=False,\n",
    "                            collate_fn=collate_batch, num_workers=0)\n",
    "\n",
    "    model = GRUClassifier().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_acc = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, TUNE_EPOCHS + 1):\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "            print (f\"NEW BEST at epoch {epoch}, val_acc: {val_acc}\")\n",
    "        else:\n",
    "            if epoch >= EARLY_STOP_START:\n",
    "                no_improve += 1\n",
    "                if no_improve >= EARLY_STOP_PATIENCE:\n",
    "                    break\n",
    "\n",
    "    return best_acc, best_epoch\n",
    "\n",
    "# -------------------------\n",
    "# GRID EXECUTION\n",
    "# -------------------------\n",
    "\n",
    "tune_results = []\n",
    "\n",
    "for seed in TUNE_SEEDS:\n",
    "    for bs in TUNE_BATCH_SIZES:\n",
    "        for lr in TUNE_LRS:\n",
    "            best_acc, best_epoch = run_gru_once(seed, bs, lr)\n",
    "            tune_results.append({\n",
    "                \"seed\": seed,\n",
    "                \"batch_size\": bs,\n",
    "                \"lr\": lr,\n",
    "                \"best_val_acc\": best_acc,\n",
    "                \"best_epoch\": best_epoch\n",
    "            })\n",
    "\n",
    "print(\"GRU grid completed.\")\n",
    "for r in sorted(tune_results, key=lambda x: x[\"best_val_acc\"], reverse=True)[:10]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d44cc",
   "metadata": {},
   "source": [
    "#### 4.3.2 GRU Best Configurations and Results\n",
    "\n",
    "**Best GRU results on validation split**\n",
    "\n",
    "| Seed | Batch Size | Learning Rate | Epochs | Best Epoch | Best Val Accuracy |\n",
    "|------|------------|---------------|--------|------------|------------------|\n",
    "| **42** | **128** | **1e-4** | **20** | **8** | **0.8269** |\n",
    "| 23 | 128 | 1e-4 | 20 | 10 | 0.8230 |\n",
    "| 42 | 128 | 1e-3 | 20 | 2 | 0.8217 |\n",
    "| 42 | 32 | 5e-4 | 20 | 1 | 0.8210 |\n",
    "| 42 | 32 | 1e-3 | 20 | 1 | 0.8182 |\n",
    "\n",
    "A hyperparameter exploration was conducted for the GRU architecture using the same experimental protocol adopted for the Vanilla RNN. Batch size, learning rate, number of epochs, and random seed were varied while keeping the model structure and input representation fixed. Early stopping was applied after epoch 10 with a short patience to avoid redundant computation, based on the empirical observation of rapid convergence in preliminary experiments.\n",
    "\n",
    "The GRU achieves a best validation accuracy of approximately **0.83**, representing a modest but consistent improvement over the Vanilla RNN baseline. As observed for the RNN, a batch size of **128** and a low learning rate (**1e-4**) yield the most stable and performant configurations. Best results are typically reached within the first few epochs, indicating fast convergence and limited benefit from extended training.\n",
    "\n",
    "Despite its greater expressive power, the GRU does not provide a dramatic performance gain in this setting. The limited improvement suggests that the dominant performance constraint is the input representation and the intrinsic ambiguity of the dataset, rather than the capacity of the recurrent unit itself.\n",
    "\n",
    "Overall, these results explicitly show that, when properly tuned, even more basic architectures such as Vanilla RNNs can achieve performance comparable to more sophisticated recurrent models, reinforcing the importance of hyperparameter optimization over architectural complexity alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a23fe",
   "metadata": {},
   "source": [
    "### 4.4 Model Comparison and Error Analysis\n",
    "\n",
    "To complement the quantitative evaluation, a qualitative analysis of the misclassified samples, shown below, was performed on the validation set using the best-performing GRU configuration. The first twenty misclassified instances were manually inspected to better understand the nature of the remaining errors.\n",
    "\n",
    "This analysis reveals that a substantial portion of the errors corresponds to texts that are inherently ambiguous or borderline with respect to the definition of *disaster*. In several cases, the distinction between disaster and non-disaster is not explicitly encoded in the text and would require additional contextual or semantic information beyond what is available to the model. Other misclassifications appear to be attributable to potentially inconsistent or debatable labels, rather than to clear modeling failures.\n",
    "\n",
    "Importantly, no systematic error patterns were observed. The misclassified samples do not indicate a collapse toward a single class, nor do they reveal evident biases or pathological behaviors of the model. Instead, errors are distributed across a variety of edge cases, including technical descriptions, metaphorical language, and news headlines whose categorization may be disputable even for a human annotator.\n",
    "\n",
    "These observations suggest that the achieved validation accuracy of approximately **0.80–0.83** represents a meaningful result given the intrinsic ambiguity of the dataset. Under the adopted representation and without introducing explicit semantic supervision or domain-specific heuristics, further improvements are likely bounded by the quality and consistency of the labels rather than by model capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "edeeb809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 misclassified validation samples:\n",
      "\n",
      "[1]\n",
      "TEXT : england east coast. dogger bank westward. 1. seismic survey in progress by m v western regent towing a 8400 metre long cable within areau\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[2]\n",
      "TEXT : thanks i narrowly averted death that was fun you're right\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[3]\n",
      "TEXT : i liked a video j. cole fire squad (2014 forest hills drive)\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[4]\n",
      "TEXT : toddler drowned in bath after mum left room to fetch his pyjamas\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[5]\n",
      "TEXT : 10news ? water main break disrupts trolley service\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[6]\n",
      "TEXT : definite triple crown threat. him and harper both.\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[7]\n",
      "TEXT : (sj gist): 148 houses farm produce destroy... |\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[8]\n",
      "TEXT : we happily support mydrought a project bringing awareness to the la drought. track your wateru\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[9]\n",
      "TEXT : the weatherit needs to make it minds up. first snow tornadoes now would you say a heat wave?\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[10]\n",
      "TEXT : ...i'm no expert but raw uranium and nuclear reactor fuel rods are 2 very different creatures...\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[11]\n",
      "TEXT : my ipod crashed..... one direction\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[12]\n",
      "TEXT : omg omg omg and have collided in a nuclear accident at ^oo^\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[13]\n",
      "TEXT : wftv eyewitness news: tn school psychologist arrested in florida on child porn charges\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[14]\n",
      "TEXT : it was past curfew and we were at the grove\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[15]\n",
      "TEXT : snowstorm planned outside 's st mary major tonight annual occasion artificial snow remembering summer snow in 358 ad on same spot.\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[16]\n",
      "TEXT : social casualty\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[17]\n",
      "TEXT : currency transgress before payday prison ward sinking fund payment unsecured loan: jbumzqpk\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[18]\n",
      "TEXT : eyewitness identification via reddit\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n",
      "[19]\n",
      "TEXT : btw the 30th is actually next year casualty began 6th september 1986 so 2016 marks 30 years\n",
      "TRUE : 1 | PRED : 0\n",
      "--------------------------------------------------------------------------------\n",
      "[20]\n",
      "TEXT : cue the flood of people 'ironically' calling you that\n",
      "TRUE : 0 | PRED : 1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print first 20 misclassified validation samples\n",
    "# Assumes:\n",
    "# - best GRU configuration already identified\n",
    "# - same preprocessing, vocab, and model definition as in the GRU grid\n",
    "# - variables available: trn_df, val_df, stoi, itos, emb_matrix, device\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG: set BEST configuration here\n",
    "# -------------------------\n",
    "\n",
    "BEST_SEED = 42\n",
    "BEST_BATCH_SIZE = 128\n",
    "BEST_LR = 1e-4\n",
    "BEST_EPOCHS = 20\n",
    "BEST_EPOCH = 8            # best_epoch from grid\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# -------------------------\n",
    "# REPRODUCIBILITY\n",
    "# -------------------------\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(BEST_SEED)\n",
    "\n",
    "# -------------------------\n",
    "# DATASET (same as training)\n",
    "# -------------------------\n",
    "\n",
    "class TextDatasetXYL(Dataset):\n",
    "    def __init__(self, df_):\n",
    "        self.df = df_.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, TEXT_COL]\n",
    "        toks = basic_tokenize(text)[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        length = len(ids)\n",
    "        if length < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        l = torch.tensor(length, dtype=torch.long)\n",
    "        return x, y, l, text\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys, ls, texts = zip(*batch)\n",
    "    return torch.stack(xs), torch.stack(ys), torch.stack(ls), texts\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TextDatasetXYL(val_df),\n",
    "    batch_size=BEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# MODEL (same as GRU training)\n",
    "# -------------------------\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(stoi), EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.gru = nn.GRU(EMB_DIM, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, h_n = self.gru(packed)\n",
    "        return self.fc(h_n[-1]).squeeze(1)\n",
    "\n",
    "model = GRUClassifier().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=BEST_LR)\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN model up to BEST_EPOCH\n",
    "# -------------------------\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TextDatasetXYL(trn_df),\n",
    "    batch_size=BEST_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, BEST_EPOCH + 1):\n",
    "    for x, y, l, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        l = l.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss = criterion(model(x, l), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# -------------------------\n",
    "# COLLECT MISCLASSIFIED SAMPLES\n",
    "# -------------------------\n",
    "\n",
    "model.eval()\n",
    "errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, l, texts in val_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        l = l.to(device)\n",
    "\n",
    "        logits = model(x, l)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= THRESHOLD).float()\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            if preds[i] != y[i]:\n",
    "                errors.append({\n",
    "                    \"text\": texts[i],\n",
    "                    \"y_true\": int(y[i].item()),\n",
    "                    \"y_pred\": int(preds[i].item())\n",
    "                })\n",
    "            if len(errors) >= 20:\n",
    "                break\n",
    "        if len(errors) >= 20:\n",
    "            break\n",
    "\n",
    "# -------------------------\n",
    "# PRINT FIRST 20 ERRORS\n",
    "# -------------------------\n",
    "\n",
    "print(f\"First {len(errors)} misclassified validation samples:\\n\")\n",
    "for i, e in enumerate(errors, 1):\n",
    "    print(f\"[{i}]\")\n",
    "    print(f\"TEXT : {e['text']}\")\n",
    "    print(f\"TRUE : {e['y_true']} | PRED : {e['y_pred']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f3278c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle submission helper (drop-in cell)\n",
    "# Usage (after you have a trained model and the same stoi/PAD/UNK/MAX_LEN/basic_tokenize as training):\n",
    "#\n",
    "#   test_df = pd.read_csv(\"data/test.csv\")\n",
    "#   make_kaggle_submission(\n",
    "#       model=model_rnn_or_gru,\n",
    "#       test_df=test_df,\n",
    "#       out_path=\"MG-disaster-submission.csv\",\n",
    "#       use_lengths=False,   # RNN -> False\n",
    "#       # use_lengths=True,  # GRU -> True\n",
    "#       batch_size=256,\n",
    "#       threshold=0.5\n",
    "#   )\n",
    "#\n",
    "# Expected test.csv columns: id, text\n",
    "# Output columns: id, target\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_kaggle_submission(\n",
    "    model,\n",
    "    test_df: pd.DataFrame,\n",
    "    out_path: str = \"MG-disaster-submission.csv\",\n",
    "    use_lengths: bool = False,\n",
    "    batch_size: int = 256,\n",
    "    threshold: float = 0.5,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "):\n",
    "    # --- hard assumptions (by design, per your request) ---\n",
    "    # sample_submission is effectively: id + target\n",
    "    ID_COL = \"id\"\n",
    "    TEXT_COL_LOCAL = \"text\"\n",
    "    TARGET_COL = \"target\"\n",
    "\n",
    "    assert ID_COL in test_df.columns, f\"Missing column '{ID_COL}' in test_df\"\n",
    "    assert TEXT_COL_LOCAL in test_df.columns, f\"Missing column '{TEXT_COL_LOCAL}' in test_df\"\n",
    "\n",
    "    device_local = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    class _TestDataset(Dataset):\n",
    "        def __init__(self, df_):\n",
    "            self.df = df_.reset_index(drop=True)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = self.df.loc[idx, TEXT_COL_LOCAL]\n",
    "            toks = basic_tokenize(text)[:MAX_LEN]\n",
    "            ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "            length = len(ids)\n",
    "            if length < MAX_LEN:\n",
    "                ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "            x = torch.tensor(ids, dtype=torch.long)\n",
    "            if use_lengths:\n",
    "                l = torch.tensor(length, dtype=torch.long)\n",
    "                return x, l\n",
    "            return x\n",
    "\n",
    "    def _collate_no_lengths(batch):\n",
    "        return torch.stack(batch, dim=0)\n",
    "\n",
    "    def _collate_with_lengths(batch):\n",
    "        xs, ls = zip(*batch)\n",
    "        return torch.stack(xs, dim=0), torch.stack(ls, dim=0)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        _TestDataset(test_df),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=(pin_memory and device_local.type == \"cuda\"),\n",
    "        drop_last=False,\n",
    "        collate_fn=_collate_with_lengths if use_lengths else _collate_no_lengths,\n",
    "    )\n",
    "\n",
    "    preds_all = []\n",
    "\n",
    "    for batch in loader:\n",
    "        if use_lengths:\n",
    "            x, lengths = batch\n",
    "            x = x.to(device_local, non_blocking=True)\n",
    "            lengths = lengths.to(device_local, non_blocking=True)\n",
    "            logits = model(x, lengths)\n",
    "        else:\n",
    "            x = batch.to(device_local, non_blocking=True)\n",
    "            logits = model(x)\n",
    "\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (probs >= threshold).astype(np.int64)\n",
    "        preds_all.append(preds)\n",
    "\n",
    "    preds_all = np.concatenate(preds_all, axis=0)\n",
    "    assert len(preds_all) == len(test_df), \"Prediction count mismatch with test_df rows\"\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        ID_COL: test_df[ID_COL].values,\n",
    "        TARGET_COL: preds_all\n",
    "    })\n",
    "\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved submission: {out_path} | rows={len(sub)} | positives={sub[TARGET_COL].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "52e3f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_full=6835 | test=3263 | device=cuda\n",
      "vocab_size=18683 | elapsed=0.02s\n",
      "glove_hits=8777 coverage=0.4698 | elapsed=3.22s\n",
      "\n",
      "=== TRAIN RNN (FULL) ===\n",
      "trained epochs=3 | bs=128 lr=0.0005 seed=23 | elapsed=0.94s\n",
      "\n",
      "=== WRITE RNN SUBMISSION ===\n",
      "Saved: MG-disaster-RNN-submission.csv | rows=3263 | positives=0.4582\n",
      "\n",
      "=== TRAIN GRU (FULL) ===\n",
      "trained epochs=8 | bs=128 lr=0.0001 seed=42 | elapsed=3.61s\n",
      "\n",
      "=== WRITE GRU SUBMISSION ===\n",
      "Saved: MG-disaster-GRU-submission.csv | rows=3263 | positives=0.3779\n"
     ]
    }
   ],
   "source": [
    "# FINAL TRAIN (FULL train_df) + KAGGLE SUBMISSIONS\n",
    "# - Uses your WORKING RNN logic (length-aware last valid timestep)\n",
    "# - Uses your WORKING GRU logic (pack_padded_sequence)\n",
    "# - Writes:\n",
    "#     MG-disaster-RNN-submission.csv\n",
    "#     MG-disaster-GRU-submission.csv\n",
    "#\n",
    "# HARD ASSUMPTIONS (blindata):\n",
    "# - train_df exists in memory with columns: \"text\", \"target\"\n",
    "# - data/test.csv exists with columns: \"id\", \"text\"\n",
    "# - GloVe exists at: data/glove/model.txt\n",
    "# - Output format is exactly: id,target\n",
    "\n",
    "import io, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# -------------------------\n",
    "# CONSTANTS\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"target\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "GLOVE_TXT = r\"data/glove/model.txt\"\n",
    "EMB_DIM = 300\n",
    "\n",
    "MAX_VOCAB = 60000\n",
    "MAX_LEN = 64\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# -------------------------\n",
    "# BEST PARAMS (from your grids)\n",
    "# -------------------------\n",
    "# RNN best: seed=23, bs=128, lr=5e-4, best_epoch=3\n",
    "RNN_SEED = 23\n",
    "RNN_BATCH = 128\n",
    "RNN_LR = 5e-4\n",
    "RNN_EPOCHS = 3\n",
    "RNN_HIDDEN = 128\n",
    "\n",
    "# GRU best: seed=42, bs=128, lr=1e-4, best_epoch=8\n",
    "GRU_SEED = 42\n",
    "GRU_BATCH = 128\n",
    "GRU_LR = 1e-4\n",
    "GRU_EPOCHS = 8\n",
    "GRU_HIDDEN = 128\n",
    "\n",
    "# -------------------------\n",
    "# UTILS\n",
    "# -------------------------\n",
    "def basic_tokenize(s):\n",
    "    return str(s).lower().strip().split()\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@torch.no_grad()\n",
    "def write_submission_from_model(model, test_df, out_path, use_lengths: bool):\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "\n",
    "    class TestDatasetXL(Dataset):\n",
    "        def __init__(self, df_):\n",
    "            self.df = df_.reset_index(drop=True)\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "        def __getitem__(self, idx):\n",
    "            text = self.df.loc[idx, TEXT_COL]\n",
    "            toks = basic_tokenize(text)[:MAX_LEN]\n",
    "            ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "            length = len(ids)\n",
    "            if length < MAX_LEN:\n",
    "                ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "            x = torch.tensor(ids, dtype=torch.long)\n",
    "            l = torch.tensor(length, dtype=torch.long)\n",
    "            return x, l\n",
    "\n",
    "    def collate_xl(batch):\n",
    "        xs, ls = zip(*batch)\n",
    "        return torch.stack(xs, dim=0), torch.stack(ls, dim=0)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TestDatasetXL(test_df),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_xl\n",
    "    )\n",
    "\n",
    "    for x, l in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        l = l.to(device, non_blocking=True)\n",
    "        if use_lengths:\n",
    "            logits = model(x, l)\n",
    "        else:\n",
    "            logits = model(x)\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (probs >= THRESHOLD).astype(np.int64)\n",
    "        preds_all.append(preds)\n",
    "\n",
    "    preds_all = np.concatenate(preds_all, axis=0)\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        ID_COL: test_df[ID_COL].values,\n",
    "        LABEL_COL: preds_all\n",
    "    })\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(f\"Saved: {out_path} | rows={len(sub)} | positives={sub[LABEL_COL].mean():.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "train_full = train_df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "assert ID_COL in test_df.columns and TEXT_COL in test_df.columns, \"data/test.csv must have columns: id, text\"\n",
    "\n",
    "print(f\"train_full={len(train_full)} | test={len(test_df)} | device={device}\")\n",
    "\n",
    "# -------------------------\n",
    "# BUILD VOCAB + EMB MATRIX (ON FULL TRAIN)\n",
    "# -------------------------\n",
    "t0 = time.time()\n",
    "counter = Counter()\n",
    "for t in train_full[TEXT_COL]:\n",
    "    counter.update(basic_tokenize(t))\n",
    "\n",
    "itos = [PAD_TOKEN, UNK_TOKEN]\n",
    "for w, _ in counter.most_common():\n",
    "    if len(itos) >= MAX_VOCAB:\n",
    "        break\n",
    "    itos.append(w)\n",
    "\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab_size = len(stoi)\n",
    "print(f\"vocab_size={vocab_size} | elapsed={time.time()-t0:.2f}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "glove_vecs = {}\n",
    "with io.open(GLOVE_TXT, \"r\", encoding=\"utf-8\", newline=\"\\n\", errors=\"ignore\") as f:\n",
    "    _ = f.readline()\n",
    "    for line in f:\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) != EMB_DIM + 1:\n",
    "            continue\n",
    "        w = parts[0]\n",
    "        if w in stoi:\n",
    "            glove_vecs[w] = np.asarray(parts[1:], dtype=np.float32)\n",
    "\n",
    "emb_matrix = np.random.normal(0, 0.05, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "emb_matrix[stoi[PAD_TOKEN]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "\n",
    "hit = 0\n",
    "for w, i in stoi.items():\n",
    "    v = glove_vecs.get(w)\n",
    "    if v is not None:\n",
    "        emb_matrix[i] = v\n",
    "        hit += 1\n",
    "\n",
    "print(f\"glove_hits={hit} coverage={hit/vocab_size:.4f} | elapsed={time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# DATASET (x, y, length) + COLLATE (same pattern you used in the working cell)\n",
    "# -------------------------\n",
    "class TrainDatasetXYL(Dataset):\n",
    "    def __init__(self, df_):\n",
    "        self.df = df_.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        toks = basic_tokenize(self.df.loc[idx, TEXT_COL])[:MAX_LEN]\n",
    "        ids = [stoi.get(t, stoi[UNK_TOKEN]) for t in toks]\n",
    "        length = len(ids)\n",
    "        if length < MAX_LEN:\n",
    "            ids += [stoi[PAD_TOKEN]] * (MAX_LEN - length)\n",
    "        x = torch.tensor(ids, dtype=torch.long)\n",
    "        y = torch.tensor(int(self.df.loc[idx, LABEL_COL]), dtype=torch.float32)\n",
    "        l = torch.tensor(length, dtype=torch.long)\n",
    "        return x, y, l\n",
    "\n",
    "def collate_batch(batch):\n",
    "    xs, ys, ls = zip(*batch)\n",
    "    return torch.stack(xs, dim=0), torch.stack(ys, dim=0), torch.stack(ls, dim=0)\n",
    "\n",
    "# -------------------------\n",
    "# MODELS (match your WORKING logic)\n",
    "# -------------------------\n",
    "class VanillaRNNClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, freeze_emb=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.embedding.weight.requires_grad = (not freeze_emb)\n",
    "        self.rnn = nn.RNN(EMB_DIM, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.rnn(emb)\n",
    "        lengths = lengths.to(out.device)\n",
    "        idx = (lengths - 1).clamp(min=0)\n",
    "        b = torch.arange(out.size(0), device=out.device)\n",
    "        last = out[b, idx, :]\n",
    "        return self.fc(last).squeeze(1)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, freeze_emb=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=stoi[PAD_TOKEN])\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "        self.embedding.weight.requires_grad = (not freeze_emb)\n",
    "        self.gru = nn.GRU(EMB_DIM, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, h_n = self.gru(packed)\n",
    "        last = h_n[-1]\n",
    "        return self.fc(last).squeeze(1)\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN LOOPS\n",
    "# -------------------------\n",
    "def train_full_model(model, seed, batch_size, lr, epochs, use_lengths: bool):\n",
    "    seed_all(seed)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        TrainDatasetXYL(train_full),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=(PIN_MEMORY and device.type == \"cuda\"),\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_batch\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for x, y, l in loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            l = l.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if use_lengths:\n",
    "                logits = model(x, l)\n",
    "            else:\n",
    "                logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"trained epochs={epochs} | bs={batch_size} lr={lr} seed={seed} | elapsed={time.time()-t0:.2f}s\")\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN + SUBMIT: RNN\n",
    "# -------------------------\n",
    "print(\"\\n=== TRAIN RNN (FULL) ===\")\n",
    "model_rnn = VanillaRNNClassifier(hidden_dim=RNN_HIDDEN, freeze_emb=False).to(device)\n",
    "train_full_model(model_rnn, RNN_SEED, RNN_BATCH, RNN_LR, RNN_EPOCHS, use_lengths=True)\n",
    "\n",
    "print(\"\\n=== WRITE RNN SUBMISSION ===\")\n",
    "write_submission_from_model(\n",
    "    model=model_rnn,\n",
    "    test_df=test_df,\n",
    "    out_path=\"MG-disaster-RNN-submission.csv\",\n",
    "    use_lengths=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN + SUBMIT: GRU\n",
    "# -------------------------\n",
    "print(\"\\n=== TRAIN GRU (FULL) ===\")\n",
    "model_gru = GRUClassifier(hidden_dim=GRU_HIDDEN, freeze_emb=False).to(device)\n",
    "train_full_model(model_gru, GRU_SEED, GRU_BATCH, GRU_LR, GRU_EPOCHS, use_lengths=True)\n",
    "\n",
    "print(\"\\n=== WRITE GRU SUBMISSION ===\")\n",
    "write_submission_from_model(\n",
    "    model=model_gru,\n",
    "    test_df=test_df,\n",
    "    out_path=\"MG-disaster-GRU-submission.csv\",\n",
    "    use_lengths=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4125e",
   "metadata": {},
   "source": [
    "### 4.5 Model Comparison and Visualization\n",
    "\n",
    "The Vanilla RNN and GRU models were compared using the same training data, preprocessing pipeline, and evaluation protocol. As discussed in the previous sections, both architectures reach very similar validation accuracies on the internal hold-out split, with only marginal differences between their best configurations.\n",
    "\n",
    "Given the limited separation observed on the validation set and the intrinsic ambiguity of the dataset, validation accuracy alone is not sufficient to establish a clear superiority of one model over the other. For this reason, the final comparison is performed using the Kaggle test set, which serves as an external and unseen benchmark.\n",
    "\n",
    "Both models are trained using their respective best configurations and submitted to Kaggle without any task-specific heuristics or post-processing. The resulting leaderboard scores provide an objective measure of generalization performance and allow a fair comparison under real test conditions.\n",
    "\n",
    "The final model selection is therefore based on Kaggle performance, rather than on small fluctuations observed on the validation split, ensuring a robust and unbiased evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fbd23",
   "metadata": {},
   "source": [
    "![kaggle submissions](./images/kaggle_results.png)\n",
    "\n",
    "*kaggle submissions*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d0dcf5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.5 – Model Comparison: Validation vs Kaggle (Visualization)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# DATA (best results)\n",
    "# -------------------------\n",
    "data = {\n",
    "    \"Model\": [\"Vanilla RNN\", \"GRU\"],\n",
    "    \"Best_Validation_Accuracy\": [0.8178, 0.8269],\n",
    "    \"Kaggle_Public_Score\": [0.7594, 0.7864],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 1: Validation vs Kaggle\n",
    "# -------------------------\n",
    "plt.figure(figsize=(8, 4.5))\n",
    "\n",
    "x = range(len(df))\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(\n",
    "    [i - bar_width / 2 for i in x],\n",
    "    df[\"Best_Validation_Accuracy\"],\n",
    "    width=bar_width,\n",
    "    label=\"Best Validation Accuracy\"\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    [i + bar_width / 2 for i in x],\n",
    "    df[\"Kaggle_Public_Score\"],\n",
    "    width=bar_width,\n",
    "    label=\"Kaggle Public Score\"\n",
    ")\n",
    "\n",
    "plt.xticks(x, df[\"Model\"])\n",
    "plt.ylim(0.7, 0.85)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy vs Kaggle Public Score\")\n",
    "plt.legend()\n",
    "\n",
    "# value labels\n",
    "for i, v in enumerate(df[\"Best_Validation_Accuracy\"]):\n",
    "    plt.text(i - bar_width / 2, v + 0.005, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "for i, v in enumerate(df[\"Kaggle_Public_Score\"]):\n",
    "    plt.text(i + bar_width / 2, v + 0.005, f\"{v:.3f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# -------------------------\n",
    "# PLOT 2 (FINAL): Kaggle Performance Share – Pie Chart\n",
    "# -------------------------\n",
    "\n",
    "kaggle_scores = df[\"Kaggle_Public_Score\"].values\n",
    "labels = df[\"Model\"].values\n",
    "\n",
    "# normalize just for visualization\n",
    "weights = kaggle_scores / kaggle_scores.sum()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.pie(\n",
    "    weights,\n",
    "    labels=labels,\n",
    "    autopct=lambda p: f\"{p:.1f}%\",\n",
    "    startangle=90\n",
    ")\n",
    "\n",
    "plt.title(\"Relative Kaggle Performance Contribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb89702",
   "metadata": {},
   "source": [
    "### 4.6 Troubleshooting\n",
    "\n",
    "During development, I ran into a few non-obvious issues while implementing both the Vanilla RNN and the GRU in PyTorch. Initially, both models showed clearly degenerate behavior, quickly converging to almost constant predictions and misleading accuracy plateaus. At first this looked like a tuning problem, but it wasn’t.\n",
    "\n",
    "Long story short, **the culprit was sequence padding and improper handling of variable-length inputs**. The models were effectively using padded timesteps instead of real content when computing the final hidden representations. Once the true sequence lengths were passed explicitly—selecting the last valid hidden state in the Vanilla RNN and using `pack_padded_sequence` in the GRU—everything went right almost immediately and the models started learning as expected.\n",
    "\n",
    "Beyond implementation details, the dataset itself introduces limitations. The texts are short, often lack context, and in several cases are ambiguous even for a human reader. Under these conditions, targeting accuracies above 90% was never realistic without adding stronger semantic priors or manual feature engineering.\n",
    "\n",
    "Given this context, final accuracies around 0.78–0.80 are a reasonable and technically sound outcome, reflecting data ambiguity rather than model deficiencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c77779",
   "metadata": {},
   "source": [
    "### 4.7 Hyperparameter Optimization Procedure Summary\n",
    "\n",
    "Hyperparameter optimization was performed by iterating over learning rate, batch size, random seed and number of epochs for both the Vanilla RNN and the GRU. Each configuration was evaluated on the validation split, tracking performance at every epoch and applying early stopping to avoid unnecessary computation.\n",
    "\n",
    "For each model, the top-performing configurations were identified, and the final choice was made following Occam’s razor: when multiple setups achieved very similar accuracy, the simplest and least computationally expensive configuration was selected.\n",
    "\n",
    "The resulting best configurations are:\n",
    "\n",
    "* **Vanilla RNN**: batch size 128, learning rate 5e-4, 3 epochs\n",
    "* **GRU**: batch size 128, learning rate 1e-4, 8 epochs\n",
    "\n",
    "This approach ensured a fair comparison between models while maintaining a balance between performance, stability and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4f25f",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd0b5f",
   "metadata": {},
   "source": [
    "### 5.1 Results Interpretation\n",
    "\n",
    "The experimental results show that both recurrent architectures are able to learn meaningful patterns from the data and generalize reasonably well to unseen samples.\n",
    "\n",
    "On the internal validation split, both the Vanilla RNN and the GRU consistently reach accuracies in the 0.80–0.83 range once properly tuned and implemented. When evaluated on the Kaggle test set, which represents a fully unseen benchmark, performance naturally decreases, with the Vanilla RNN achieving a score of 0.76 and the GRU reaching 0.79.\n",
    "\n",
    "![kaggle submissions](./images/kaggle_results.png)\n",
    "\n",
    "*kaggle submissions*\n",
    "\n",
    "\n",
    "\n",
    "This drop in performance is expected and does not indicate overfitting or implementation issues. Instead, it highlights the difference between validation on a controlled split and evaluation on real test data that may contain distribution shifts, additional noise, or ambiguous samples. Importantly, the relative ordering between models is preserved: the GRU shows slightly better generalization on Kaggle, while the Vanilla RNN remains surprisingly competitive given its simplicity.\n",
    "\n",
    "Overall, these results confirm that the chosen modeling approach is sound and that the observed performance levels are primarily constrained by the characteristics of the dataset rather than by shortcomings of the architectures themselves. The Kaggle scores provide a realistic estimate of achievable performance under these conditions and serve as a reliable external reference for the final model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef1ee7",
   "metadata": {},
   "source": [
    "### 5.2 Learnings\n",
    "\n",
    "The biggest learning was that implementation details matter more than model choice. Both RNN and GRU were initially failing for non-obvious reasons, and no amount of tuning helped until sequence lengths were handled correctly and padded timesteps were excluded from the final representation.\n",
    "\n",
    "Another clear takeaway is that hyperparameter tuning beats architectural complexity. Once batch size and learning rate were set properly, a simple Vanilla RNN reached almost the same performance as a GRU, with faster convergence and lower computational cost.\n",
    "\n",
    "Early stopping also proved useful in practice: most models peaked very early, making long training runs unnecessary and sometimes misleading.\n",
    "\n",
    "Finally, inspecting misclassified samples made it clear that part of the error is unavoidable. Many texts are short, ambiguous, or weakly labeled, and even a human reader would struggle in some cases. At that point, the model is no longer the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be33ac",
   "metadata": {},
   "source": [
    "### 5.3 Takeaways: What Didn’t Work (and What Did)\n",
    "\n",
    "**What Didn’t Work**\n",
    "\n",
    "Despite investing a significant amount of time exploring multiple hyperparameter combinations, hyperparameters alone did not make a dramatic difference. Learning rate, batch size, number of epochs and random seed affected stability and convergence speed, but they did not fundamentally change the ceiling of achievable performance. Once reasonable values were found, further fine-tuning mostly produced marginal gains.\n",
    "\n",
    "Increasing architectural complexity also did not help as much as expected. The GRU slightly outperformed the Vanilla RNN, but the improvement was modest and far from transformative. This suggests that model capacity was not the main limiting factor in this task.\n",
    "\n",
    "\n",
    "**What did work**\n",
    "\n",
    "What made the biggest difference was starting from a clean and well-prepared dataset. Careful data cleaning, normalization, and consistent preprocessing were fundamental in allowing the models to learn anything meaningful at all. Without this step, no amount of tuning or architectural changes would have compensated for noisy or inconsistent inputs.\n",
    "\n",
    "Beyond that, correct implementation and stable training dynamics mattered more than aggressive optimization. Handling variable-length sequences properly, fixing padding-related issues, using early stopping, and keeping the training loop simple and reproducible had a clear positive impact on results.\n",
    "\n",
    "Finally, understanding the dataset itself proved essential. Many samples are short, ambiguous, or borderline even for a human reader, which naturally limits achievable accuracy. In this context, validation and Kaggle scores around 0.78–0.83 reflect solid learning rather than a lack of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd7e86",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### 5.4 Improvements and Future Work\n",
    "\n",
    "To reach the highest positions in this competition (around 0.85 accuracy), it would clearly be necessary to actively help the model beyond plain text encoding. Given the presence of many ambiguous or borderline phrases, additional features are needed to guide the classification toward the correct category.\n",
    "\n",
    "One effective direction would be to introduce explicit word weighting, assigning stronger influence to terms that are strongly associated with real disasters, such as 'earthquake', 'flood', or 'drown'. At the same time, this would need to be balanced with awareness of composite expressions and word tuples. For example, phrases like 'fire squad' should not be interpreted as disasters, despite containing the word 'fire' which, by itself, it's a disaster bearer if in the right context, but it's also used in slang tweets.\n",
    "\n",
    "Finally, more sophisticated approaches—such as richer semantic representations, stronger sequence models, or ensemble strategies—would likely provide further gains. These techniques could help resolve edge cases and reduce ambiguity, ultimately pushing performance closer to the top leaderboard scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3fd18",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "[1] Kaggle. Natural Language Processing with Disaster Tweets.\n",
    "https://www.kaggle.com/competitions/nlp-getting-started\n",
    "\n",
    "[2] PyTorch Documentation. Recurrent Neural Networks.\n",
    "https://pytorch.org/docs/stable/nn.html#recurrent-layers\n",
    "\n",
    "[3] PyTorch Documentation. pack_padded_sequence.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "\n",
    "[4] Pennington, J., Socher, R., & Manning, C. (2014).\n",
    "GloVe: Global Vectors for Word Representation.\n",
    "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
    "\n",
    "[5] University of Colorado Boulder. Introduction to Deep Learning - Deep Learning on Sequential data\n",
    "https://www.coursera.org/learn/introduction-to-deep-learning-boulder-two/home/module/4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
